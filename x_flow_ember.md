This is something I found related to the flow/graph system I want to implement, maybe we can use it?

docs\cli\CLI_STATUS.md:
<code>

# Ember CLI Status

## Current Status

The Ember CLI is currently being developed separately from the core Python framework. It has been excluded from the main development workflow and git tracking to allow for independent development and prevent integration issues.

## Structure

The CLI is built with:

- TypeScript/Node.js
- Commander.js for command-line parsing
- Python-bridge for communication with the Python framework

## Key Components

1. **Command Modules**

   - model.ts - Manage LLM models
   - provider.ts - Manage providers
   - invoke.ts - Invoke models
   - config.ts - Manage configuration
   - project.ts - Project scaffolding
   - version.ts - Version information

2. **Services**

   - config-manager.ts - Manage CLI configuration
   - python-bridge.ts - Interface with the Python framework

3. **UI Components**
   - Spinners, progress bars, banners
   - Interactive prompts

The CLI is currently excluded from the standard installation. When development resumes, it will be available as a separate package or optional component.
</code>

docs\cli\DEVELOPERS.md:
<code>

# Ember CLI Developer Guide

This guide is intended for developers who want to contribute to the Ember CLI or extend it with plugins and custom functionality.

## Architecture Overview

The Ember CLI uses a hybrid architecture:

1. **Frontend**: Node.js/TypeScript CLI interface built with Commander.js
2. **Backend**: Python Ember core framework accessed via python-bridge
3. **Configuration**: Local storage using the conf package with encryption

```
┌─────────────────────────────────────────┐
│             Node.js Frontend            │
├─────────────┬───────────────┬───────────┤
│ UI/UX       │ Command       │ Config    │
│ Components  │ Handlers      │ Manager   │
├─────────────┴───────┬───────┴───────────┤
│        Python Bridge                    │
├─────────────────────┼───────────────────┤
│        Python Backend (Ember Core)      │
└─────────────────────────────────────────┘
```

### Key Components

- **CLI Entry Point** (`src/cli/index.ts`): Main entry point that sets up commands
- **Command Modules** (`src/cli/commands/`): Individual command implementations
- **Python Bridge** (`src/cli/bridge/`): Interface between TypeScript and Python
- **UI Components** (`src/cli/ui/`): User interface elements
- **Services** (`src/cli/services/`): Shared functionality
- **Utilities** (`src/cli/utils/`): Helper functions

## Development Setup

### Prerequisites

- Node.js 16+
- Python 3.11+
- TypeScript 4.9+
- Ember AI package

### Installation

```bash
# Clone the repository
git clone https://github.com/pyember/ember.git
cd ember

# Install dependencies
npm install

# Build the CLI
npm run build

# Create a link for local development
npm link
```

### Development Workflow

```bash
# Run in development mode with watch
npm run watch

# Test your changes
ember --debug <command>

# Lint code
npm run lint

# Run tests
npm test
```

## Adding a New Command

To add a new command to the CLI, follow these steps:

1. Create a new file in `src/cli/commands/` for your command
2. Implement the command with the Commander.js pattern
3. Register your command in `src/cli/index.ts`

Here's an example of a new command:

```typescript
// src/cli/commands/example.ts
import { Command } from "commander";
import chalk from "chalk";
import ora from "ora";

import { getPythonBridge } from "../bridge/python-bridge";
import { isJsonOutput } from "../utils/options";
import { displaySection, displaySuccess } from "../ui/intro";

/**
 * Register example command with the CLI program
 *
 * @param program The commander program instance
 */
export function registerExampleCommand(program: Command): void {
  program
    .command("example")
    .description("Example command to demonstrate development")
    .option("-n, --name <name>", "Name parameter")
    .action(async (options) => {
      await runExample(options);
    });
}

/**
 * Run the example command
 *
 * @param options Command options
 */
async function runExample(options: any): Promise<void> {
  const spinner = ora("Running example...").start();

  try {
    // Get name from options or use default
    const name = options.name || "world";

    // Get Python bridge
    const bridge = getPythonBridge();
    await bridge.initialize();

    // Call Python backend (example)
    const version = await bridge.getVersion();

    // Stop spinner
    spinner.stop();

    // Format and display result
    if (isJsonOutput()) {
      // JSON output
      console.log(
        JSON.stringify(
          {
            message: `Hello, ${name}!`,
            version,
          },
          null,
          2
        )
      );
    } else {
      // Human-readable output
      displaySection("Example Command");
      console.log(`Hello, ${chalk.cyan(name)}!`);
      console.log(`Ember version: ${version}`);
      displaySuccess("Command completed successfully");
    }
  } catch (error: any) {
    // Handle errors
    spinner.fail("Command failed");
    console.error(chalk.red("Error:"), error.message);
  }
}
```

Then register your command in `src/cli/index.ts`:

```typescript
// src/cli/index.ts
import { registerExampleCommand } from "./commands/example";

// Register commands
registerVersionCommand(program);
registerProviderCommands(program);
registerModelCommands(program);
registerProjectCommands(program);
registerInvokeCommand(program);
registerConfigCommands(program);
registerExampleCommand(program); // Add your new command here
```

## Extending the Python Bridge

If you need to add new functionality to the Python bridge:

1. Add a method to the `EmberPythonBridge` interface in `src/cli/bridge/python-bridge.ts`
2. Implement the method in the `PythonBridgeImpl` class
3. Add corresponding Python code that will be executed by the bridge

Example:

```typescript
// Add to the EmberPythonBridge interface
/**
 * Run a custom function in the Python backend
 */
runCustomFunction(name: string, args: Record<string, any>): Promise<any>;

// Implement in the PythonBridgeImpl class
async runCustomFunction(name: string, args: Record<string, any>): Promise<any> {
  await this.ensureInitialized();

  const argsJson = JSON.stringify(args);

  return await this.bridge.eval`
import json
try:
    # Call the function dynamically
    result = getattr(service, ${name})(**json.loads(${argsJson}))
    json.dumps(result)
except Exception as e:
    json.dumps({"error": str(e)})
`;
}
```

## UI Components

The CLI uses several UI components to create a beautiful user experience:

- **Banner**: Displays the Ember CLI logo
- **Spinners**: Shows progress for async operations
- **Tables**: Formats tabular data
- **Colors**: Highlights important information
- **Emoji**: Adds visual cues

When creating new UI components, follow these guidelines:

1. Use the `chalk` library for colors
2. Use `ora` for spinners
3. Use `table` for tabular data
4. Use `emoji` for visual cues
5. Always respect the `--quiet` and `--no-color` flags

## Configuration Management

The CLI uses the `conf` library to store configuration. Key features:

- **Encryption**: Sensitive data is encrypted
- **Schema Validation**: Configuration follows a defined schema
- **Persistence**: Configuration is stored between runs

When accessing configuration, always use the `ConfigManager` class:

```typescript
// Get config manager singleton
const configManager = ConfigManager.getInstance();

// Get a setting
const value = configManager.getSetting("my.setting", defaultValue);

// Set a setting
configManager.setSetting("my.setting", newValue);
```

## Error Handling

Follow these guidelines for error handling:

1. Always catch exceptions in async functions
2. Use `try/catch` blocks around Python bridge calls
3. Show user-friendly error messages
4. Include technical details in debug mode
5. Use spinners to indicate progress/failure

Example:

```typescript
try {
  // Code that might throw
  const result = await riskyOperation();
  // Handle success
} catch (error: any) {
  // Handle error
  console.error(chalk.red("Error:"), error.message);
  if (isDebugMode()) {
    console.error(error.stack);
  }
}
```

## Testing

The CLI includes a comprehensive test suite:

- **Unit Tests**: Test individual components
- **Integration Tests**: Test command flows
- **Mock Tests**: Test with mocked Python bridge

To write tests:

1. Create test files in the `__tests__` directory
2. Use Jest for testing
3. Mock external dependencies
4. Test both success and failure cases

Example test:

```typescript
// __tests__/commands/version.test.ts
import { registerVersionCommand } from "../../src/cli/commands/version";
import { getPythonBridge } from "../../src/cli/bridge/python-bridge";

// Mock the Python bridge
jest.mock("../../src/cli/bridge/python-bridge");

describe("Version Command", () => {
  beforeEach(() => {
    // Setup mocks
    (getPythonBridge as jest.Mock).mockImplementation(() => ({
      initialize: jest.fn().mockResolvedValue(undefined),
      getVersion: jest.fn().mockResolvedValue("0.1.0"),
    }));
  });

  it("should display version information", async () => {
    // Create a mock Commander instance
    const program = {
      command: jest.fn().mockReturnThis(),
      description: jest.fn().mockReturnThis(),
      option: jest.fn().mockReturnThis(),
      action: jest.fn().mockReturnThis(),
    };

    // Register command
    registerVersionCommand(program as any);

    // Verify command was registered
    expect(program.command).toHaveBeenCalledWith("version");

    // Get the action callback
    const actionCallback = program.action.mock.calls[0][0];

    // Create a mock console.log
    const originalLog = console.log;
    console.log = jest.fn();

    // Run the action
    await actionCallback({});

    // Verify output
    expect(console.log).toHaveBeenCalled();
    expect(getPythonBridge().getVersion).toHaveBeenCalled();

    // Restore console.log
    console.log = originalLog;
  });
});
```

## Building and Publishing

To build and publish the CLI:

```bash
# Build the CLI
npm run build

# Test the build
node dist/index.js version

# Publish to npm
npm publish
```

## Style Guide

Follow these coding style guidelines:

1. Use TypeScript for all JavaScript code
2. Use async/await for asynchronous operations
3. Document all public functions and classes with JSDoc
4. Use SOLID principles for code organization
5. Use semantic versioning for releases

## Best Practices

1. **Security**:

   - Never log API keys or sensitive information
   - Always encrypt stored credentials
   - Validate user input

2. **Performance**:

   - Minimize Python bridge calls
   - Batch operations when possible
   - Use streaming for large responses

3. **User Experience**:

   - Always show progress for long operations
   - Provide clear error messages
   - Include helpful tips and examples

4. **Code Quality**:
   - Write unit and integration tests
   - Use TypeScript types strictly
   - Document complex code
   - Follow the SOLID principles

## Troubleshooting Development Issues

### Python Bridge Issues

If you encounter problems with the Python bridge:

1. Check that Python 3.11+ is installed and accessible
2. Verify Ember AI is installed in the Python environment
3. Use debug mode to see Python errors
4. Check that Python bridge is being initialized correctly

### TypeScript Compilation Issues

For TypeScript errors:

1. Run `npm run lint` to find code issues
2. Check import paths (case-sensitive)
3. Ensure type definitions are correct
4. Run `tsc --noEmit` to type-check without building

## Getting Help

For development questions:

1. Check the existing code for examples
2. Review the documentation in code comments
3. Open an issue on GitHub for unanswered questions

## Contributing

We welcome contributions to the Ember CLI! Follow these steps:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Write tests for your changes
5. Run the test suite
6. Submit a pull request

## License

The Ember CLI is licensed under the MIT License.
</code>

docs\cli\ERROR_HANDLING.md:
<code>

# Error Handling in Ember CLI

Ember CLI includes a robust error handling system that provides structured errors, detailed information, and helpful suggestions to users. This document describes how error handling works and how to use it when developing new commands or features.

## Key Features

- **Structured Errors**: All errors extend a base `EmberCliError` class
- **Error Categorization**: Errors are categorized by type with specific error codes
- **Rich Error Information**: Errors can include suggestions, documentation links, and context
- **Python Error Mapping**: Python exceptions are properly mapped to TypeScript errors
- **Consistent Display**: Errors are displayed consistently across the CLI

## Error Class Hierarchy

The Ember CLI error classes follow this hierarchy:

```
EmberCliError (base class)
├── PythonBridgeError - For Python bridge communication issues
├── ModelError - For model and provider errors
├── ProjectError - For project-related errors
├── ValidationError - For user input validation errors
├── ConfigurationError - For configuration errors
├── AuthorizationError - For authentication and authorization errors
└── NetworkError - For network-related errors
```

## Error Codes

Errors are assigned unique error codes for identification and documentation:

- **1000-1999**: General CLI errors
- **2000-2999**: Python bridge errors
- **3000-3999**: Model and provider errors
- **4000-4999**: Project errors

## Using Error Handling in Commands

### Creating and Throwing Errors

When you need to create an error in your command:

```typescript
import { createModelError, ModelErrorCodes } from "../errors";

// Create and throw an error
throw createModelError(
  "Model not found: gpt-4",
  ModelErrorCodes.MODEL_NOT_FOUND,
  {
    suggestions: [
      "Run `ember models` to list available models",
      "Check that your OpenAI API key is set correctly",
    ],
  }
);
```

### Using the Try-Catch Helper

You can use the `tryCatch` helper to handle errors consistently:

```typescript
import { tryCatch } from "../errors";

await tryCatch(
  async () => {
    // Your code here
    const result = await someAsyncFunction();
    return result;
  },
  {}, // Error format options
  true // Exit process on error
);
```

### Handling Errors in Command Actions

For command actions, wrap your function in a try-catch block:

```typescript
.action(async (options) => {
  try {
    await commandFunction(options);
  } catch (error) {
    handleError(error, {}, true);
  }
});
```

## Error Format Options

When displaying errors with `handleError`, you can customize the output:

```typescript
handleError(error, {
  includeCode: true, // Include error code in output
  includeSuggestions: true, // Include suggestions
  includeDocsLinks: true, // Include documentation links
  includeStack: false, // Include stack trace
  useColor: true, // Use colored output
  asJson: false, // Format as JSON
});
```

## Python Error Translation

The Python bridge translates Python exceptions to TypeScript errors:

```
Python Exception         →  TypeScript Error
-----------------           ---------------
ModelNotFoundError      →  ModelError (MODEL_NOT_FOUND)
ProviderAPIError        →  ModelError (PROVIDER_API_ERROR)
ValidationError         →  ValidationError (INVALID_ARGUMENT)
FileNotFoundError       →  ValidationError (FILE_NOT_FOUND)
```

## Best Practices

1. **Use Specific Error Types**: Use the most specific error type for the situation
2. **Include Helpful Suggestions**: Always include suggestions to help users resolve the issue
3. **Add Context**: Include relevant context information for debugging
4. **Clean Up Resources**: Always clean up resources in error handlers (like spinners)
5. **Validate Early**: Validate user input early to avoid deeper errors
6. **Handle All Errors**: Never let errors go unhandled or display as generic errors
7. **Add Documentation Links**: For complex issues, include links to documentation

## Example: Command Error Handling

Here's a complete example of error handling in a command:

```typescript
async function myCommand(options: any): Promise<void> {
  const spinner = ora("Loading...").start();

  try {
    // Validate input
    if (!options.required) {
      throw createValidationError(
        "Missing required option",
        GeneralErrorCodes.MISSING_REQUIRED_ARGUMENT
      );
    }

    // Do something that might fail
    const result = await someAsyncOperation();

    // Handle success
    spinner.succeed("Operation completed");
    console.log(result);
  } catch (error) {
    // Stop spinner
    spinner.stop();

    // Rethrow (will be handled by command wrapper)
    throw error;
  }
}
```

</code>

docs\cli\QUICKREF.md:
<code>

# Ember CLI Quick Reference

This quick reference guide covers the most common Ember CLI commands and usage patterns.

## Getting Started

```bash
# Install the CLI
npm install -g ember-cli

# Check version
ember version

# Get help
ember help
```

## Provider Management

```bash
# List providers
ember provider list

# Configure a provider
ember provider configure openai

# Set default provider
ember provider use openai
```

## Model Management

```bash
# List models
ember model list

# List models for a specific provider
ember model list --provider openai

# Get model information
ember model info openai:gpt-4o

# Set default model
ember model use openai:gpt-4o
```

## Invoking Models

```bash
# Basic invocation (uses default model if set)
ember invoke --prompt "Hello, world!"

# Invoke specific model
ember invoke --model openai:gpt-4o --prompt "Hello, world!"

# Read prompt from file
ember invoke --model openai:gpt-4o --file myprompt.txt

# Add system prompt
ember invoke --model openai:gpt-4o --prompt "List 5 capitals" --system "You are a geography expert"

# Save output to file
ember invoke --model openai:gpt-4o --prompt "Write a poem" --output poem.txt

# Show token usage
ember invoke --model openai:gpt-4o --prompt "Explain quantum computing" --show-usage

# Stream response
ember invoke --model openai:gpt-4o --prompt "Write a story" --stream
```

## Project Management

```bash
# Create new project
ember project new myproject

# Create project with specific template
ember project new myproject --template api

# List available templates
ember project templates

# Analyze project
ember project analyze ./myproject
```

## Configuration Management

```bash
# List configuration
ember config list

# Set configuration
ember config set defaultModel openai:gpt-4o

# Get specific configuration
ember config get defaultProvider

# Export/import configuration
ember config export config.json
ember config import config.json

# Reset configuration
ember config reset
```

## JSON Output (for scripting)

```bash
# Get providers as JSON
ember provider list --json

# Get models as JSON
ember model list --json

# Run model and get JSON result
ember invoke --model openai:gpt-4o --prompt "Hello" --json > result.json
```

## Environment Variables

```bash
# Set API keys
export OPENAI_API_KEY="your-api-key"
export ANTHROPIC_API_KEY="your-api-key"

# Set defaults
export EMBER_DEFAULT_PROVIDER="openai"
export EMBER_DEFAULT_MODEL="openai:gpt-4o"

# Other options
export EMBER_DEBUG=1
export EMBER_NO_COLOR=1
```

## Common Options

```bash
# Debug mode
ember <command> --debug

# Disable colors
ember <command> --no-color

# Quiet mode (minimal output)
ember <command> --quiet

# JSON output
ember <command> --json
```

## Shell Completion

```bash
# Install shell completion
ember completion install

# Generate completion for a specific shell
ember completion bash
ember completion zsh
ember completion fish
ember completion powershell

# Output to a file
ember completion bash ~/.bash_completion.d/ember
```

</code>

docs\cli\README.md:
<code>

# Ember CLI Documentation

The Ember CLI is a powerful command-line interface for interacting with the Ember AI framework. It provides a beautiful, intuitive interface for managing models, providers, configurations, and projects.

## Installation

### Prerequisites

- Node.js 16.0 or higher
- Python 3.11 or higher
- Ember AI package installed (`uv pip install ember-ai`)

### Installing the CLI

```bash
# Global installation
npm install -g ember-cli

# Local installation
npm install ember-cli
```

## Getting Started

### Quick Start

```bash
# Display the version
ember version

# List available providers
ember provider list

# Configure a provider with your API key
ember provider configure openai

# List available models
ember model list

# Invoke a model
ember invoke --model openai:gpt-4o-mini --prompt "Hello, world!"
```

### Basic Concepts

Ember CLI organizes functionality around these core concepts:

1. **Providers**: AI service providers like OpenAI, Anthropic, etc.
2. **Models**: Specific LLM models offered by providers, referenced as `provider:model`
3. **Projects**: Ember applications that utilize the framework
4. **Configuration**: Settings for CLI, providers, and models

## Command Reference

### Global Options

These options work with all commands:

| Option       | Description                          |
| ------------ | ------------------------------------ |
| `--debug`    | Enable debug mode with detailed logs |
| `--json`     | Output results as JSON               |
| `--quiet`    | Suppress non-essential output        |
| `--no-color` | Disable colored output               |

### Shell Completion

Ember CLI supports shell completion for Bash, Zsh, Fish, and PowerShell. This provides tab completion for commands, options, and even dynamic values like model IDs and provider names.

```bash
# To install shell completion for your current shell
ember completion install

# Generate completion script for a specific shell
ember completion bash > ~/.bash_completion.d/ember
ember completion zsh > ~/.zsh/completion/_ember
ember completion fish > ~/.config/fish/completions/ember.fish
```

For detailed instructions, see [Shell Completion Documentation](SHELL_COMPLETION.md).

### Core Commands

#### Version

Display version information about the CLI and backend.

```bash
ember version [options]

Options:
  --check    Check for updates
```

#### Providers

Manage LLM providers in Ember.

```bash
# List available providers
ember provider list

# Configure a provider with API key
ember provider configure <provider> [options]
Options:
  -k, --key <key>    API key (omit to be prompted securely)
  -f, --force        Overwrite existing configuration

# Display provider information
ember provider info <provider>

# Set a provider as default
ember provider use <provider>
```

#### Models

Manage LLM models in Ember.

```bash
# List available models
ember model list [options]
Options:
  -p, --provider <provider>    Filter models by provider

# Display model information
ember model info <model>

# Set a model as default
ember model use <model>

# Benchmark a model's performance
ember model benchmark <model> [options]
Options:
  -t, --tests <tests>              Number of tests to run (default: 5)
  -c, --concurrency <concurrency>  Concurrency level (default: 1)
```

#### Invoke

Invoke a model with a prompt directly from the CLI.

```bash
ember invoke [options]

Options:
  -m, --model <model>          Model ID to use
  -p, --prompt <prompt>        Prompt text to send to the model
  -f, --file <file>            Read prompt from file
  -s, --system <system>        System prompt (for chat models)
  -t, --temperature <temp>     Temperature setting (0.0-2.0) (default: 1.0)
  -u, --show-usage             Show token usage statistics
  -o, --output <file>          Save output to file
  -r, --raw                    Display raw output without formatting
  --stream                     Stream the response token by token
```

#### Projects

Create and manage Ember projects.

```bash
# Create a new project
ember project new <name> [options]
Options:
  -t, --template <template>     Project template (default: basic)
  -d, --directory <directory>   Project directory (defaults to name)
  -p, --provider <provider>     Default provider to use
  -m, --model <model>           Default model to use

# List available project templates
ember project templates

# Analyze an existing project
ember project analyze [directory]
```

#### Configuration

Manage Ember CLI configuration.

```bash
# List configuration settings
ember config list [options]
Options:
  --show-keys    Show API keys (not recommended)

# Set a configuration value
ember config set <key> <value>

# Get a configuration value
ember config get <key>

# Reset configuration to defaults
ember config reset [options]
Options:
  -f, --force    Skip confirmation

# Export configuration to a file
ember config export <file>

# Import configuration from a file
ember config import <file> [options]
Options:
  -f, --force    Overwrite existing configuration

# Configure usage tracking
ember config usage-tracking <enabled>
```

## Usage Examples

### Configuring Providers

```bash
# List available providers
$ ember provider list
▸ Available Providers

Provider    Status           Default
──────────  ───────────────  ───────
openai      Not Configured
anthropic   Not Configured
google      Not Configured

💡 Tip: Configure a provider with ember provider configure <provider>

# Configure OpenAI
$ ember provider configure openai
Enter API key for openai: [input is hidden]
✅ Provider openai configured successfully.
✅ Provider openai set as default.

# Check provider information
$ ember provider info openai
▸ Provider: openai

Name: OpenAI
Description: Provider for OpenAI models including GPT-4 and GPT-3.5
Website: https://openai.com

Available Models:
  • gpt-4o
  • gpt-4o-mini
  • gpt-4-turbo
  • gpt-3.5-turbo

Authentication:
  Environment Variable: OPENAI_API_KEY
  Status: Configured
```

### Working with Models

```bash
# List all models
$ ember model list

▸ Available Models

openai
  ✓ gpt-4o
   gpt-4o-mini
   gpt-4-turbo
   gpt-3.5-turbo

anthropic
   claude-3-opus
   claude-3-sonnet

💡 Tip: Get model details with ember model info <model>

# Get model information
$ ember model info openai:gpt-4o
▸ Model: openai:gpt-4o

Name: GPT-4o
Description: OpenAI's most advanced model for vision, language, audio and video.
Provider: openai

Capabilities:
  ✓ text
  ✓ images
  ✓ audio
  ✓ function_calling

Context Size: 128,000 tokens

Cost:
  Input: $0.005 per 1K tokens
  Output: $0.015 per 1K tokens

💡 Tip: Invoke this model with ember invoke --model openai:gpt-4o --prompt "Your prompt"

# Set default model
$ ember model use openai:gpt-4o
✅ Model openai:gpt-4o set as default.
```

### Invoking Models

```bash
# Invoke a model with a prompt
$ ember invoke --model openai:gpt-4o --prompt "Explain quantum computing in simple terms"
Invoking openai:gpt-4o...
────────────────────────────────────────────────────────────────
Quantum computing is like having a super-powerful calculator that works in a completely different way than regular computers.

Regular computers use bits (0s and 1s) to process information one step at a time. Quantum computers use quantum bits or "qubits" that can be 0, 1, or both at the same time (called superposition).

This means quantum computers can look at many possible solutions simultaneously instead of checking them one by one. It's like being able to take all possible paths through a maze at once, rather than trying each path individually.

This makes quantum computers potentially much faster at solving certain types of problems, like breaking encryption codes, simulating molecules for drug discovery, or optimizing complex systems like traffic flows in a city.

However, quantum computers are still very new, expensive, and difficult to build and maintain. They need to be kept extremely cold, and they're prone to errors. So while they show promise for specific applications, they won't replace your laptop anytime soon!

# Invoke with system prompt and save output
$ ember invoke -m openai:gpt-4o -p "List 5 renewable energy sources" -s "You are an environmental scientist" -o energy.txt
Invoking openai:gpt-4o...
────────────────────────────────────────────────────────────────
As an environmental scientist, I can identify these 5 key renewable energy sources:

1. Solar Energy: Harnessed through photovoltaic panels or concentrated solar power systems that convert sunlight directly into electricity or heat.

2. Wind Energy: Generated by turbines that convert kinetic energy from wind into mechanical power then electricity, with offshore wind farms showing particularly high efficiency.

3. Hydroelectric Power: Utilizes the gravitational force of flowing or falling water, typically through dams or run-of-river systems, representing one of the most established renewable sources.

4. Geothermal Energy: Taps into Earth's internal thermal energy through wells that access heated water and steam reservoirs, providing consistent baseload power with minimal land footprint.

5. Biomass Energy: Derived from organic materials like wood, agricultural residues, or dedicated energy crops that store solar energy through photosynthesis, which can be converted to heat, electricity, or liquid biofuels.

Each of these sources represents a sustainable alternative to fossil fuels with significantly lower lifecycle carbon emissions, though each has specific geographic, environmental, and economic considerations for optimal implementation.

✅ Output saved to /Users/username/energy.txt
```

### Creating Projects

```bash
# List available templates
$ ember project templates

▸ Available Project Templates

Basic Project (basic)
Simple starter project with minimal dependencies

Features:
  • Direct model invocation
  • Basic usage examples
──────────────────────────────────────────────────

Complete Project (complete)
Full-featured project with all operators and utilities

Features:
  • Operators
  • Evaluation tools
  • Configuration examples
  • Advanced usage patterns
──────────────────────────────────────────────────

API Project (api)
Project template for building APIs with Ember

Features:
  • FastAPI integration
  • API endpoint examples
  • Authentication boilerplate
──────────────────────────────────────────────────

Notebook Project (notebook)
Jupyter notebook-based project for experimentation

Features:
  • Jupyter notebooks
  • Example experiments
  • Visualization tools
──────────────────────────────────────────────────

💡 Tip: Create a project with ember project new <name> --template <template>

# Create a new project
$ ember project new my-ember-app --template complete
✅ Initializing project
✅ Installing dependencies
✅ Setting up configuration
✅ Finalizing project
✅ Project my-ember-app created successfully in my-ember-app

To get started, run:
  cd my-ember-app
  uv pip install -e "."

💡 Tip: Set up your API keys as environment variables or configure them with ember provider configure <provider>

# Analyze a project
$ ember project analyze my-ember-app

▸ Project Analysis: my-ember-app

Project Type: Complete Project
Directory: /Users/username/my-ember-app

Project Structure:
  Operators: 2
  Models: 1
  Data Sources: 1
  Test Files: 3

Dependencies:
  Required: ember-ai, openai, pandas
  All dependencies satisfied

Configuration:
  Providers: openai
  Status: Configured

Suggestions:
  • Add automated tests for your operators
  • Consider implementing usage tracking
  • Update to the latest Ember version
```

### Managing Configuration

```bash
# List configuration
$ ember config list

▸ Ember CLI Configuration

Default Provider: openai
Default Model: openai:gpt-4o
Usage Tracking: Enabled

Configured Providers:
  ✓ openai
   anthropic

💡 Tip: Set a configuration value with ember config set <key> <value>

# Set configuration
$ ember config set defaultModel openai:gpt-4o-mini
✅ Configuration value for defaultModel set to "openai:gpt-4o-mini"

# Export configuration
$ ember config export ember-config.json
✅ Configuration exported to /Users/username/ember-config.json
⚠️ This file contains sensitive information such as API keys.

# Reset configuration
$ ember config reset
This will reset all configuration to defaults. Continue? (y/N) y
✅ Configuration reset to defaults
```

## Environment Variables

The CLI respects the following environment variables:

| Variable                 | Description                             |
| ------------------------ | --------------------------------------- |
| `OPENAI_API_KEY`         | OpenAI API key                          |
| `ANTHROPIC_API_KEY`      | Anthropic API key                       |
| `GOOGLE_API_KEY`         | Google API key                          |
| `EMBER_DEFAULT_PROVIDER` | Default provider                        |
| `EMBER_DEFAULT_MODEL`    | Default model                           |
| `EMBER_NO_COLOR`         | Disable colors when set to any value    |
| `EMBER_DEBUG`            | Enable debug mode when set to any value |

## Extending the CLI

The Ember CLI is designed to be extensible. You can create plugins and extensions by following our plugin development guidelines.

## Troubleshooting

### Common Issues

1. **Python Bridge Errors**

   - Ensure Python 3.11+ is installed and in your PATH
   - Verify Ember AI is installed (`uv pip list | grep ember-ai`)

2. **API Key Issues**

   - Check provider configuration (`ember config list`)
   - Verify environment variables are set correctly

3. **Performance Problems**
   - For large responses, use the `--output` option to save to file
   - Consider using streaming with `--stream` option

### Getting Help

```bash
# Get general help
ember help

# Get help for a specific command
ember provider --help
```

## Advanced Topics

### Scripting with the CLI

The `--json` flag makes the CLI suitable for scripting:

```bash
# Get models as JSON and filter with jq
models=$(ember model list --json | jq '.models[]' | grep 'gpt-4')

# Use in shell scripts
for model in $models; do
  echo "Testing $model"
  ember invoke --model $model --prompt "Hello" --json > results_$model.json
done
```

### Using Configuration Files

You can use configuration files to manage settings across environments:

```bash
# Export production configuration
ember config export prod-config.json

# Import configuration on another machine
ember config import prod-config.json

# Use different configurations for different projects
cd project1
ember config import project1-config.json
```

## Best Practices

1. **Security**

   - Never commit configuration files with API keys
   - Use environment variables when possible
   - Rotate API keys regularly

2. **Usage**

   - Set default provider and model to reduce typing
   - Use templates for consistent project structure
   - Leverage the `invoke` command for quick experiments

3. **Performance**
   - Use streaming for large responses
   - Consider model benchmark results for latency-sensitive applications
   - Start with smaller, more efficient models when appropriate
     </code>

docs\cli\SHELL_COMPLETION.md:
<code>

# Ember CLI Shell Completion

Ember CLI provides powerful shell completion for Bash, Zsh, Fish, and PowerShell. This feature gives you tab completion for commands, options, and arguments, making the CLI much more user-friendly and efficient.

## Completion Features

The shell completion provides context-aware suggestions for:

- Commands and subcommands
- Options and flags
- Provider names
- Model IDs
- Project templates
- File and directory paths
- Configuration keys

## Installation

### Quick Installation

The easiest way to install completion is to use the built-in installation command:

```bash
# Install for the current shell
ember completion install

# Force overwrite of existing completion
ember completion install --force
```

### Manual Installation

You can also generate the completion script and install it manually:

#### Bash

```bash
# Generate completion script
ember completion bash > ~/.bash_completion.d/ember

# Add to your ~/.bashrc
echo 'source ~/.bash_completion.d/ember' >> ~/.bashrc
```

#### Zsh

```bash
# Create completions directory if it doesn't exist
mkdir -p ~/.zsh/completion

# Generate completion script
ember completion zsh > ~/.zsh/completion/_ember

# Add to your ~/.zshrc
echo 'fpath=(~/.zsh/completion $fpath)' >> ~/.zshrc
echo 'autoload -U compinit' >> ~/.zshrc
echo 'compinit' >> ~/.zshrc
```

#### Fish

```bash
# Generate completion script
ember completion fish > ~/.config/fish/completions/ember.fish
```

#### PowerShell

```powershell
# Generate completion script
ember completion powershell > $PROFILE.CurrentUserAllHosts
```

## Usage Examples

Once installed, you can use tab completion throughout the Ember CLI:

### Command Completion

```bash
# Press TAB to see available commands
ember [TAB]
# Output: version  provider  model  invoke  project  config  completion

# Press TAB to see subcommands
ember provider [TAB]
# Output: list  configure  info  use
```

### Option Completion

```bash
# Press TAB to see available options
ember invoke --[TAB]
# Output: --model  --prompt  --file  --system  --temperature  --show-usage  --output  --raw  --stream

# Complete option values
ember model list --provider [TAB]
# Output: openai  anthropic  google  ibm
```

### Dynamic Completion

The completion system can fetch data from your Ember installation to provide dynamic suggestions:

```bash
# Complete provider names
ember provider configure [TAB]
# Output will list available providers

# Complete model IDs
ember invoke --model [TAB]
# Output will list available models
```

## Troubleshooting

If you encounter issues with completion:

1. Make sure your shell startup files are properly configured
2. Try reinstalling completion with `ember completion install --force`
3. Check that the completion script is being sourced properly
4. Restart your shell or terminal

### Common Issues

- **No completion in Bash**: Check that your `~/.bash_completion.d/ember` file exists and is sourced in your `~/.bashrc`
- **No completion in Zsh**: Check that `fpath` is properly configured in your `~/.zshrc`
- **Outdated completions**: Regenerate completions after updating Ember CLI

## How It Works

The shell completion is implemented using:

- Command metadata declared in TypeScript interfaces
- Shell-specific generators that create native completion scripts
- Dynamic data sourcing from the Ember backend
- Context-aware argument parsing

The system follows a declarative approach, where commands and their arguments are defined once and specialized generators produce shell-specific implementations.

## For Developers

If you're extending the CLI with new commands, you should update the command specifications in `src/cli/utils/completion.ts` to ensure proper completion support.

The completion system follows SOLID principles, particularly:

- **Single Responsibility**: Each generator is responsible for one shell
- **Open/Closed**: You can add new shell support without modifying existing code
- **Liskov Substitution**: All generators implement the same interface
- **Interface Segregation**: Clean separation of concerns
- **Dependency Inversion**: High-level completion logic does not depend on shell-specific details
  </code>

docs\design\dataset_implementation_plan.md:
<code>

# Dataset Implementation Plan: AIME 2024, GPQA Diamond, and Codeforces

This document outlines the implementation plan for integrating three new datasets into the Ember framework: AIME 2024 (Competition Math), GPQA Diamond (PhD-Level Science Questions), and Codeforces (Competitive Programming Problems).

## Table of Contents

1. [Overview](#1-overview)
2. [Implementation Structure](#2-implementation-structure)
3. [Detailed Implementation Plan](#3-detailed-implementation-plan)
   - [3.1. AIME 2024](#31-aime-2024-competition-math)
   - [3.2. GPQA Diamond](#32-gpqa-diamond-phd-level-science-questions)
   - [3.3. Codeforces](#33-codeforces-competitive-programming-problems)
4. [Implementation Steps](#4-implementation-steps)
5. [Testing Strategy](#5-testing-strategy)
6. [Implementation Details](#6-implementation-details)
   - [6.1. AIME 2024 Implementation](#61-aime-2024-implementation)
   - [6.2. GPQA Diamond Implementation](#62-gpqa-diamond-implementation)
   - [6.3. Codeforces Implementation](#63-codeforces-implementation)
   - [6.4. Custom Evaluator for Codeforces](#64-custom-evaluator-for-codeforces)
7. [Integration Examples](#7-integration-examples)
8. [Documentation](#8-documentation)
9. [Code Style and Engineering Quality](#9-code-style-and-engineering-quality)
10. [Timeline and Milestones](#10-timeline-and-milestones)
11. [Future Extensions](#11-future-extensions)
12. [Implementation Progress](#12-implementation-progress)

## 1. Overview

This plan details the integration of three datasets into Ember, following the architectural patterns and coding standards already established in the codebase. Each dataset will have its own dedicated implementation in the datasets registry, with appropriate preppers, configuration, and evaluation mechanisms.

## 2. Implementation Structure

For each dataset, we'll create:

1. **Dataset Prepper**: A class that transforms raw dataset entries into Ember's standardized `DatasetEntry` format
2. **Dataset Configuration** (if needed): For datasets with configurable options
3. **Dataset Registration**: Integration with Ember's registration system
4. **Evaluation Strategy**: Implementation of appropriate evaluators for each dataset type
5. **Documentation**: Comprehensive docstrings and comments following the Jeff Dean/Sanjay Ghemawat style

## 3. Detailed Implementation Plan

### 3.1. AIME 2024 (Competition Math)

#### Implementation Files

1. Create `src/ember/core/utils/data/datasets_registry/aime.py`

#### AIME Dataset Structure

```python
class AIMEConfig(BaseDatasetConfig):
    """Configuration for the AIME dataset.

    Allows loading either all problems or specific years.
    """
    year: Optional[int] = None  # Specific year filter (2024 by default)
    contest: Optional[str] = None  # 'I' or 'II' for specific contest

class AIMEPrepper(IDatasetPrepper):
    """Prepares AIME competition math problems.

    Transforms HuggingFace AIME dataset entries into DatasetEntry format.
    """
    # Implementation will handle processing LaTeX-formatted problems

# Registration
register_metadata(
    name="aime",
    description="American Invitational Mathematics Examination questions",
    source="Maxwell-Jia/AIME_2024",
    task_type=TaskType.SHORT_ANSWER,
    prepper_class=AIMEPrepper,
)
```

#### Evaluation Implementation

```python
class NumericAnswerEvaluator(IEvaluator[str, str]):
    """Evaluator for numeric answers that handles exact integer matching.

    Extracts numeric values from model responses and compares with reference.
    """
    # Implementation will handle integer comparison for AIME answers
```

### 3.2. GPQA Diamond (PhD-Level Science Questions)

#### Implementation Files

1. Create `src/ember/core/utils/data/datasets_registry/gpqa.py`

#### GPQA Dataset Structure

```python
class GPQAConfig(BaseDatasetConfig):
    """Configuration for the GPQA dataset.

    Allows loading specifically the Diamond subset or other configurations.
    """
    subset: str = "gpqa_diamond"  # Default to the diamond subset
    difficulty: Optional[str] = None  # Optional filter by difficulty

class GPQAPrepper(IDatasetPrepper):
    """Prepares GPQA science questions.

    Transforms HuggingFace GPQA dataset entries into DatasetEntry format.
    """
    # Implementation will handle multiple-choice structure

# Registration
register_metadata(
    name="gpqa",
    description="Graduate-level PhD science questions (physics, chemistry, biology)",
    source="Idavidrein/gpqa",
    task_type=TaskType.MULTIPLE_CHOICE,
    prepper_class=GPQAPrepper,
)
```

#### Evaluation Implementation

Will use existing `ExactMatchEvaluator` for multiple-choice evaluation.

### 3.3. Codeforces (Competitive Programming Problems)

#### Implementation Files

1. Create `src/ember/core/utils/data/datasets_registry/codeforces.py`
2. Create supporting evaluation classes in `src/ember/core/utils/eval/code_execution.py`

#### Codeforces Dataset Structure

```python
class CodeForcesConfig(BaseDatasetConfig):
    """Configuration for the Codeforces dataset.

    Allows filtering by difficulty, tags, or other attributes.
    """
    difficulty_range: Optional[Tuple[int, int]] = None  # Range of difficulty ratings
    tags: Optional[List[str]] = None  # Problem tags for filtering
    limit: Optional[int] = None  # Max number of problems to include

class CodeForcesPrepper(IDatasetPrepper):
    """Prepares Codeforces programming problems.

    Transforms HuggingFace Codeforces dataset entries into DatasetEntry format.
    """
    # Implementation will handle problem statement, input specs, test cases

# Registration
register_metadata(
    name="codeforces",
    description="Competitive programming problems from Codeforces",
    source="open-r1/codeforces",
    task_type=TaskType.CODE_COMPLETION,
    prepper_class=CodeForcesPrepper,
)
```

#### Evaluation Implementation

```python
class CodeCompetitionEvaluator(IEvaluator[str, Dict[str, Any]]):
    """Evaluator for competitive programming problems.

    Executes student code against multiple test cases with safety controls.
    Handles input/output format validation and time/space complexity analysis.
    """
    # Implementation with sandboxed code execution and test case validation
```

## 4. Implementation Steps

1. **Setup & Structure** (1 day)

   - Create skeleton files for each dataset
   - Define interfaces and base classes
   - Create test scaffolding

2. **AIME 2024 Implementation** (1 day)

   - Implement AIMEConfig and AIMEPrepper
   - Add registration code
   - Create evaluator for math answers
   - Add tests

3. **GPQA Diamond Implementation** (1 day)

   - Implement GPQAConfig and GPQAPrepper
   - Add registration code
   - Test with existing MCQ evaluators
   - Add tests

4. **Codeforces Implementation** (2 days)

   - Implement CodeForcesConfig and CodeForcesPrepper
   - Create code execution evaluator with safety controls
   - Integrate test case validation
   - Add tests

5. **Integration & Testing** (1 day)

   - End-to-end tests for all three datasets
   - Performance optimization
   - Documentation review

6. **Example Notebooks & Documentation** (1 day)
   - Create example usage notebooks
   - Add comprehensive documentation
   - Update API references

## 5. Testing Strategy

For each dataset:

1. **Unit Tests**

   - Test prepper functionality in isolation
   - Test evaluator functionality in isolation
   - Test configuration validation

2. **Integration Tests**

   - End-to-end dataset loading tests
   - Evaluation pipeline tests

3. **Performance Tests**
   - Benchmark dataset loading speeds
   - Optimize for large datasets

## 6. Implementation Details

### 6.1. AIME 2024 Implementation

```python
from typing import Any, Dict, List, Optional

from ember.core.utils.data.base.config import BaseDatasetConfig
from ember.core.utils.data.base.models import DatasetEntry
from ember.core.utils.data.base.preppers import IDatasetPrepper
from ember.core.utils.data.registry import register_metadata
from ember.core.utils.data.base.models import TaskType


class AIMEConfig(BaseDatasetConfig):
    """Configuration for the AIME dataset.

    Controls filtering and loading options for the AIME math competition dataset.
    """
    year: Optional[int] = 2024  # Default to 2024
    contest: Optional[str] = None  # 'I' or 'II' for specific contest


class AIMEPrepper(IDatasetPrepper):
    """Prepper for AIME competition math problems.

    Transforms the HuggingFace AIME dataset entries into Ember's DatasetEntry format,
    handling LaTeX-formatted math problems and numeric answers.
    """

    def __init__(self, config: Optional[Any] = None) -> None:
        """Initialize the AIME prepper with optional configuration.

        Args:
            config: Either a string (year), AIMEConfig instance, or None.
                   If None, defaults to all 2024 problems.
        """
        if isinstance(config, str) and config.isdigit():
            config = AIMEConfig(year=int(config))
        elif config is None:
            config = AIMEConfig()
        super().__init__(config)
        self.year = self._config.year
        self.contest = self._config.contest

    def get_required_keys(self) -> List[str]:
        """Return required keys for AIME dataset items.

        Returns:
            List of required fields: ID, Problem, Answer
        """
        return ["ID", "Problem", "Answer"]

    def create_dataset_entries(self, *, item: Dict[str, Any]) -> List[DatasetEntry]:
        """Create a DatasetEntry from an AIME problem.

        Args:
            item: Raw dataset item containing ID, Problem, Answer and Solution.

        Returns:
            DatasetEntry with the problem as query and answer in metadata.
        """
        problem_id = str(item["ID"])
        problem_text = str(item["Problem"])
        answer = str(item["Answer"])
        solution = item.get("Solution", "")

        # Filter by year/contest if specified
        if self.year or self.contest:
            # AIME IDs have format "YYYY-C-N" where C is contest (I/II) and N is problem number
            parts = problem_id.split("-")
            if len(parts) >= 3:
                id_year = int(parts[0]) if parts[0].isdigit() else None
                id_contest = parts[1]

                if (self.year and id_year != self.year) or \
                   (self.contest and id_contest != self.contest):
                    return []  # Skip this problem

        return [
            DatasetEntry(
                query=problem_text,
                choices={},  # No choices for short answer problems
                metadata={
                    "correct_answer": answer,
                    "solution": solution,
                    "problem_id": problem_id,
                }
            )
        ]


# Register the AIME dataset
register_metadata(
    name="aime",
    description="American Invitational Mathematics Examination problems",
    source="Maxwell-Jia/AIME_2024",
    task_type=TaskType.SHORT_ANSWER,
    prepper_class=AIMEPrepper,
)
```

### 6.2. GPQA Diamond Implementation

```python
from typing import Any, Dict, List, Optional

from ember.core.utils.data.base.config import BaseDatasetConfig
from ember.core.utils.data.base.models import DatasetEntry
from ember.core.utils.data.base.preppers import IDatasetPrepper
from ember.core.utils.data.registry import register_metadata
from ember.core.utils.data.base.models import TaskType


class GPQAConfig(BaseDatasetConfig):
    """Configuration for the GPQA dataset.

    Controls loading options for the GPQA PhD-level science questions.
    """
    subset: str = "gpqa_diamond"  # Default to Diamond subset
    difficulty: Optional[str] = None  # Optional filter by difficulty level


class GPQAPrepper(IDatasetPrepper):
    """Prepper for GPQA Diamond science questions.

    Transforms HuggingFace GPQA dataset entries into DatasetEntry format,
    handling the multiple-choice structure with correct and incorrect answers.
    """

    def __init__(self, config: Optional[Any] = None) -> None:
        """Initialize the GPQA prepper with optional configuration.

        Args:
            config: Either a string (subset name), GPQAConfig instance, or None.
                   If None, defaults to Diamond subset.
        """
        if isinstance(config, str):
            config = GPQAConfig(subset=config)
        elif config is None:
            config = GPQAConfig()
        super().__init__(config)
        self.subset = self._config.subset
        self.difficulty = self._config.difficulty

    def get_required_keys(self) -> List[str]:
        """Return required keys for GPQA dataset items.

        Returns:
            List of required fields for processing.
        """
        return ["question", "correct_answer", "incorrect_answers"]

    def create_dataset_entries(self, *, item: Dict[str, Any]) -> List[DatasetEntry]:
        """Create a DatasetEntry from a GPQA question.

        Args:
            item: Raw dataset item containing question text and answers.

        Returns:
            DatasetEntry with the question as query and answer choices.
        """
        question = str(item["question"])
        correct_answer = str(item["correct_answer"])
        incorrect_answers = [str(ans) for ans in item["incorrect_answers"]]

        # Apply difficulty filter if specified
        if self.difficulty and item.get("difficulty") != self.difficulty:
            return []  # Skip this question

        # Create choices dictionary with letter keys (A, B, C, D)
        all_choices = [correct_answer] + incorrect_answers
        choices = {chr(65 + i): choice for i, choice in enumerate(all_choices)}

        # The correct answer is always the first one (A)
        return [
            DatasetEntry(
                query=question,
                choices=choices,
                metadata={
                    "correct_answer": "A",  # The first choice is always correct in our formatting
                    "subject": item.get("subject", ""),
                    "difficulty": item.get("difficulty", ""),
                }
            )
        ]


# Register the GPQA dataset
register_metadata(
    name="gpqa",
    description="Graduate-level PhD science questions (GPQA Diamond subset)",
    source="Idavidrein/gpqa",
    task_type=TaskType.MULTIPLE_CHOICE,
    prepper_class=GPQAPrepper,
)
```

### 6.3. Codeforces Implementation

```python
from typing import Any, Dict, List, Optional, Tuple

from ember.core.utils.data.base.config import BaseDatasetConfig
from ember.core.utils.data.base.models import DatasetEntry
from ember.core.utils.data.base.preppers import IDatasetPrepper
from ember.core.utils.data.registry import register_metadata
from ember.core.utils.data.base.models import TaskType


class CodeForcesConfig(BaseDatasetConfig):
    """Configuration for the Codeforces dataset.

    Controls filtering and loading options for competitive programming problems.
    """
    difficulty_range: Optional[Tuple[int, int]] = None  # Min/max difficulty
    tags: Optional[List[str]] = None  # Problem tags for filtering
    limit: Optional[int] = None  # Max number of problems


class CodeForcesPrepper(IDatasetPrepper):
    """Prepper for Codeforces programming problems.

    Transforms HuggingFace Codeforces dataset entries into DatasetEntry format,
    handling problem statements, input specifications, and test cases.
    """

    def __init__(self, config: Optional[Any] = None) -> None:
        """Initialize the Codeforces prepper with optional configuration.

        Args:
            config: CodeForcesConfig instance or None for defaults.
        """
        if config is None:
            config = CodeForcesConfig()
        super().__init__(config)
        self.difficulty_range = self._config.difficulty_range
        self.tags = self._config.tags
        self.limit = self._config.limit
        self.problem_count = 0

    def get_required_keys(self) -> List[str]:
        """Return required keys for Codeforces dataset items.

        Returns:
            List of required fields for processing.
        """
        return ["name", "description", "input_spec", "output_spec", "samples"]

    def create_dataset_entries(self, *, item: Dict[str, Any]) -> List[DatasetEntry]:
        """Create a DatasetEntry from a Codeforces problem.

        Args:
            item: Raw dataset item containing problem details.

        Returns:
            DatasetEntry with the problem as query and test cases in metadata.
        """
        # Apply filters
        if self.limit and self.problem_count >= self.limit:
            return []  # Skip if we've reached the limit

        difficulty = item.get("rating")
        if self.difficulty_range and difficulty:
            min_diff, max_diff = self.difficulty_range
            if difficulty < min_diff or difficulty > max_diff:
                return []  # Skip if outside difficulty range

        problem_tags = item.get("tags", [])
        if self.tags and not any(tag in problem_tags for tag in self.tags):
            return []  # Skip if no matching tags

        # Extract problem components
        name = str(item["name"])
        description = str(item["description"])
        input_spec = str(item["input_spec"])
        output_spec = str(item["output_spec"])
        samples = item["samples"]

        # Format the complete problem statement
        problem_statement = f"""# {name}

## Problem Description
{description}

## Input Specification
{input_spec}

## Output Specification
{output_spec}

## Examples
"""
        # Add sample test cases
        for i, sample in enumerate(samples, 1):
            problem_statement += f"""
### Sample {i}
#### Input:
```

{sample['input']}

```

#### Output:
```

{sample['output']}

```
"""

        # Increment counter for limit tracking
        self.problem_count += 1

        return [
            DatasetEntry(
                query=problem_statement,
                choices={},  # No choices for code problems
                metadata={
                    "name": name,
                    "difficulty": difficulty,
                    "tags": problem_tags,
                    "problem_id": item.get("problem_id", ""),
                    "test_cases": samples,
                }
            )
        ]


# Register the Codeforces dataset
register_metadata(
    name="codeforces",
    description="Competitive programming problems from Codeforces",
    source="open-r1/codeforces",
    task_type=TaskType.CODE_COMPLETION,
    prepper_class=CodeForcesPrepper,
)
```

### 6.4. Custom Evaluator for Codeforces

```python
import json
import os
import subprocess
import tempfile
import time
from typing import Any, Dict, List, Optional

from ember.core.utils.eval.base_evaluator import EvaluationResult, IEvaluator


class CodeCompetitionEvaluator(IEvaluator[str, Dict[str, Any]]):
    """Evaluator for competitive programming problems.

    Executes submitted code against multiple test cases in a controlled
    environment, with time limits and memory monitoring.
    """

    def __init__(
        self,
        time_limit: float = 2.0,
        memory_limit_mb: int = 512,
        languages: Optional[List[str]] = None
    ) -> None:
        """Initialize the code competition evaluator.

        Args:
            time_limit: Maximum execution time per test case (seconds)
            memory_limit_mb: Maximum memory usage allowed (MB)
            languages: List of supported languages, defaults to ["python"]
        """
        self.time_limit = time_limit
        self.memory_limit_mb = memory_limit_mb
        self.languages = languages or ["python"]

    def evaluate(
        self, system_output: str, correct_answer: Dict[str, Any], **kwargs: Any
    ) -> EvaluationResult:
        """Evaluate generated code against test cases.

        Args:
            system_output: Generated code solution
            correct_answer: Dictionary containing test cases and metadata
            **kwargs: Additional parameters including language

        Returns:
            EvaluationResult with test case results and execution metrics
        """
        language = kwargs.get("language", "python")
        if language not in self.languages:
            return EvaluationResult(
                is_correct=False,
                score=0.0,
                metadata={"error": f"Unsupported language: {language}"}
            )

        test_cases = correct_answer.get("test_cases", [])
        if not test_cases:
            return EvaluationResult(
                is_correct=False,
                score=0.0,
                metadata={"error": "No test cases available"}
            )

        # Create a temporary directory for code execution
        with tempfile.TemporaryDirectory() as temp_dir:
            # Write the code to a file
            code_file = os.path.join(temp_dir, f"solution.{self._get_extension(language)}")
            with open(code_file, "w") as f:
                f.write(system_output)

            # Run each test case
            test_results = []
            passed_count = 0

            for i, test in enumerate(test_cases):
                test_input = test.get("input", "")
                expected_output = test.get("output", "").strip()

                result = self._run_test_case(
                    code_file=code_file,
                    language=language,
                    test_input=test_input,
                    expected_output=expected_output
                )

                if result["passed"]:
                    passed_count += 1

                test_results.append({
                    "test_index": i,
                    "passed": result["passed"],
                    "execution_time": result["execution_time"],
                    "memory_used_mb": result.get("memory_used_mb"),
                    "error": result.get("error"),
                })

        # Calculate overall score based on passed test cases
        total_tests = len(test_cases)
        score = passed_count / total_tests if total_tests > 0 else 0.0
        is_correct = passed_count == total_tests

        return EvaluationResult(
            is_correct=is_correct,
            score=score,
            metadata={
                "test_results": test_results,
                "passed_count": passed_count,
                "total_tests": total_tests,
            }
        )

    def _get_extension(self, language: str) -> str:
        """Get file extension for language.

        Args:
            language: Programming language

        Returns:
            File extension string
        """
        extensions = {
            "python": "py",
            "c++": "cpp",
            "java": "java",
        }
        return extensions.get(language, "txt")

    def _run_test_case(
        self,
        code_file: str,
        language: str,
        test_input: str,
        expected_output: str
    ) -> Dict[str, Any]:
        """Run a single test case for the submitted code.

        Args:
            code_file: Path to the source code file
            language: Programming language
            test_input: Input for the test case
            expected_output: Expected output

        Returns:
            Dictionary with test results
        """
        command = self._get_run_command(language, code_file)

        try:
            start_time = time.time()

            process = subprocess.Popen(
                command,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )

            stdout, stderr = process.communicate(
                input=test_input,
                timeout=self.time_limit
            )

            execution_time = time.time() - start_time

            # Compare output (ignoring whitespace differences)
            actual_output = stdout.strip()
            passed = actual_output == expected_output

            return {
                "passed": passed,
                "execution_time": execution_time,
                "actual_output": actual_output,
                "stderr": stderr,
                "exit_code": process.returncode,
            }

        except subprocess.TimeoutExpired:
            return {
                "passed": False,
                "execution_time": self.time_limit,
                "error": "Time limit exceeded",
            }
        except Exception as e:
            return {
                "passed": False,
                "execution_time": 0,
                "error": str(e),
            }

    def _get_run_command(self, language: str, code_file: str) -> List[str]:
        """Get the command to run code in the specified language.

        Args:
            language: Programming language
            code_file: Path to source code

        Returns:
            Command list for subprocess
        """
        if language == "python":
            return ["python", code_file]
        elif language == "c++":
            executable = code_file.replace(".cpp", ".exe")
            return ["g++", code_file, "-o", executable, "&&", executable]
        elif language == "java":
            return ["java", code_file]
        else:
            raise ValueError(f"Unsupported language: {language}")
```

## 7. Integration Examples

### 7.1. Basic Usage Example

```python
from ember.api import datasets, models

# Load and explore AIME dataset
aime_data = datasets("aime")
print(f"AIME dataset size: {len(aime_data)}")

# Get first AIME problem
aime_problem = aime_data[0]
print(f"Problem: {aime_problem.query}")

# Test a model on an AIME problem
model = models.anthropic.claude35_sonnet()
response = model(aime_problem.query)

# Evaluate the response with the proper evaluator
from ember.core.utils.eval.evaluators import NumericToleranceEvaluator
evaluator = NumericToleranceEvaluator()
result = evaluator.evaluate(response, aime_problem.metadata["correct_answer"])
print(f"Evaluation result: {result}")
```

### 7.2. Advanced Usage with Builder Pattern

```python
from ember.api import DatasetBuilder, EvaluationPipeline, models

# Create a dataset with filtered problems
gpqa_dataset = (
    DatasetBuilder()
    .from_registry("gpqa")  # Use registered GPQA dataset
    .sample(10)  # Get 10 random problems
    .transform(lambda entry: {
        **entry,
        "query": f"Choose the correct answer to this PhD-level question:\n\n{entry['query']}"
    })
    .build()
)

# Create an evaluation pipeline
pipeline = EvaluationPipeline()
pipeline.add_model(models.openai.gpt4o())
pipeline.add_model(models.anthropic.claude35_sonnet())

# Run evaluation
results = pipeline.evaluate(gpqa_dataset)
print(results.summary())
```

## 8. Documentation

### 8.1. README.md for Datasets

````markdown
# Ember Datasets

Ember provides access to a diverse set of evaluation datasets through a unified interface. Here's how to use the recently added datasets:

## AIME 2024 (Competition Math)

The AIME dataset contains 30 challenging problems from the American Invitational Mathematics Examination.

```python
# Load AIME dataset (all problems from 2024)
from ember.api import datasets
aime_data = datasets("aime")

# Load only AIME I contest problems
from ember.api import DatasetBuilder
aime_data = DatasetBuilder().from_registry("aime").configure(contest="I").build()
```
````

## GPQA Diamond (PhD-Level Science)

GPQA Diamond contains 198 graduate-level multiple-choice science questions spanning physics, chemistry, and biology.

```python
# Load GPQA Diamond dataset
gpqa_data = datasets("gpqa")

# Filter by subject area
gpqa_physics = DatasetBuilder().from_registry("gpqa").filter(
    lambda entry: "physics" in entry.metadata.get("subject", "").lower()
).build()
```

## Codeforces (Competitive Programming)

The Codeforces dataset contains competitive programming problems from the Codeforces platform.

```python
# Load Codeforces problems with difficulty range 800-1200
codeforces_data = DatasetBuilder().from_registry("codeforces").configure(
    difficulty_range=(800, 1200), limit=10
).build()
```

````

## 9. Code Style and Engineering Quality

Throughout implementation, we'll adhere to the highest code quality standards:

1. **Clean, Minimalist Code**
   - Precise, concise implementations
   - No unnecessary abstractions
   - Efficient algorithms and data structures

2. **Type Safety**
   - Comprehensive type annotations
   - Proper use of generics
   - Validation of inputs and outputs

3. **Documentation**
   - Thorough, accurate docstrings
   - Purpose, behavior, parameters, returns, exceptions
   - Clear examples

4. **Error Handling**
   - Specific, actionable error messages
   - Graceful degradation
   - Precise exception types

5. **Testing**
   - Comprehensive unit tests
   - Integration tests
   - Property-based tests for evaluators

## 10. Timeline and Milestones

1. **Day 1-2: Core Implementation**
   - Create and implement skeleton classes
   - Set up registration system
   - Implement dataset loaders

2. **Day 3-4: Evaluators and Testing**
   - Implement specialized evaluators
   - Create test suite
   - Performance optimization

3. **Day 5: Integration and Documentation**
   - End-to-end integration
   - Documentation
   - Example notebooks

## 11. Future Extensions

1. **Enhanced Evaluation Metrics**
   - Partial credit for AIME problems
   - Runtime efficiency scoring for Codeforces
   - Domain-specific analysis for GPQA

2. **UI Integration**
   - Interactive visualization of results
   - Leaderboard comparisons

3. **Expanded Dataset Coverage**
   - Additional AIME years
   - GPQA full dataset
   - More competitive programming platforms

## 12. Implementation Progress

### Implementation Checklist

- [x] **Setup & Structure**
  - [x] Create directory structure
  - [x] Set up test fixtures
  - [x] Create skeleton files

- [x] **AIME 2024**
  - [x] Implement AIMEConfig
  - [x] Implement AIMEPrepper (fully)
  - [x] Create AIMEAnswerEvaluator
  - [x] Write unit tests
  - [x] Create standalone test for validation

- [ ] **GPQA Diamond**
  - [x] Implement GPQAConfig (skeleton)
  - [ ] Implement GPQAPrepper (fully)
  - [ ] Configure MCQ evaluator
  - [x] Write unit tests (skeleton)

- [ ] **Codeforces**
  - [x] Implement CodeForcesConfig (skeleton)
  - [ ] Implement CodeForcesPrepper (fully)
  - [x] Create CodeCompetitionEvaluator (skeleton)
  - [ ] Implement sandboxed execution
  - [x] Write unit tests (skeleton)

- [x] **Documentation**
  - [x] Create comprehensive guide for adding datasets
  - [x] Document architectural patterns
  - [x] Include integration instructions
  - [x] Create example usage code

- [ ] **Final Integration**
  - [ ] Update central registry
  - [ ] Add evaluator exports
  - [ ] End-to-end validation
  - [ ] Performance testing

### Revised Implementation Strategy

We've updated our approach based on a deeper understanding of Ember's architecture:

1. **Dataset Implementation**
   - Implement dataset prepper classes with full functionality following Ember patterns
   - Keep all implementation focused only on data transformation
   - Remove any direct registration code from dataset files

2. **Central Registration**
   - Update `initialize_registry()` in `registry.py` to include our new datasets
   - Add proper imports for all dataset modules
   - Follow existing patterns for registration using `UNIFIED_REGISTRY.register_metadata()`

3. **Evaluators Implementation**
   - Implement evaluators for each dataset type
   - Integration with existing evaluator interfaces
   - Unit tests for evaluator correctness

4. **Documentation**
   - Added comprehensive guide on adding new datasets
   - Document architectural patterns and best practices
   - Create example usage notebooks

5. **Testing Strategy**
   - Comprehensive unit tests for prepper classes
   - Integration tests verifying end-to-end functionality
   - Performance testing for large datasets
</code>

docs\design\enhanced_jit_design_doc.md:
<code>
# Enhanced JIT API for Ember

## Overview

This document describes the design for an enhanced JIT (Just-In-Time) compilation API for Ember, providing a cleaner, more JAX-like user experience for building and executing complex operator DAGs.

> **Note**: This document describes the JIT compilation vision for Ember. The current implementation includes three complementary approaches:
>
> 1. **autograph** - A context manager for explicit graph building when maximum control is needed
> 2. **jit** - A decorator that traces execution and automatically builds graphs from observed behavior
> 3. **structural_jit** - An advanced decorator that analyzes operator composition without execution tracing
>
> Together, these provide a JAX-like experience where operators can be composed naturally with automatic parallelization. The system handles most common use cases, with ongoing development of advanced features like complete transforms integration and advanced optimization.

## Goals

1. Simplify the user experience when working with complex operator DAGs
2. Eliminate the need for manual graph construction in common cases
3. Enable transparent caching and reuse of execution plans
4. Allow flexible configuration of execution parameters
5. Match the elegance of JAX-like systems while preserving Ember's eager-mode compatible UX

## Implementation Components

### 1. Enhanced JIT Decorator

The core of the enhanced system is an improved `@jit` decorator that:

- Automatically traces operator execution
- Builds execution graphs from traces
- Caches graphs for reuse
- Supports sample inputs for initialization-time tracing

```python
@jit(sample_input={"query": "example"})
class MyOperator(Operator[InputType, OutputType]):
    # Implementation
    ...
````

### 2. Execution Options Context

A thread-local context manager that provides control over execution parameters:

```python
with execution_options(scheduler="parallel", max_workers=10):
    result = jit_op(inputs={"query": "example"})
```

### 3. Graph Builder

Internal utilities that automatically convert traces to execution graphs:

- Discovers dependencies between operators
- Creates efficient execution plans
- Handles both sequential and parallel execution
- Properly manages nested operator relationships
- Supports branching and merging execution patterns

## User Experience

### Basic Usage

```python
# Define a JIT-enabled operator
@jit()
class Ensemble(Operator):
    # ...implementation...
    pass

# Create and use it - no manual graph building required
ensemble = Ensemble(num_units=5, model_name="gpt-4o")
result = ensemble(inputs={"query": "What is machine learning?"})
```

### Composition Patterns

The enhanced API supports three composition patterns:

1. **Nested Pipeline Class**:

```python
@jit()
class Pipeline(Operator):
    def __init__(self):
        self.refiner = QuestionRefinement()
        self.ensemble = Ensemble()
        self.aggregator = MostCommon()

    def forward(self, inputs):
        refined = self.refiner(inputs)
        answers = self.ensemble(refined)
        return self.aggregator(answers)
```

2. **Functional Composition**:

```python
pipeline = compose(aggregator, compose(ensemble, refiner))
result = pipeline(inputs)
```

3. **Sequential Chaining**:

```python
def pipeline(inputs):
    refined = refiner(inputs)
    answers = ensemble(refined)
    return aggregator(answers)
```

### Execution Control

```python
# Default execution
result = pipeline(inputs)

# With custom execution options
with execution_options(scheduler="topo_sort_parallel_dispatch"):
    result = pipeline(inputs)
```

## Performance Benefits

The enhanced JIT system offers several performance advantages:

1. **Reduced overhead**: Only build graphs once, reuse for subsequent calls
2. **Automatic parallelism**: Intelligently schedule operations in parallel, based on topo sort of Operator DAG.
3. **Optimized memory usage**: Minimize redundant data copying, and optionally cache calls

## Implementation Details

The core implementation accomplishes several key technical goals:

1. **Hierarchical Analysis**: The system builds a hierarchy map to understand parent-child relationships between operators, enabling proper handling of nested execution.

2. **Advanced Dependency Detection**: The dependency analysis algorithm identifies true data dependencies while respecting hierarchical relationships between operators.

3. **Execution Flow Modeling**: The system correctly models complex execution patterns including branching, merging, and nested operator invocations.

4. **Comprehensive Testing**: The implementation includes robust tests for a wide range of execution patterns, ensuring correct behavior in complex scenarios.

## Design Principles

The implementation adheres to several key design principles:

1. **SOLID**: SOLID adherence for modularity and extensibility. People should be able to add custom schedulers.
2. **Minimalism**: Keep the API surface small and focused
3. **Composability**: Enable building gnarly, complex pipelines from simple components
4. **Pythonic**: Follow Python idioms and feel natural to Python developers with an ML research background
5. **Progressive disclosure**: Easy for beginners, whilst powerful for experts (tensegrity)

## Current Status

The implementation now provides:

1. **Complete tracing system**: Records detailed execution information
2. **Sophisticated dependency analysis**: Properly handles nested operators
3. **Advanced graph building**: Constructs execution graphs with correct dependencies
4. **Support for complex patterns**: Handles branching, merging, and nested execution

</code>

docs\models\README.md:
<code>

# Ember Model System

The Ember Model System provides a clean, intuitive interface for interacting with language models from various providers. It follows key design principles:

1. **One obvious way to do things**: A single, consistent entry point to access models
2. **Hide complexity behind simple interfaces**: Clean abstractions for the underlying components
3. **Prioritize common use cases**: Make simple things simple and complex things possible
4. **Explicit dependency management**: First-class support for dependency injection and isolation

## Quick Start

```python
from ember.api import models

# One-shot usage
response = models.model("gpt-4o")("What is the capital of France?")
print(response)  # "Paris is the capital of France."

# Reusable model with configuration
gpt4 = models.model("gpt-4o", temperature=0.7)
response1 = gpt4("Tell me a joke")
response2 = gpt4("Explain quantum computing")

# Provider namespaces for cleaner access
response = models.openai.gpt4o("What is the capital of France?")

# Temporary configuration overrides
with models.configure(temperature=0.2, max_tokens=100):
    response = models.model("gpt-4o")("Write a haiku")
```

## Key Features

### Function-Style API

The API is designed to be more intuitive and natural, using Python's function calling semantics:

```python
# Traditional API
model = ModelAPI("openai:gpt-4o")
response = model.generate(prompt="What is the capital of France?")

# New Function-Style API
response = model("gpt-4o")("What is the capital of France?")
```

### Provider Namespaces

Provider namespaces provide a cleaner way to access models from specific providers:

```python
from ember.api.models import openai, anthropic, deepmind

# OpenAI models
response = openai.gpt4o("What is the capital of France?")

# Anthropic models
response = anthropic.claude("Tell me a joke")

# Deepmind models
response = deepmind.gemini("Explain quantum mechanics")
```

### Response Object

The Response object provides a clean, consistent interface for working with model responses:

```python
# Get response text via string conversion
print(response)  # "Paris is the capital of France."

# Access usage information
print(f"Tokens: {response.usage.total_tokens}")
print(f"Cost: ${response.usage.cost:.6f}")

# Visual representation in notebooks
response.visualize()
```

### Dependency Injection

The API provides first-class support for dependency injection, allowing you to create isolated contexts:

```python
from ember.api.models import ModelContext, ContextConfig, model

# Create a test context
test_context = ModelContext(
    config=ContextConfig(
        api_keys={"openai": "test-key"},
        auto_discover=False
    )
)

# Use the context
test_model = model("gpt-4o", context=test_context)
response = test_model("This is a test")
```

### Complete Function

The `complete()` function provides a convenient way to generate completions in one line:

```python
from ember.api.models import complete

answer = complete(
    "Explain the significance of the year 1969 in space exploration.",
    model="gpt-4o",
    temperature=0.7
)
```

## Advanced Features

### Configuration Management

The API provides multiple layers of configuration management:

```python
# Global configuration
from ember.api.models import config
config.temperature = 0.7

# Instance configuration
gpt4 = model("gpt-4o", temperature=0.7)

# Temporary configuration
with configure(temperature=0.2):
    response = model("gpt-4o")("Write a haiku")

# Call-specific configuration
response = gpt4("Tell me a joke", temperature=0.9)
```

### Custom Contexts

You can create custom contexts with different configurations for testing, production, etc:

```python
# Production context
prod_context = create_context(
    config=ContextConfig(
        api_keys={"openai": os.environ.get("OPENAI_API_KEY")},
        auto_discover=True
    )
)

# Test context
test_context = create_context(
    config=ContextConfig(
        api_keys={"openai": "test-key"},
        auto_discover=False
    )
)

# Use the contexts
prod_model = model("gpt-4o", context=prod_context)
test_model = model("gpt-4o", context=test_context)
```

## Migration Guide

If you're migrating from the previous API, see the [Migration Guide](MIGRATION_GUIDE.md) for details.
</code>

docs\quickstart\adding_datasets.md:
<code>

# Using Specialized Datasets in Ember

This guide demonstrates how to use Ember's specialized datasets for mathematics (AIME), PhD-level science (GPQA), and competitive programming (Codeforces) evaluation.

## Quick Start

```python
from ember.api import datasets, DatasetBuilder, models

# Mathematics: AIME dataset
aime_data = datasets("aime")

# PhD-level science: GPQA dataset (requires HuggingFace authentication)
# First run: huggingface-cli login
try:
    gpqa_data = datasets("gpqa")
except Exception as e:
    print(f"Authentication error: {e}")

# Competitive programming: Codeforces dataset
cf_data = (
    DatasetBuilder()
    .from_registry("codeforces")
    .configure(difficulty_range=(800, 1200))
    .sample(5)
    .build()
)
```

## Dataset Details

### AIME (American Invitational Mathematics Examination)

Mathematical competition problems with clean numerical answers (0-999).

```python
from ember.api import datasets, models
from ember.core.utils.eval.numeric_answer import AIMEAnswerEvaluator

# Load dataset
aime_data = datasets("aime")

# Filter by contest
aime_i_data = (
    DatasetBuilder()
    .from_registry("aime")
    .configure(contest="I")  # I or II
    .build()
)

# Evaluate on single problem
problem = aime_data[0]
model = models.openai.gpt4o()
response = model(problem.query)

evaluator = AIMEAnswerEvaluator()
result = evaluator.evaluate(response, problem.metadata["correct_answer"])
print(f"Correct: {result.is_correct}")
```

### GPQA (Graduate-level Physics Questions and Answers)

PhD-level physics and chemistry questions with multistep reasoning.

```python
from ember.api import datasets, models
from ember.core.utils.eval.evaluators import MultipleChoiceEvaluator

# Authentication required - will raise GatedDatasetAuthenticationError if not authenticated
try:
    gpqa_data = datasets("gpqa")
except Exception as e:
    print(f"Run 'huggingface-cli login' first: {e}")
    # Request access at https://huggingface.co/datasets/Idavidrein/gpqa

# Filter by subject
physics_only = (
    DatasetBuilder()
    .from_registry("gpqa")
    .filter(lambda item: "physics" in item.metadata.get("subject", "").lower())
    .build()
)

# Evaluate
problem = gpqa_data[0] if gpqa_data else None
if problem:
    # Format prompt with choices
    prompt = problem.query + "\n\n"
    for key, choice in problem.choices.items():
        prompt += f"{key}. {choice}\n"

    model = models.openai.gpt4o()
    response = model(prompt)

    evaluator = MultipleChoiceEvaluator()
    result = evaluator.evaluate(response, problem.metadata["correct_answer"])
```

### Codeforces (Competitive Programming)

Algorithmic programming problems with test cases for evaluation.

````python
from ember.api import datasets, models
from ember.core.utils.eval.code_execution import CodeCompetitionEvaluator

# Load by difficulty (rating range)
cf_data = (
    DatasetBuilder()
    .from_registry("codeforces")
    .configure(difficulty_range=(800, 1200))  # Beginner-friendly
    .build()
)

# Example problem evaluation
problem = cf_data[0]
model = models.anthropic.claude_3_opus()
solution = model(f"Solve this programming problem and provide a Python solution:\n\n{problem.query}")

# Extract code from solution
import re
code_match = re.search(r"```python\n(.*?)```", solution, re.DOTALL)
if code_match:
    code = code_match.group(1)

    evaluator = CodeCompetitionEvaluator(language="python")
    result = evaluator.evaluate(code, problem.metadata["test_cases"])
    print(f"Tests passed: {result.is_correct}")
````

## Authentication for Gated Datasets

GPQA requires HuggingFace authentication:

1. **Authenticate** with `huggingface-cli login`
2. **Request access** at https://huggingface.co/datasets/Idavidrein/gpqa
3. **Handle errors** using try/except:

```python
from ember.core.exceptions import GatedDatasetAuthenticationError

try:
    gpqa_data = datasets("gpqa")
except GatedDatasetAuthenticationError as e:
    print(f"Authentication required: {e.recovery_hint}")
    # Will show: "Run `huggingface-cli login` to authenticate..."
except Exception as e:
    print(f"Other error: {e}")
```

## Batch Evaluation Example

Run multiple models on a dataset:

```python
from ember.api import datasets, models
from ember.core.utils.eval.numeric_answer import AIMEAnswerEvaluator

aime_data = datasets("aime")
problems = aime_data.sample(5)  # 5 random problems

model_configs = [
    ("gpt-4o", models.openai.gpt4o()),
    ("claude-3-opus", models.anthropic.claude_3_opus()),
]

evaluator = AIMEAnswerEvaluator()
results = {}

for name, model in model_configs:
    results[name] = []
    for problem in problems:
        response = model(problem.query)
        result = evaluator.evaluate(response, problem.metadata["correct_answer"])
        results[name].append(result.is_correct)

    accuracy = sum(results[name]) / len(results[name])
    print(f"{name}: {accuracy:.2%} accuracy")
```

## Performance Considerations

- **Cache dataset loading**: HuggingFace datasets are cached locally
- **Use sampling** for large datasets: `sample(n)` to work with subset
- **Preload for batch inference**: Fully load datasets before batch evaluation
- **Reuse evaluators**: Create evaluator instances once and reuse them

## Testing Your Setup

Run this to verify your environment is correctly set up:

```bash
uv run python -m ember.examples.data.new_datasets_example --skip-model-calls
```

The script checks dataset availability and attempts to provide helpful error messages if any issues are found.
</code>

docs\quickstart\configuration.md:
<code>

# Ember Configuration System

The Ember configuration system provides a thread-safe way to manage application settings with schema validation and environment variable resolution.

## Core Components

1. **ConfigManager**: Central access point for configuration
2. **EmberConfig**: Pydantic schema defining configuration structure
3. **Loader**: Handles loading and merging configuration from sources

## Basic Usage

```python
from ember.core.config.manager import create_config_manager

# Create a configuration manager
config_manager = create_config_manager()

# Access configuration
config = config_manager.get_config()
auto_discover = config.registry.auto_discover  # or config.model_registry for legacy code
openai_enabled = config.get_provider("openai").enabled
```

## Configuration Sources

Ember looks for configuration in these locations, in order:

1. Custom path provided to `create_config_manager(config_path="path/to/config.yaml")`
2. Path specified in the `EMBER_CONFIG` environment variable
3. `./config.yaml` in the current working directory

## Environment Variables

Configure Ember using environment variables with the `EMBER_` prefix:

```bash
# Set configuration via environment variables
export EMBER_REGISTRY_AUTO_DISCOVER=false
export EMBER_LOGGING_LEVEL=DEBUG
```

Common API keys are automatically loaded from environment:

```bash
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-...
```

You can reference environment variables in YAML configuration:

```yaml
registry:
  providers:
    openai:
      enabled: true
      api_keys:
        default:
          key: "${OPENAI_API_KEY}"
```

## Configuration Schema

The configuration schema includes:

- **registry**: Model registry settings (legacy code may use `model_registry`)
  - **auto_discover**: Whether to auto-discover models
  - **auto_register**: Whether to auto-register discovered models
  - **providers**: Provider-specific settings
    - **enabled**: Whether the provider is enabled
    - **api_keys**: API keys for the provider
    - **models**: Model-specific settings
      - **id**: Unique model identifier
      - **name**: Display name for the model
      - **cost_input**: Cost per 1000 input tokens
      - **cost_output**: Cost per 1000 output tokens
      - **rate_limit**: Rate limiting configuration
- **logging**: Logging settings
  - **level**: Logging level

Example configuration:

```yaml
registry:
  auto_discover: true
  providers:
    openai:
      enabled: true
      api_keys:
        default:
          key: "${OPENAI_API_KEY}"
      models:
        gpt-4:
          id: "gpt-4"
          name: "GPT-4"
          cost_input: 5.0
          cost_output: 15.0
```

## Application Context Integration

Ember's application context integrates with the configuration system:

```python
from ember.core.app_context import create_ember_app, get_ember_context

# Create app context with custom config
app_context = create_ember_app(config_path="my_config.yaml")

# Get config from app context
context = get_ember_context()
config = context.config_manager.get_config()

# Access configuration
auto_discover = config.registry.auto_discover
openai_config = config.get_provider("openai")

# API keys from environment variables are automatically loaded
model_registry = context.model_registry
openai_model = model_registry.get_model_info("openai:gpt-4")
```

## API Methods

Key ConfigManager methods:

```python
# Create a configuration manager
config_manager = create_config_manager(config_path="config.yaml")

# Get configuration
config = config_manager.get_config()

# Set a provider API key
config_manager.set_provider_api_key("openai", "new-api-key")

# Reload configuration from sources
config = config_manager.reload()
```

Key EmberConfig methods:

```python
# Get provider configuration
provider = config.get_provider("openai")

# Get model configuration
model = config.get_model_config("openai:gpt-4")

# Access model cost information (both forms work)
cost_input = model.cost_input  # Direct field access
cost_per_thousand = model.cost.input_cost_per_thousand  # Via computed cost property
```

## Model Registry Integration

Initialize the model registry with configuration:

```python
from ember.core.registry.model.initialization import initialize_registry

# Initialize with configuration
registry = initialize_registry(config_path="config.yaml")

# Or use an existing config manager
registry = initialize_registry(config_manager=config_manager)
```

## Backward Compatibility

For legacy code compatibility:

```python
# Old way - still works but shows deprecation warning
from ember.core.registry.model.config.settings import initialize_ember
registry = initialize_ember(auto_discover=True)

# New way - preferred approach
from ember.core.registry.model.initialization import initialize_registry
registry = initialize_registry()
```

## Best Practices

1. **Store secrets in environment variables**:

   ```yaml
   registry:
     providers:
       openai:
         api_keys:
           default:
             key: "${OPENAI_API_KEY}"
   ```

2. **Validate configuration early**:

   ```python
   config_manager = create_config_manager()
   config = config_manager.get_config()  # Validates on first access
   ```

3. **Create environment-specific configs**:

   ```python
   # Development
   config_manager = create_config_manager("config.dev.yaml")

   # Production
   config_manager = create_config_manager("config.prod.yaml")
   ```

   </code>

docs\quickstart\data.md:
<code>

# Ember Data Processing - Quickstart Guide

This guide introduces Ember's data processing system, which provides tools for loading, transforming, and evaluating data across various benchmarks and datasets.

## 1. Introduction to Ember Data

Ember's data module provides:

- **Standardized Dataset Access**: Unified interface to common benchmarks and custom datasets
- **Flexible Transformations**: Pipelines for preprocessing and normalizing data
- **Evaluation Framework**: Tools for measuring model performance across tasks
- **Sampling Controls**: Methods for dataset subsampling and stratification
- **Data Registry**: Central registry of popular evaluation benchmarks with metadata

## 2. Loading Datasets - The Simple Way

```python
from ember.api import datasets

# Load a standard benchmark dataset
mmlu_data = datasets("mmlu", config={"subset": "high_school_biology", "split": "test"})

# Access dataset entries
for entry in mmlu_data:
    print(f"Question: {entry.query}")
    print(f"Choices: {entry.choices}")
    print(f"Answer: {entry.metadata.get('correct_answer')}")
    print("---")
```

## 3. Using DatasetBuilder

```python
from ember.api import DatasetBuilder

# Create and configure dataset using builder pattern
transformed_data = (DatasetBuilder()
    .from_registry("mmlu")
    .subset("high_school_biology")
    .split("test")
    .sample(100)
    .transform(lambda item: {
        **item,
        "question": f"Please answer: {item['question']}"
    })
    .build())

# Access the transformed data
for entry in transformed_data:
    print(f"Question: {entry.query}")
    print(f"Choices: {entry.choices}")
    print(f"Answer: {entry.metadata.get('correct_answer')}")
    print("---")
```

## 4. Creating Custom Datasets

```python
from ember.api import register, Dataset, DatasetEntry, TaskType
from typing import List, Dict, Any
import json

# Define dataset class with registration decorator
@register("my_dataset", source="custom/qa", task_type=TaskType.QUESTION_ANSWERING)
class CustomDataset:
    def load(self, config=None) -> List[DatasetEntry]:
        # Load data from a file
        with open("my_dataset.json", "r") as f:
            data = json.load(f)

        # Convert to DatasetEntry objects
        entries = []
        for entry in data:
            item = DatasetEntry(
                id=entry["id"],
                content={
                    "question": entry["question"],
                    "choices": entry["options"],
                    "answer": entry["correct_option"]
                },
                metadata={
                    "category": entry["category"],
                    "difficulty": entry["difficulty"]
                }
            )
            entries.append(item)

        return entries

# Use it like any other dataset
my_data = datasets("my_dataset")
```

## 5. Filtering and Transforming Datasets

```python
from ember.api import DatasetBuilder

# Custom transformation function
def add_context_to_question(item):
    return {
        **item,
        "question": f"In the context of {item['metadata']['category']}: {item['question']}"
    }

# Filtering to specific categories
science_questions = (DatasetBuilder()
    .from_registry("mmlu")
    .subset("high_school_chemistry")
    .filter(lambda item: "reaction" in item["question"].lower())
    .transform(add_context_to_question)
    .build())

print(f"Found {len(science_questions)} chemistry reaction questions")
```

## 6. Evaluating Model Performance

```python
from ember.api import datasets
from ember.core.utils.eval.pipeline import EvaluationPipeline
from ember.core.utils.eval.evaluators import MultipleChoiceEvaluator
from ember.api.models import ModelBuilder

# Load a dataset
mmlu_data = datasets("mmlu", config={"subset": "high_school_biology", "split": "test"})

# Initialize model
model = ModelBuilder().temperature(0.0).build("openai:gpt-4o")

# Create evaluator
evaluator = MultipleChoiceEvaluator()

# Set up and run evaluation pipeline
eval_pipeline = EvaluationPipeline(
    dataset=mmlu_data.entries,
    evaluators=[evaluator],
    model=model
)

# Run evaluation
results = eval_pipeline.evaluate()

# Print results
print(f"Accuracy: {results.metrics['accuracy']:.2f}")
print(f"Per-category breakdown: {results.metrics.get('category_accuracy', {})}")
```

## 7. Working with Evaluation Results

```python
import matplotlib.pyplot as plt

# Assuming we have evaluation results
# results: EvaluationResults

# Access overall metrics
accuracy = results.metrics["accuracy"]
f1 = results.metrics.get("f1_score", 0.0)

# Access per-item results
for item_result in results.item_results:
    item_id = item_result.item_id
    correct = item_result.correct
    model_answer = item_result.model_output
    expected = item_result.expected_output

    if not correct:
        print(f"Item {item_id} was incorrect:")
        print(f"  Expected: {expected}")
        print(f"  Model output: {model_answer}")

# Plot results if category data is available
if "category_accuracy" in results.metrics:
    categories = results.metrics["category_accuracy"].keys()
    accuracies = [results.metrics["category_accuracy"][cat] for cat in categories]

    plt.figure(figsize=(10, 6))
    plt.bar(categories, accuracies)
    plt.title("Accuracy by Category")
    plt.xlabel("Category")
    plt.ylabel("Accuracy")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig("category_results.png")
```

## 8. Built-in Datasets

Ember provides ready-to-use loaders for popular benchmarks:

```python
from ember.api import datasets, list_available_datasets

# List all available datasets
available_datasets = list_available_datasets()
print(f"Available datasets: {available_datasets}")

# MMLU (Massive Multitask Language Understanding)
mmlu = datasets("mmlu", config={"subset": "high_school_mathematics"})

# TruthfulQA
truthful_qa = datasets("truthful_qa", config={"subset": "generation"})

# HaluEval (Hallucination Evaluation)
halu_eval = datasets("halueval", config={"subset": "knowledge"})

# CommonsenseQA
commonsense_qa = datasets("commonsense_qa", config={"split": "validation"})

# AIME (American Invitational Mathematics Examination)
aime = datasets("aime")

# GPQA (Graduate-level Physics Questions)
gpqa = datasets("gpqa")

# Codeforces Programming Problems
codeforces = datasets("codeforces", config={"difficulty_range": (800, 1200)})
```

## 9. Best Practices

1. **Use the Builder Pattern**: `DatasetBuilder` provides a clean, fluent interface
2. **Registry Integration**: Register custom datasets for seamless integration
3. **Transformation Order**: Consider the sequence of transformations and filters
4. **Stratified Sampling**: Ensure representative subsets with appropriate sampling
5. **Multiple Evaluators**: Use specialized evaluators for comprehensive assessment
6. **Configuration**: Use structured configs for reproducibility

## Next Steps

Learn more about:

- [Model Registry](model_registry.md) - Managing LLM configurations
- [Operators](operators.md) - Building computational units
- [Evaluation Metrics](../advanced/evaluation_metrics.md) - Detailed metrics and evaluation approaches
- [Custom Transformers](../advanced/custom_transformers.md) - Building custom data transformations
  </code>

docs\quickstart\model_registry.md:
<code>

# Ember Model Module - Quickstart Guide

This quickstart guide will help you integrate LLM models into your project using Ember's model module. The guide follows SOLID principles and best practices for modularity and maintainability.

## 1. Installation

```bash
# Clone the repository
git clone https://github.com/pyember/ember.git
cd ember

# Install using uv
uv pip install -e "."
```

## 2. API Key Setup

Set your API keys as environment variables:

```bash
# For bash/zsh
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"

# For Windows PowerShell
$env:OPENAI_API_KEY="your-openai-key"
$env:ANTHROPIC_API_KEY="your-anthropic-key"
$env:GOOGLE_API_KEY="your-google-key"
```

Alternatively, create a `.env` file in your project:

```env
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key
```

## 3. Basic Usage (Provider Namespaces)

```python
from ember.api import models

# Access provider models directly via namespaces
response = models.openai.gpt4o("What is the capital of France?")
print(response.data)
```

## 4. Builder Pattern

```python
from ember.api.models import ModelBuilder

# Create model with builder pattern
model = (
    ModelBuilder()
    .temperature(0.7)
    .max_tokens(100)
    .build("anthropic:claude-3-5-sonnet")
)

# Generate response
response = model.generate(prompt="Explain quantum computing in 50 words")
print(response.data)
```

## 5. Direct Model Access (ModelAPI)

```python
from ember.api.models import ModelAPI

# Get model directly for more control
model = ModelAPI(id="openai:gpt-4o")

# Use the model directly
response = model.generate(prompt="What's the capital of France?")
print(response.data)
```

## 6. Usage Tracking

```python
from ember.api.models import get_usage_service

# Access usage service
usage_service = get_usage_service()

# Make model requests using any method
response = models.anthropic.claude_3_5_sonnet("Hello world!")

# Get usage statistics for a specific model
model_id = "anthropic:claude-3-5-sonnet"
usage_summary = usage_service.get_usage_summary(model_id=model_id)

print(f"Model: {usage_summary.model_name}")
print(f"Total tokens: {usage_summary.total_tokens_used}")
print(f"Prompt tokens: {usage_summary.total_usage.prompt_tokens}")
print(f"Completion tokens: {usage_summary.total_usage.completion_tokens}")
print(f"Estimated cost: ${usage_summary.total_usage.cost_usd:.4f}")
```

## 7. Available Models

You can use any of these models by their ID or corresponding ModelEnum:

### OpenAI Models

- `openai:gpt-4o` or `ModelEnum.gpt4o`
- `openai:gpt-4o-mini` or `ModelEnum.gpt4o_mini`
- `openai:gpt-4` or `ModelEnum.gpt4`
- `openai:gpt-4-turbo` or `ModelEnum.gpt4_turbo`
- `openai:gpt-3.5-turbo` or `ModelEnum.gpt3_5_turbo`
- `openai:o1-2024-12-17` or `ModelEnum.o1`

### Anthropic Models

- `anthropic:claude-3.7-sonnet` or `ModelEnum.claude_3_7_sonnet`
- `anthropic:claude-3-5-sonnet` or `ModelEnum.claude_3_5_sonnet`
- `anthropic:claude-3-5-haiku` or `ModelEnum.claude_3_5_haiku`
- `anthropic:claude-3-opus` or `ModelEnum.claude_3_opus`
- `anthropic:claude-3-haiku` or `ModelEnum.claude_3_haiku`

### Deepmind Models

- `deepmind:gemini-1.5-pro` or `ModelEnum.gemini_1_5_pro`
- `deepmind:gemini-1.5-flash` or `ModelEnum.gemini_1_5_flash`
- `deepmind:gemini-1.5-flash-8b` or `ModelEnum.gemini_1_5_flash_8b`
- `deepmind:gemini-2.0-flash` or `ModelEnum.gemini_2_0_flash`
- `deepmind:gemini-2.0-flash-lite` or `ModelEnum.gemini_2_0_flash_lite`
- `deepmind:gemini-2.0-pro` or `ModelEnum.gemini_2_0_pro`

## 8. Error Handling

```python
try:
    response = models.openai.gpt4o("Hello world!")
    print(response.data)
except Exception as e:
    print(f"Error: {str(e)}")
```

## 9. Advanced: Adding Custom Models

```python
from ember.api.models import get_registry, ModelInfo, ModelCost, RateLimit

# Get registry
registry = get_registry()

# Create model info
custom_model = ModelInfo(
    id="custom:my-model",
    name="my-custom-model",
    cost=ModelCost(
        input_cost_per_thousand=0.0005,
        output_cost_per_thousand=0.0015
    ),
    rate_limit=RateLimit(
        tokens_per_minute=100000,
        requests_per_minute=3000
    ),
    provider={
        "name": "CustomProvider",
        "default_api_key": "${CUSTOM_API_KEY}",
        "base_url": "https://api.custom-provider.com/v1"
    }
)

# Register custom model
registry.register_model(model_info=custom_model)
```

## 10. Type-safe Model Invocation with Enums

```python
from ember.api.models import ModelAPI, ModelEnum

# Create model using an enum for type safety
model = ModelAPI.from_enum(ModelEnum.gpt4o)

# Generate response
response = model.generate(prompt="Hello world!")
print(response.data)
```

## 11. Batch Processing with Multiple Models

```python
from concurrent.futures import ThreadPoolExecutor
from ember.api.models import ModelAPI

# Define prompts and models
prompts = [
    "What is machine learning?",
    "Explain neural networks.",
    "What is transfer learning?",
    "Describe reinforcement learning."
]

model_ids = [
    "openai:gpt-4o",
    "anthropic:claude-3-5-sonnet",
    "deepmind:gemini-1.5-pro",
    "openai:gpt-4-turbo"
]

# Process in parallel
def process_prompt(args):
    model_id, prompt = args
    model = ModelAPI(id=model_id)
    response = model.generate(prompt=prompt)
    return model_id, prompt, response.data

# Execute in parallel with ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(process_prompt, zip(model_ids, prompts)))

# Print results
for model_id, prompt, result in results:
    print(f"Model: {model_id}")
    print(f"Prompt: {prompt}")
    print(f"Result: {result[:100]}...")  # Truncate for display
    print()
```

## Next Steps

For more advanced usage, check out:

- Custom provider integration
- Multi-model ensembling
- Prompt templating
- Streaming responses

## Provider Discovery System

The model registry integrates a robust provider discovery mechanism with three key design principles:

### 1. Resilience

The discovery system ensures application stability through:

- **Graceful degradation** - Fallback models when APIs are unreachable
- **Timeout protection** - Automatic 30s timeout with ThreadPoolExecutor to prevent indefinite blocking
- **Error isolation** - API failures in one provider don't affect others

### 2. Testing Strategy

Testing model discovery requires both:

- **Unit tests** - Fast, deterministic verification with mocks
- **Integration tests** - Selective API verification with real credentials

Enable integration tests with environment flags:

```bash
RUN_PROVIDER_INTEGRATION_TESTS=1 pytest tests/integration/core/registry/test_provider_discovery.py -v
```

### 3. Design Patterns

The discovery system employs:

- **Adapter Pattern** - Unified interface across varying provider APIs
- **Factory Pattern** - Dynamic provider instantiation based on available credentials
- **Dependency Injection** - Explicit API key configuration to avoid global state
- **Template Method** - Common discovery workflow with provider-specific implementations
  </code>

docs\quickstart\non.md:
<code>

# Ember NON (Networks of Networks)

This guide introduces Ember's Networks of Networks (NON) module, which provides composable patterns for building robust CAIS workflows.

## Core Operators

The NON module provides reusable operator implementations:

```python
from ember.core.non import (
    UniformEnsemble,    # Multiple identical model instances
    VariedEnsemble,     # Different model configurations
    JudgeSynthesis,     # Analyze and synthesize multiple responses
    MostCommon,         # Statistical majority voting
    Verifier,           # Validate and correct candidate answers
    Sequential          # Chain multiple operators
)
```

## Basic Usage

### UniformEnsemble

Creates multiple instances of the same model to mitigate non-determinism.

```python
# Create an ensemble with 3 identical instances
ensemble = UniformEnsemble(
    num_units=3,
    model_name="openai:gpt-4o",
    temperature=0.7,
    max_tokens=100
)

# Execute the ensemble
result = ensemble(inputs={"query": "What causes earthquakes?"})
responses = result.responses  # List of responses from each model
```

### JudgeSynthesis

Analyzes multiple responses and synthesizes a better answer.

```python
# Create a judge with a high-quality model
judge = JudgeSynthesis(
    model_name="anthropic:claude-3-sonnet",
    temperature=0.1
)

# Synthesize responses
result = judge(inputs={
    "query": "What causes earthquakes?",
    "responses": ensemble_responses
})

final_answer = result.final_answer
```

### MostCommon

Implements majority voting across multiple responses.

```python
# Create a most-common selector
majority = MostCommon()

# Find the most common answer
result = majority(inputs={
    "query": "What is 2+2?",
    "responses": ["4", "4", "3", "4", "5"]
})

most_common_answer = result.final_answer  # "4"
```

### Verifier

Checks answers for correctness and provides corrections when needed.

```python
# Create a verification operator
verifier = Verifier(
    model_name="openai:gpt-4o",
    temperature=0.2
)

# Verify an answer
result = verifier(inputs={
    "query": "What is the capital of Australia?",
    "candidate_answer": "Sydney is the capital of Australia."
})

verdict = result.verdict           # "incorrect"
explanation = result.explanation   # Explains the error
revised_answer = result.revised_answer  # "Canberra is the capital of Australia."
```

## Building Complex Pipelines

### Ensemble-Judge-Verifier Pipeline

Create a robust multi-step pipeline with JIT optimization:

```python
from ember.core.non import UniformEnsemble, JudgeSynthesis, Verifier, Sequential
from ember.xcs import jit

# Define a JIT-optimized pipeline class
@jit
class RobustQAPipeline(Sequential):
    """Pipeline that combines ensemble, judge, and verification steps."""

    ensemble: UniformEnsemble
    judge: JudgeSynthesis
    verifier: Verifier

    def __init__(self):
        # Create the ensemble operator
        self.ensemble = UniformEnsemble(
            num_units=3,
            model_name="openai:gpt-4o",
            temperature=0.7
        )

        # Create the judge operator
        self.judge = JudgeSynthesis(
            model_name="anthropic:claude-3-opus",
            temperature=0.1
        )

        # Create the verifier operator
        self.verifier = Verifier(
            model_name="openai:gpt-4o",
            temperature=0.2
        )

        # Chain the operators in sequence
        super().__init__(operators=[self.ensemble, self.judge, self.verifier])

    def forward(self, *, inputs):
        # First, run the ensemble to get multiple answers
        ensemble_result = self.ensemble(inputs=inputs)

        # Next, judge the ensemble responses
        judge_result = self.judge(inputs={
            "query": inputs["query"],
            "responses": ensemble_result.responses
        })

        # Finally, verify the judge's answer
        verifier_result = self.verifier(inputs={
            "query": inputs["query"],
            "candidate_answer": judge_result.final_answer
        })

        return verifier_result

# Create and use the optimized pipeline
pipeline = RobustQAPipeline()
result = pipeline(inputs={"query": "What is the speed of light?"})

print(f"Verdict: {result.verdict}")
print(f"Final answer: {result.revised_answer}")
print(f"Explanation: {result.explanation}")
```

### Multi-model Pipeline

Combine different model types with automatic parallelization:

```python
from ember.core.non import VariedEnsemble, JudgeSynthesis
from ember.core.registry.model.model_module import LMModuleConfig
from ember.xcs.engine.execution_options import execution_options

# Define varied model configurations
model_configs = [
    LMModuleConfig(model_name="openai:gpt-4o", temperature=0.3),
    LMModuleConfig(model_name="anthropic:claude-3-haiku", temperature=0.4),
    LMModuleConfig(model_name="openai:gpt-4o-mini", temperature=0.5)
]

# Create operators
varied_ensemble = VariedEnsemble(model_configs=model_configs)
judge = JudgeSynthesis(model_name="anthropic:claude-3-sonnet")

# Execute with auto-parallelization
with execution_options(scheduler="wave", max_workers=len(model_configs)):
    # Get responses from the diverse models
    ensemble_result = varied_ensemble(inputs={"query": "Explain quantum computing."})

    # Synthesize into a single answer
    final_result = judge(inputs={
        "query": "Explain quantum computing.",
        "responses": ensemble_result.responses
    })

print(f"Final answer: {final_result.final_answer}")
```

## Creating Custom NON Operators

Create specialized operators that follow Ember's patterns:

```python
from typing import ClassVar, Type
from ember.core.non import UniformEnsemble
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel

class FactCheckerInput(EmberModel):
    """Input for the fact checker operator."""
    query: str
    domain: str

class FactCheckerOutput(EmberModel):
    """Output from the fact checker operator."""
    facts: list[str]
    sources: list[str]
    confidence: float

class FactCheckerSpecification(Specification):
    """Specification for the FactChecker operator."""
    input_model: Type[EmberModel] = FactCheckerInput
    structured_output: Type[EmberModel] = FactCheckerOutput

    prompt_template = """Check the following statement for factual accuracy in the {domain} domain.
    Statement: {query}

    Provide a list of verified facts and their sources.
    """

class FactChecker(Operator[FactCheckerInput, FactCheckerOutput]):
    """Custom operator for domain-specific fact checking."""

    # Class-level specification
    specification: ClassVar[Specification] = FactCheckerSpecification()

    # Instance attributes
    ensemble: UniformEnsemble
    confidence_threshold: float

    def __init__(self, *, confidence_threshold: float = 0.7):
        self.ensemble = UniformEnsemble(
            num_units=3,
            model_name="openai:gpt-4o",
            temperature=0.2
        )
        self.confidence_threshold = confidence_threshold

    def forward(self, *, inputs: FactCheckerInput) -> FactCheckerOutput:
        # Generate responses with the ensemble
        ensemble_result = self.ensemble(inputs={"query": inputs.query})

        # Process responses to extract facts and sources
        # (implementation details omitted for brevity)
        facts = ["Earth orbits the Sun", "A day on Earth is approximately 24 hours"]
        sources = ["Astronomy textbook", "NASA website"]
        confidence = 0.95

        return FactCheckerOutput(
            facts=facts,
            sources=sources,
            confidence=confidence
        )

# Use the custom operator
fact_checker = FactChecker(confidence_threshold=0.8)
result = fact_checker(inputs={"query": "The Earth orbits the Sun", "domain": "astronomy"})
```

## Related Documentation

- [Operators](operators.md) - Building custom computation units
- [Model Registry](model_registry.md) - Managing LLM configurations
- [Enhanced JIT](../xcs/JIT_OVERVIEW.md) - Optimized tracing and execution
  </code>

docs\quickstart\operators.md:
<code>

# Ember Operators

Operators provide a typed, composable abstraction for building AI workflows. This guide introduces core concepts and usage patterns.

## 1. Core Concepts

Operators are:

- Typed computation units with structured inputs/outputs
- Composable into complex processing graphs
- Automatically parallelizable
- Thread-safe through immutable design

```python
from typing import List
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.types.ember_model import EmberModel

class ClassifierInput(EmberModel):
    text: str
    categories: List[str]

class ClassifierOutput(EmberModel):
    category: str
    confidence: float
```

## 2. Basic Implementation

```python
from typing import ClassVar, Type
from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.specification.specification import Specification

class ClassifierSpecification(Specification):
    input_model: Type[EmberModel] = ClassifierInput
    structured_output: Type[EmberModel] = ClassifierOutput
    prompt_template: str = """Classify the following text into one of these categories: {categories}

Text: {text}

Respond with a JSON object with two keys:
- "category": The best matching category
- "confidence": A number between 0 and 1 indicating confidence
"""

class TextClassifierOperator(Operator[ClassifierInput, ClassifierOutput]):
    specification: ClassVar[Specification] = ClassifierSpecification()
    lm_module: LMModule

    def __init__(self, *, model_name: str = "openai:gpt-4o", temperature: float = 0.0) -> None:
        self.lm_module = LMModule(
            config=LMModuleConfig(
                model_name=model_name,
                temperature=temperature,
                response_format={"type": "json_object"}
            )
        )

    def forward(self, *, inputs: ClassifierInput) -> ClassifierOutput:
        prompt = self.specification.render_prompt(inputs=inputs)
        response = self.lm_module(prompt=prompt)

        try:
            import json
            result = json.loads(response)
            return ClassifierOutput(
                category=result["category"],
                confidence=result["confidence"]
            )
        except Exception as e:
            raise ValueError(f"Failed to parse LLM response: {e}")
```

## 3. Invocation

```python
# Instantiate
classifier = TextClassifierOperator(model_name="anthropic:claude-3-haiku")

# Execute with dict-style inputs
result = classifier(inputs={
    "text": "The sky is blue and the sun is shining brightly today.",
    "categories": ["weather", "politics", "technology", "sports"]
})

print(f"Category: {result.category}, Confidence: {result.confidence:.2f}")
```

## 4. Composition Patterns

### Sequential

```python
from ember.core import non

pipeline = non.Sequential(operators=[preprocessor, classifier, postprocessor])
result = pipeline(inputs={"text": "Your input text here"})
```

### Parallel

```python
from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.engine.unified_engine import execute_graph

# Setup operators
sentiment = SentimentAnalysisOperator(model_name="openai:gpt-4o")
classifier = TextClassifierOperator(model_name="anthropic:claude-3-haiku")
summarizer = TextSummarizerOperator(model_name="openai:gpt-4o-mini")

# Build computation graph
graph = XCSGraph()
graph.add_node(operator=sentiment, node_id="sentiment")
graph.add_node(operator=classifier, node_id="classifier")
graph.add_node(operator=summarizer, node_id="summarizer")
graph.add_node(operator=aggregator, node_id="aggregator")

# Define dependencies
graph.add_edge(from_id="sentiment", to_id="aggregator")
graph.add_edge(from_id="classifier", to_id="aggregator")
graph.add_edge(from_id="summarizer", to_id="aggregator")

# Execute with parallelization
result = execute_graph(
    graph=graph,
    global_input={"text": "Your input text here"},
    max_workers=3
)
```

### Execution Control

```python
from ember.xcs.engine.execution_options import execution_options

with execution_options(scheduler="parallel", max_workers=4):
    result = pipeline(inputs={"query": "What is the capital of France?"})
```

## 5. Built-in Operators

```python
from ember.core import non

# Ensemble with multiple identical models
ensemble = non.UniformEnsemble(
    num_units=3,
    model_name="openai:gpt-4o",
    temperature=0.7
)

# Meta-reasoning synthesis
judge = non.JudgeSynthesis(model_name="anthropic:claude-3-sonnet")

# Execution pipeline
ensemble_result = ensemble(inputs={"query": "What is the capital of France?"})
final_result = judge(inputs={
    "query": "What is the capital of France?",
    "responses": ensemble_result.responses
})
```

## 6. JIT and Transformations

```python
from typing import ClassVar
from ember.xcs.jit import jit
from ember.xcs.transforms.vmap import vmap
from ember.xcs.transforms.pmap import pmap

# JIT optimization
@jit(sample_input={"text": "Sample", "categories": ["a", "b"]})
class OptimizedOperator(Operator[ClassifierInput, ClassifierOutput]):
    specification: ClassVar[Specification] = ClassifierSpecification()

    def forward(self, *, inputs: ClassifierInput) -> ClassifierOutput:
        # Implementation
        return ClassifierOutput(category="category", confidence=0.95)

# Container pattern
@jit
class Pipeline(Operator[ProcessingInput, ProcessingOutput]):
    specification: ClassVar[Specification] = PipelineSpecification()
    preprocessor: PreprocessOperator
    classifier: OptimizedOperator

    def __init__(self, *, config_param: str = "default") -> None:
        self.preprocessor = PreprocessOperator(param=config_param)
        self.classifier = OptimizedOperator()

    def forward(self, *, inputs: ProcessingInput) -> ProcessingOutput:
        intermediate = self.preprocessor(inputs=inputs)
        return self.classifier(inputs=intermediate)

# Vectorization
batch_processor = vmap(single_item_processor)
results = batch_processor(inputs=[input1, input2, input3])

# Parallelization
parallel_processor = pmap(compute_intensive_operator)
```

## 7. Best Practices

1. **Strong Typing**: Use EmberModel for all inputs/outputs with proper typing
2. **Class-Level Fields**: Declare fields with types at class level
3. **Immutable Design**: Keep state immutable after initialization
4. **Named Parameters**: Use `__init__(*, param1, param2)` pattern
5. **ClassVar for Specifications**: Use `specification: ClassVar[Specification]`
6. **Dict-Style Inputs**: Use `inputs={"key": value}` format
7. **Clean Forward Methods**: Keep `forward()` methods pure and deterministic
8. **Error Handling**: Catch LLM failures with specific error messages
9. **Small Components**: Build small, focused operators for composition
10. **Validate Outputs**: Return properly typed model instances

## Next Steps

- [Prompt Specifications](prompt_signatures.md) - Type-safe templating
- [Model Registry](model_registry.md) - LLM configuration
- [NON Patterns](non.md) - Networks of Networks
- [XCS Overview](../xcs/README.md) - Computation system
- [JIT](../xcs/JIT_OVERVIEW.md) - Just-In-Time compilation
- [Execution Options](../xcs/EXECUTION_OPTIONS.md) - Execution control
- [Transforms](../xcs/TRANSFORMS.md) - Vectorization and parallelization
  </code>

docs\quickstart\prompt_signatures.md:
<code>

# Ember Prompt Specifications - Quickstart Guide

This guide introduces Ember's Prompt Specification system, which provides type-safe, composable prompt engineering with strong validation guarantees.

## 1. Introduction to Specifications

Specifications in Ember define the contract between inputs and outputs for an operator, including:

- Input schema (what data is required for the operation)
- Output schema (what structure the result will have)
- Prompt template (how to format inputs into a prompt)

They provide automatic validation, clear error messages, and consistent prompt formatting.

## 2. Creating a Basic Specification

```python
from ember.core.registry.specification import Specification
from pydantic import BaseModel, Field
from typing import List

# Define input schema
class QuestionAnsweringInput(BaseModel):
    question: str
    context: str = Field(description="Background information to answer the question")

# Define output schema
class QuestionAnsweringOutput(BaseModel):
    answer: str
    confidence: float = Field(ge=0.0, le=1.0, description="Confidence score between 0 and 1")
    reasoning: str = Field(description="Step-by-step reasoning process")

# Create the specification
class QASpecification(Specification):
    input_model = QuestionAnsweringInput
    structured_output = QuestionAnsweringOutput
    prompt_template = """Answer the following question based on the provided context.

Context: {context}

Question: {question}

Provide a clear, concise answer along with your confidence level and reasoning.
"""
```

## 3. Using Specifications with Operators

```python
from ember.core.registry.operator.base import Operator
from ember.core.registry.model.model_module import LMModule, LMModuleConfig

class QuestionAnsweringOperator(Operator[QuestionAnsweringInput, QuestionAnsweringOutput]):
    specification = QASpecification()

    def __init__(self, model_name: str = "openai:gpt-4o"):
        self.lm_module = LMModule(LMModuleConfig(
            model_name=model_name,
            response_format={"type": "json_object"}
        ))

    def forward(self, *, inputs: QuestionAnsweringInput) -> QuestionAnsweringOutput:
        # The specification automatically validates inputs
        prompt = self.specification.render_prompt(inputs=inputs)

        # Call LLM and parse response
        response_text = self.lm_module(prompt)
        import json
        response_data = json.loads(response_text)

        # The specification automatically validates output
        return self.specification.validate_output(output=response_data)
```

## 4. Specification Features

### Automatic Validation

```python
try:
    # This will fail validation (missing required field)
    invalid_input = {"question": "Who was Ada Lovelace?"}
    validated = specification.validate_inputs(inputs=invalid_input)
except Exception as e:
    print(f"Validation error: {e}")

# This will pass validation
valid_input = {
    "question": "Who was Ada Lovelace?",
    "context": "Ada Lovelace was an English mathematician and writer..."
}
validated = specification.validate_inputs(inputs=valid_input)
```

### Flexible Prompt Rendering

```python
# Render from dictionary
prompt1 = specification.render_prompt(inputs={
    "question": "What is quantum computing?",
    "context": "Quantum computing uses quantum bits or qubits..."
})

# Render from validated model
input_model = QuestionAnsweringInput(
    question="What is quantum computing?",
    context="Quantum computing uses quantum bits or qubits..."
)
prompt2 = specification.render_prompt(inputs=input_model)
```

### JSON Schema Generation

```python
# Generate JSON schema for documentation or client SDKs
input_schema = specification.model_json_schema(by_alias=True)
print(input_schema)
```

## 5. Advanced Specification Patterns

### Inheritance and Composition

```python
# Create a base specification
class BaseQASpecification(Specification):
    input_model = QuestionAnsweringInput
    structured_output = QuestionAnsweringOutput

# Extended version with different prompt
class DetailedQASpecification(BaseQASpecification):
    prompt_template = """Analyze the following question in detail, using the context.

Context:
{context}

Question:
{question}

Provide a detailed answer with reasoning and your confidence level.
"""
```

### Dynamic Templates

```python
class DynamicSpecification(Specification):
    input_model = QuestionAnsweringInput
    structured_output = QuestionAnsweringOutput

    def __init__(self, style: str = "concise"):
        super().__init__()
        if style == "concise":
            self.prompt_template = "Answer briefly: {question}\nContext: {context}"
        elif style == "detailed":
            self.prompt_template = """Provide a detailed analysis.

Question: {question}

Full context:
{context}

Include reasoning and confidence level.
"""
```

### Custom Validation Logic

```python
from pydantic import model_validator

class ValidatedInput(BaseModel):
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=100)

    @model_validator(mode="after")
    def validate_temperature_range(self):
        if self.temperature < 0 or self.temperature > 1:
            raise ValueError("Temperature must be between 0 and 1")
        return self

class CustomValidationSpecification(Specification):
    input_model = ValidatedInput

    def validate_inputs(self, *, inputs):
        validated = super().validate_inputs(inputs=inputs)
        # Add custom validation logic
        if validated.temperature > 0.9 and validated.max_tokens < 50:
            raise ValueError("High temperature requires higher max_tokens")
        return validated
```

## 6. Best Practices

1. **Type Everything**: Define clear input and output models
2. **Explicit Placeholders**: Make all placeholders explicit in your template
3. **Nested Models**: Use nested Pydantic models for complex data structures
4. **Clear Errors**: Provide descriptive error messages in custom validators
5. **Reuse Specifications**: Create base specifications and extend them for specific use cases
6. **Documentation**: Use Field descriptions to document your schemas

## 7. Using with Different LLM Providers

```python
from ember.core.registry.model.model_module import LMModuleConfig
from ember.core.registry.model.base.services import ModelService
from ember import initialize_ember

# Initialize model registry
service = initialize_ember(auto_register=True)

# Create operator with specification
class FlexibleQAOperator(Operator[QuestionAnsweringInput, QuestionAnsweringOutput]):
    specification = QASpecification()

    def __init__(self, model_id: str):
        self.model = service.get_model(model_id)

    def forward(self, *, inputs: QuestionAnsweringInput) -> QuestionAnsweringOutput:
        prompt = self.specification.render_prompt(inputs=inputs)
        response_text = self.model(prompt)
        # Process response and validate output
        # ...

# Create instances for different providers
openai_qa = FlexibleQAOperator("openai:gpt-4o")
anthropic_qa = FlexibleQAOperator("anthropic:claude-3-sonnet")
```

## Next Steps

Learn more about:

- [Operators](operators.md) - Building computational units in Ember
- [Model Registry](model_registry.md) - Managing LLM configurations
- [Non Patterns](non.md) - Networks of Networks composition
- [Structured Output](../advanced/structured_output.md) - Advanced output parsing techniques
  </code>

docs\xcs\API_REFERENCE.md:
<code>

# XCS API Reference

This document provides detailed API references for the XCS module.

## Core Functions

### jit

```python
from ember.api.xcs import jit

@jit(options=None)
class MyOperator(Operator):
    def forward(self, *, inputs):
        return processed_inputs
```

The `jit` decorator enables just-in-time compilation for operators. It traces execution and compiles optimized execution plans by analyzing the actual runtime behavior of the operator.

**Parameters:**

- `options` (Optional[JITOptions]): Configuration options for JIT compilation
- `sample_input` (Dict[str, Any]): Sample input for pre-compilation during initialization
- `force_trace` (bool): When True, always trace execution even for cached graphs
- `recursive` (bool): Whether to trace and compile nested operator calls

**Options:**

- `cache_size` (int): Maximum number of cached execution plans (default: 128)
- `sample_input` (Dict[str, Any]): Sample input for precompilation
- `trace_level` (str): Tracing detail level ("minimal", "standard", "verbose")
- `fallback` (bool): Whether to fall back to original function on errors

**When to use:**

- For most operator optimization needs
- When execution patterns can vary based on inputs
- When you need to optimize based on actual runtime behavior

### structural_jit

```python
from ember.api.xcs import structural_jit

@structural_jit(execution_strategy="parallel")
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()

    def forward(self, *, inputs):
        intermediate = self.op1(inputs=inputs)
        result = self.op2(inputs=intermediate)
        return result
```

The `structural_jit` decorator analyzes operator structure directly to optimize execution without requiring execution traces. It examines the composition relationships between operators to identify optimization opportunities.

**Parameters:**

- `execution_strategy` (str): Execution strategy ("auto", "parallel", "sequential")
- `parallel_threshold` (int): Minimum number of nodes to trigger parallel execution in auto mode
- `max_workers` (Optional[int]): Maximum number of worker threads for parallel execution
- `cache_graph` (bool): Whether to cache and reuse the compiled graph

**When to use:**

- For complex composite operators with many subcomponents
- When operator structure is known and static
- For maximum optimization of operator composition
- To parallelize independent operations in composite operators

### autograph

```python
from ember.api.xcs import autograph, execute

with autograph() as graph:
    result1 = op1(inputs={"query": "Example"})
    result2 = op2(inputs=result1)

results = execute(graph)
```

The `autograph` context manager provides explicit control over graph construction by recording operator calls to build a computational graph. Unlike the decorators, this approach requires manual graph building and execution.

**Returns:**

- `XCSGraph`: A computational graph representing the recorded operations

**When to use:**

- When you need explicit control over graph construction
- For debugging execution paths
- When you want to construct a graph once and execute it multiple times
- To create execution graphs for visualization or analysis

> **Note:** For a comprehensive comparison and detailed explanation of the relationship between these approaches, see [JIT_OVERVIEW.md](JIT_OVERVIEW.md).

### execute

```python
from ember.api.xcs import execute

results = execute(graph, inputs={"query": "Example"})
```

Executes a computational graph with the specified inputs.

**Parameters:**

- `graph` (XCSGraph): The graph to execute
- `inputs` (Dict[str, Any]): Input values for the graph
- `options` (Optional[XCSExecutionOptions]): Execution configuration

**Returns:**

- `Dict[str, Any]`: The results of graph execution

## Transforms

### vmap

```python
from ember.api.xcs import vmap

# Basic usage
batch_fn = vmap(single_item_function)
results = batch_fn([item1, item2, item3])

# With axis specification
batch_fn = vmap(function, in_axes=(0, None))
results = batch_fn(batch_inputs, constant_arg)
```

Vectorizes a function to process batched inputs in parallel.

**Parameters:**

- `func` (Callable): The function to vectorize
- `in_axes` (Tuple[Optional[int], ...]): Input axes to vectorize (None for constants)
- `out_axes` (Optional[int]): Output axis for batched results (default: 0)
- `options` (Optional[TransformOptions]): Additional configuration

**Returns:**

- `Callable`: A vectorized version of the input function

### pmap

```python
from ember.api.xcs import pmap

parallel_fn = pmap(heavy_computation)
results = parallel_fn(large_dataset)
```

Parallelizes a function to execute across multiple cores or devices.

**Parameters:**

- `func` (Callable): The function to parallelize
- `num_workers` (Optional[int]): Number of worker threads (default: auto)
- `options` (Optional[TransformOptions]): Additional configuration

**Returns:**

- `Callable`: A parallelized version of the input function

### mesh_sharded

```python
from ember.api.xcs import mesh_sharded, DeviceMesh, PartitionSpec

# Create a device mesh
mesh = DeviceMesh(shape=(2, 2))

# Define the partition spec
pspec = PartitionSpec(0, 1)

# Create a sharded function
sharded_fn = mesh_sharded(function, mesh=mesh, partition_spec=pspec)
results = sharded_fn(inputs)
```

Executes a function with inputs sharded across a device mesh.

**Parameters:**

- `func` (Callable): The function to shard
- `mesh` (DeviceMesh): The device mesh to use
- `partition_spec` (PartitionSpec): How to partition inputs across the mesh
- `options` (Optional[TransformOptions]): Additional configuration

**Returns:**

- `Callable`: A sharded version of the input function

## Configuration Types

### XCSExecutionOptions

```python
from ember.api.xcs import XCSExecutionOptions

with XCSExecutionOptions(
    scheduler="parallel",
    max_workers=4,
    timeout=30.0
):
    results = complex_operation(data)
```

Configuration options for XCS execution.

**Parameters:**

- `scheduler` (str): Scheduler to use ("auto", "parallel", "sequential")
- `max_workers` (Optional[int]): Maximum number of worker threads
- `timeout` (Optional[float]): Execution timeout in seconds
- `trace_execution` (bool): Whether to trace execution for debugging
- `fail_fast` (bool): Whether to stop on first error

### JITOptions

```python
from ember.api.xcs import JITOptions

options = JITOptions(
    sample_input={"query": "example"},
    cache_size=100
)
```

Configuration options for JIT compilation.

**Parameters:**

- `cache_size` (int): Maximum number of cached execution plans
- `sample_input` (Dict[str, Any]): Sample input for precompilation
- `trace_level` (str): Tracing detail level
- `fallback` (bool): Whether to fall back to original function on errors

### TransformOptions

```python
from ember.api.xcs import TransformOptions

options = TransformOptions(
    propagate_errors=True,
    timeout=10.0
)
```

Configuration options for transformations.

**Parameters:**

- `propagate_errors` (bool): Whether to propagate errors or catch them
- `timeout` (Optional[float]): Execution timeout in seconds

## Utility Types

### DeviceMesh

```python
from ember.api.xcs import DeviceMesh

# Create a 2x2 mesh
mesh = DeviceMesh(shape=(2, 2))

# Create a mesh with specific devices
mesh = DeviceMesh(devices=["gpu:0", "gpu:1", "gpu:2", "gpu:3"], shape=(2, 2))
```

Represents a logical grid of devices for distributed computation.

**Parameters:**

- `devices` (Optional[List[str]]): List of device identifiers
- `shape` (Optional[Tuple[int, ...]]): Logical shape of the mesh

### PartitionSpec

```python
from ember.api.xcs import PartitionSpec

# Partition along first dimension
pspec = PartitionSpec(0)

# Partition along first and second dimensions
pspec = PartitionSpec(0, 1)
```

Specifies how data should be partitioned across a device mesh.

**Parameters:**

- `*axes` (int): Dimensions to partition along
  </code>

docs\xcs\ARCHITECTURE.md:
<code>

# XCS Architecture

The Executable Computation System (XCS) provides a high-performance execution framework built around computational graphs, tracing, and parallelization. This document describes the architectural design behind XCS.

## System Architecture

XCS is built with a modular, layered architecture that separates concerns and enables flexibility:

```
┌─────────────────────────────────────────────────────────┐
│                    API Layer (api/)                     │
└───────────────────────────┬─────────────────────────────┘
                            │
┌─────────────────┬─────────┴───────────┬─────────────────┐
│  Tracer Layer   │   Engine Layer      │ Transform Layer │
│   (tracer/)     │    (engine/)        │  (transforms/)  │
└────────┬────────┴────────┬────────────┴────────┬────────┘
         │                 │                     │
         │         ┌───────┴───────┐             │
         └─────────►  Graph Layer  ◄─────────────┘
                   │   (graph/)    │
                   └───────┬───────┘
                           │
                   ┌───────┴───────┐
                   │ Utility Layer │
                   │   (utils/)    │
                   └───────────────┘
```

## Key Components

### API Layer

The API layer provides a simplified interface for users of the XCS system. It:

- Exposes core functionality through clean, simple imports
- Handles type conversion and validation
- Provides sensible defaults for complex operations

**Key Files:**

- `api/core.py`: Core API implementation
- `api/types.py`: Type definitions and validators

### Tracer Layer

The tracer layer is responsible for just-in-time compilation and execution tracing:

- Records operator calls and data flow
- Analyzes code structure for optimization
- Constructs execution graphs from traces

**Key Components:**

- `tracer_decorator.py`: JIT compilation functionality
- `structural_jit.py`: Structure-aware JIT optimization
- `autograph.py`: Automatic graph construction
- `xcs_tracing.py`: Core tracing infrastructure

### Engine Layer

The engine layer handles the execution of computational graphs:

- Schedules operations for efficient execution
- Manages concurrency and parallelism
- Handles data flow between operations

**Key Components:**

- `xcs_engine.py`: Core execution engine
- `execution_options.py`: Configuration for execution
- `xcs_parallel_scheduler.py`: Parallel execution scheduler
- `xcs_noop_scheduler.py`: No-operation scheduler for testing

### Graph Layer

The graph layer provides a representation for computational graphs:

- Defines node and edge structures
- Manages graph transformation
- Handles serialization and deserialization

**Key Components:**

- `xcs_graph.py`: Graph implementation

### Transform Layer

The transform layer provides functional transformations for operators:

- Vectorization (vmap) for batch processing
- Parallelization (pmap) for concurrent execution
- Mesh sharding for distributed execution

**Key Components:**

- `vmap.py`: Vectorized mapping implementation
- `pmap.py`: Parallel mapping implementation
- `mesh.py`: Sharded execution on device meshes

### Utility Layer

The utility layer provides common functionality used across the codebase:

- Tree manipulation utilities
- Type handling and conversions
- Common data structures

**Key Components:**

- `tree_util.py`: Tree manipulation utilities

## Data Flow

The typical data flow through the XCS system:

1. User code calls a JIT-decorated operator
2. The tracer intercepts the call and analyzes the operator structure
3. A computational graph is constructed (automatically or explicitly)
4. The graph is passed to the execution engine
5. The engine schedules operations based on dependencies
6. Operations are executed according to the schedule
7. Results are collected and returned to the caller

## Design Principles

XCS is built on several key design principles:

1. **Immutability**: Data structures are immutable to enable easy reasoning and parallelization
2. **Composability**: Transforms and operations can be freely composed
3. **Type Safety**: Strong typing with runtime protocol checking
4. **Fail-Fast**: Errors are detected early and reported clearly
5. **Performance**: Design choices prioritize efficient execution
6. **Testability**: Components are designed for easy testing

## Extension Points

XCS can be extended in several ways:

1. **Custom Schedulers**: Implement new scheduling strategies for the execution engine
2. **New Transforms**: Add new functional transformations
3. **Graph Optimizers**: Create optimization passes for computational graphs
4. **Custom Tracers**: Implement specialized tracing for specific operator types

## Performance Considerations

XCS is designed for high-performance execution:

1. **Graph Caching**: Compiled graphs are cached for repeated execution
2. **Intelligent Scheduling**: Operations are scheduled based on data dependencies
3. **Parallel Execution**: Independent operations are executed concurrently
4. **Minimal Overhead**: Core execution paths are optimized for minimal overhead
5. **Memory Efficiency**: Data structures are designed to minimize memory usage
   </code>

docs\xcs\EXECUTION_OPTIONS.md:
<code>

# Execution Options Guide

## Overview

The execution options system in Ember XCS allows you to control how operations are executed, including parallelism settings, device selection, and scheduling strategies. This guide explains how to use execution options effectively in your applications.

## Basic Usage

Execution options can be applied in two ways:

1. As a temporary context using the `execution_options` context manager
2. As a global setting using `set_execution_options`

### Context Manager (Recommended)

Using the context manager is the recommended approach for most use cases, as it ensures options are only applied within a specific scope and automatically restored afterward:

```python
from ember.xcs.engine.execution_options import execution_options

# Run with parallel execution and 4 workers
with execution_options(use_parallel=True, max_workers=4):
    result = vectorized_op(inputs={"prompt": prompt, "seed": seeds})

# Run with sequential execution
with execution_options(use_parallel=False):
    result = vectorized_op(inputs={"prompt": prompt, "seed": seeds})
```

### Global Settings

For cases where you want to set execution options for an entire application, you can use the `set_execution_options` function:

```python
from ember.xcs.engine.execution_options import set_execution_options

# Set global execution options
set_execution_options(use_parallel=True, max_workers=8)
```

You can retrieve the current execution options with `get_execution_options()`.

## Available Options

| Option            | Type              | Default  | Description                                                |
| ----------------- | ----------------- | -------- | ---------------------------------------------------------- |
| `use_parallel`    | `bool`            | `True`   | Whether to use parallel execution where possible           |
| `max_workers`     | `Optional[int]`   | `None`   | Maximum number of worker threads for parallel execution    |
| `device_strategy` | `str`             | `"auto"` | Strategy for device selection ('auto', 'cpu', 'gpu', etc.) |
| `enable_caching`  | `bool`            | `False`  | Whether to cache intermediate results                      |
| `trace_execution` | `bool`            | `False`  | Whether to trace execution for debugging                   |
| `timeout_seconds` | `Optional[float]` | `None`   | Maximum execution time in seconds before timeout           |
| `scheduler`       | `Optional[str]`   | `None`   | Legacy parameter for backward compatibility                |

## Integration with XCS Transforms

### vmap

Execution options are particularly useful when combined with vmap for controlling how batched operations are processed:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.transforms.vmap import vmap
from ember.xcs.engine.execution_options import execution_options
from ember.core.registry.specification import Specification

# Define a single-item operator with proper field declarations
class TextProcessor(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = TextProcessorSpecification()

    # Field declarations
    model_name: str
    max_length: int

    def __init__(self, *, model_name: str, max_length: int = 100) -> None:
        self.model_name = model_name
        self.max_length = max_length

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        text = inputs.get("text", "")
        return {"processed": f"Processed: {text[:self.max_length]}"}

# Create vectorized version of the operator
processor = TextProcessor(model_name="text-processor")
vectorized_processor = vmap(processor)

# Process a batch with parallel execution
with execution_options(use_parallel=True, max_workers=4):
    batch_result = vectorized_processor(inputs={
        "text": ["Sample text 1", "Sample text 2", "Sample text 3"],
        "options": {"format": "plain"}  # Non-batched parameter applied to all items
    })
```

### pmap

For pmap, execution options control the underlying execution behavior:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.transforms.pmap import pmap
from ember.xcs.engine.execution_options import execution_options
from ember.core.registry.specification import Specification

# Define an operator with proper field declarations
class DocumentAnalyzer(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = AnalyzerSpecification()

    # Field declarations
    analyzer_type: str
    depth: int

    def __init__(self, *, analyzer_type: str = "basic", depth: int = 3) -> None:
        self.analyzer_type = analyzer_type
        self.depth = depth

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        documents = inputs.get("documents", [])
        # Analyze documents
        return {"analysis": [f"Analysis of {doc}" for doc in documents]}

# Create parallelized version with specific worker count
analyzer = DocumentAnalyzer(analyzer_type="comprehensive")
parallelized_analyzer = pmap(analyzer, num_workers=4)

# execution_options can still affect other aspects of execution
with execution_options(timeout_seconds=10.0, enable_caching=True):
    result = parallelized_analyzer(inputs={"documents": ["doc1.txt", "doc2.txt", "doc3.txt"]})
```

### structural_jit

The structural JIT decorator analyzes operator composition and optimizes execution using the XCS graph system:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.tracer.structural_jit import structural_jit
from ember.xcs.engine.execution_options import execution_options
from ember.core.registry.specification import Specification

# Define sub-operators with proper field declarations
class ExtractorOperator(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = ExtractorSpecification()

    # Field declarations
    extractor_type: str

    def __init__(self, *, extractor_type: str = "entities") -> None:
        self.extractor_type = extractor_type

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        text = inputs.get("text", "")
        return {"extracted": f"Extracted {self.extractor_type} from: {text}"}

class ClassifierOperator(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = ClassifierSpecification()

    # Field declarations
    model_name: str
    threshold: float

    def __init__(self, *, model_name: str, threshold: float = 0.5) -> None:
        self.model_name = model_name
        self.threshold = threshold

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        extracted = inputs.get("extracted", "")
        return {"classification": f"Classified with {self.model_name}: {extracted}"}

# Define a JIT-optimized composite operator
@structural_jit
class NLPPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = PipelineSpecification()

    # Field declarations for sub-operators
    extractor: ExtractorOperator
    classifier: ClassifierOperator

    def __init__(self, *, model_name: str = "default") -> None:
        # Create sub-operators
        self.extractor = ExtractorOperator(extractor_type="entities")
        self.classifier = ClassifierOperator(model_name=model_name)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Connect operators in the pipeline
        extracted = self.extractor(inputs=inputs)
        return self.classifier(inputs=extracted)

# Execution options affect the graph execution strategy
pipeline = NLPPipeline(model_name="advanced-classifier")
with execution_options(use_parallel=True, max_workers=8):
    result = pipeline(inputs={"text": "Sample document text for analysis"})
```

## Combining Transforms

Ember XCS transforms can be combined for powerful computation patterns. Here are common integration patterns:

### vmap + structural_jit

Combine batch processing with graph optimization:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.transforms.vmap import vmap
from ember.xcs.tracer.structural_jit import structural_jit
from ember.xcs.engine.execution_options import execution_options
from ember.core.registry.specification import Specification

# Define simple operators with proper field declarations
class FeatureExtractor(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = FeatureExtractorSpecification()

    # Field declarations
    feature_type: str

    def __init__(self, *, feature_type: str = "basic") -> None:
        self.feature_type = feature_type

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        text = inputs.get("text", "")
        return {"features": f"Features({self.feature_type}): {text[:20]}..."}

class SentimentAnalyzer(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = SentimentAnalyzerSpecification()

    # Field declarations
    model_name: str

    def __init__(self, *, model_name: str) -> None:
        self.model_name = model_name

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        features = inputs.get("features", "")
        return {"sentiment": f"Sentiment({self.model_name}): {features}"}

# Define a JIT-optimized composite operator
@structural_jit
class TextAnalysisPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = AnalysisPipelineSpecification()

    # Field declarations for sub-operators
    extractor: FeatureExtractor
    analyzer: SentimentAnalyzer

    def __init__(self, *, feature_type: str = "advanced", model_name: str = "sentiment-v2") -> None:
        # Create sub-operators
        self.extractor = FeatureExtractor(feature_type=feature_type)
        self.analyzer = SentimentAnalyzer(model_name=model_name)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Connect operators in the pipeline
        features = self.extractor(inputs=inputs)
        return self.analyzer(inputs=features)

# Create a pipeline instance
pipeline = TextAnalysisPipeline(feature_type="comprehensive", model_name="sentiment-v3")

# Create a vectorized version of the JIT-optimized pipeline
vectorized_pipeline = vmap(pipeline)

# Process a batch with execution options controlling both vectorization and JIT
with execution_options(use_parallel=True, max_workers=8, enable_caching=True):
    results = vectorized_pipeline(inputs={
        "text": [
            "This product exceeded my expectations!",
            "The service was terrible and I want a refund.",
            "Average experience, neither good nor bad."
        ],
        "include_details": True  # Non-batched parameter applied to all items
    })
```

### pmap + vmap (Nested Parallelism)

Parallelize across devices and batch within each device:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.transforms.vmap import vmap
from ember.xcs.transforms.pmap import pmap
from ember.xcs.engine.execution_options import execution_options
from ember.core.registry.specification import Specification

# Define an operator with proper field declarations
class ImageProcessor(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = ImageProcessorSpecification()

    # Field declarations
    model_name: str
    resolution: str

    def __init__(self, *, model_name: str, resolution: str = "high") -> None:
        self.model_name = model_name
        self.resolution = resolution

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Process a single image
        image_data = inputs.get("image", "")
        return {
            "processed": f"Processed({self.model_name}, {self.resolution}): {image_data[:10]}...",
            "metadata": {"format": "png", "size": len(image_data)}
        }

# Create a processor instance
processor = ImageProcessor(model_name="image-model-v2", resolution="medium")

# First apply vmap to handle batches within each worker
vectorized_processor = vmap(processor)

# Then apply pmap to distribute batches across workers
distributed_processor = pmap(vectorized_processor, num_workers=4)

# Structure data as a large batch of images
large_batch = {
    "image": [f"image_data_{i}" for i in range(100)],
    "options": {"format": "json"}  # Non-batched parameter for all images
}

# Execute with additional options
with execution_options(enable_caching=True, timeout_seconds=30.0, device_strategy="auto"):
    # pmap will divide the batch into chunks, and vmap will process each chunk as a batch
    results = distributed_processor(inputs=large_batch)
```

### structural_jit + pmap

Optimize complex operators and distribute them across workers:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.tracer.structural_jit import structural_jit
from ember.xcs.transforms.pmap import pmap
from ember.xcs.engine.execution_options import execution_options
from ember.core.registry.specification import Specification

# Define sub-operators with proper field declarations
class DataPreprocessor(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = PreprocessorSpecification()

    # Field declarations
    preprocessor_type: str

    def __init__(self, *, preprocessor_type: str = "standard") -> None:
        self.preprocessor_type = preprocessor_type

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        data = inputs.get("data", "")
        return {"preprocessed": f"Preprocessed({self.preprocessor_type}): {data[:10]}..."}

class ModelInferencer(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = InferencerSpecification()

    # Field declarations
    model_name: str
    precision: str

    def __init__(self, *, model_name: str, precision: str = "float32") -> None:
        self.model_name = model_name
        self.precision = precision

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        preprocessed = inputs.get("preprocessed", "")
        return {"prediction": f"Prediction({self.model_name}, {self.precision}): {preprocessed}"}

class PostProcessor(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = PostProcessorSpecification()

    # Field declarations
    formatter: str

    def __init__(self, *, formatter: str = "json") -> None:
        self.formatter = formatter

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        prediction = inputs.get("prediction", "")
        return {"result": f"Result({self.formatter}): {prediction}"}

# Define a complex JIT-optimized operator with multiple stages
@structural_jit
class InferencePipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = InferencePipelineSpecification()

    # Field declarations for sub-operators
    preprocessor: DataPreprocessor
    inferencer: ModelInferencer
    postprocessor: PostProcessor

    def __init__(self, *, model_name: str, preprocessor_type: str = "advanced") -> None:
        # Create sub-operators
        self.preprocessor = DataPreprocessor(preprocessor_type=preprocessor_type)
        self.inferencer = ModelInferencer(model_name=model_name)
        self.postprocessor = PostProcessor(formatter="structured")

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Connect operators in the pipeline
        preprocessed = self.preprocessor(inputs=inputs)
        predicted = self.inferencer(inputs=preprocessed)
        return self.postprocessor(inputs=predicted)

# Create a pipeline instance
pipeline = InferencePipeline(model_name="complex-model", preprocessor_type="enhanced")

# Create a distributed version to process multiple items in parallel
distributed_pipeline = pmap(pipeline, num_workers=8)

# Prepare a batch of data items
batch_data = {
    "data": [f"data_sample_{i}" for i in range(50)],
    "config": {"option1": "value1", "option2": "value2"}  # Shared configuration
}

# Execute with device targeting and tracing for performance analysis
with execution_options(device_strategy="gpu", trace_execution=True):
    results = distributed_pipeline(inputs=batch_data)
```

## Performance Considerations

When combining transforms and using execution options, keep these factors in mind:

1. **Worker Count vs. Batch Size**: Tune `max_workers` based on your hardware and batch size. Too many workers for small batches can increase overhead without benefit.

2. **Nested Parallelism Overhead**: When combining `vmap` and `pmap`, be aware of potential thread contention. The total number of threads can grow quickly with the formula:

   ```
   total_threads = pmap_workers × vmap_parallel_operations
   ```

3. **JIT Warm-up**: The first call to a JIT-optimized function includes compilation overhead. Subsequent calls benefit from the cached optimization. For critical applications, consider:

   ```python
   # Warm up the JIT compilation with a small batch
   pipeline(inputs=sample_input)  # Compilation happens here

   # Now process the real data
   with execution_options(use_parallel=True):
       results = pipeline(inputs=actual_input)  # Uses cached compilation
   ```

4. **Memory Utilization**: Larger batches with parallel execution increase memory usage. For memory-intensive operations, consider using smaller batches or fewer workers:

   ```python
   # Memory-efficient processing with controlled batch size
   with execution_options(max_workers=4):
       for mini_batch in chunk_data(large_data, size=16):
           partial_results = vectorized_op(inputs=mini_batch)
           process_results(partial_results)
   ```

5. **Caching Trade-offs**: Enabling `enable_caching` can improve performance for repeated operations but increases memory usage. Consider selectively enabling it:

   ```python
   # Enable caching for the expensive inference step
   with execution_options(enable_caching=True):
       embeddings = embedding_model(inputs=texts)

   # Disable caching for the less expensive post-processing
   with execution_options(enable_caching=False):
       processed = post_processor(inputs=embeddings)
   ```

## Transform Selection Guide

| Need                               | Transform                   | Configuration                                      |
| ---------------------------------- | --------------------------- | -------------------------------------------------- |
| Process batched inputs efficiently | `vmap`                      | `in_axes` to specify batch dimensions              |
| Parallelize across cores           | `pmap`                      | `num_workers` based on CPU cores                   |
| Optimize complex operator graphs   | `structural_jit`            | `execution_strategy="auto"` for adaptive execution |
| Distribute batches across workers  | `pmap(vmap(...))`           | Tune `max_workers` and `in_axes`                   |
| Optimize then distribute           | `pmap(structural_jit(...))` | Use `enable_caching=True` for repeated execution   |

## Best Practices

1. **Prefer the context manager** over global settings to limit the scope of changes.

2. **Match workers to workload and hardware**:  
   Set `max_workers` based on your workload characteristics and available CPU cores. A good starting point is the number of physical cores for compute-bound tasks, or more for I/O-bound tasks.

3. **Consider caching for repeated operations**:  
   Enable caching when the same operation is performed multiple times with identical inputs.

4. **Set appropriate timeouts**:  
   Use `timeout_seconds` to prevent operations from running indefinitely, especially in production environments.

5. **Benchmark different settings**:  
   Test different combinations of options to find the optimal configuration for your specific use case.

6. **Choose the right transform combination**:
   - Use `vmap` for vectorizing operations on batched inputs
   - Use `pmap` for parallel execution across workers
   - Use `structural_jit` for optimizing complex operator compositions
   - Combine transforms based on workload characteristics

## Legacy Support

For compatibility with older code, the `scheduler="sequential"` option is supported and will automatically set `use_parallel=False`. This ensures older code continues to work without modification.

```python
# Legacy approach (still works)
with execution_options(scheduler="sequential"):
    result = vectorized_op(inputs={"prompt": prompt, "seed": seeds})

# Modern approach (preferred)
with execution_options(use_parallel=False):
    result = vectorized_op(inputs={"prompt": prompt, "seed": seeds})
```

## Exception Handling

Invalid option names will raise a `ValueError` with a message indicating the invalid option. This helps catch configuration errors early in development:

```python
try:
    with execution_options(invalid_option=True):
        result = op(inputs=data)
except ValueError as e:
    print(f"Configuration error: {e}")  # Output: Configuration error: Invalid execution option: invalid_option
```

</code>

docs\xcs\JIT_OVERVIEW.md:
<code>

# XCS JIT System Overview

This document provides a comprehensive explanation of Ember's Just-In-Time (JIT) compilation system and how its components work together to optimize operator execution.

## Core Components

Ember's JIT system consists of three complementary components that represent an evolution in approach and capabilities:

### 1. Autograph Context Manager

```python
from ember.api.xcs import autograph, execute

with autograph() as graph:
    # Operations recorded but not executed
    result1 = op1(inputs={"query": "Example"})
    result2 = op2(inputs=result1)

# Execute the recorded graph
results = execute(graph)
```

**Key Characteristics:**

- Manual, explicit approach to graph building
- Provides fine-grained control over graph construction
- Separates graph construction from execution
- Requires explicit execute() call to run the graph
- First-generation approach requiring more user involvement

**When to Use:**

- When you need explicit control over graph construction
- For debugging execution paths
- When you want to construct a graph once and execute it multiple times
- To create execution graphs for visualization or analysis

### 2. JIT Decorator (`@jit`)

```python
from ember.api.xcs import jit

@jit
class MyOperator(Operator):
    def forward(self, *, inputs):
        # Implementation
        return processed_result
```

**Key Characteristics:**

- Traces actual execution to identify operator dependencies
- Records operator calls, inputs, and outputs
- Uses AutoGraphBuilder to construct optimized graphs from traces
- Caches compiled graphs for repeated use
- Execution-based approach that adapts to runtime behavior

**When to Use:**

- For most operator optimization needs
- When execution patterns vary based on inputs
- When you need automatic tracing of actual behavior
- For operators with complex, dynamic execution patterns

### 3. Structural JIT (`@structural_jit`)

```python
from ember.api.xcs import structural_jit

@structural_jit(execution_strategy="parallel")
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()

    def forward(self, *, inputs):
        intermediate = self.op1(inputs=inputs)
        result = self.op2(inputs=intermediate)
        return result
```

**Key Characteristics:**

- Analyzes operator structure without requiring execution
- Examines the composition of operators directly
- Identifies potential parallelism through structural analysis
- More advanced approach that can optimize before first execution
- Supports multiple execution strategies (auto, parallel, sequential)

**When to Use:**

- For complex composite operators with many subcomponents
- When operator structure is known and static
- For maximum optimization of operator composition
- To parallelize independent operations in composite operators

## Relationship Between Components

These three components represent an evolution in Ember's JIT system:

1. **autograph** - First generation: manual graph building
2. **jit** - Second generation: automatic tracing-based optimization
3. **structural_jit** - Third generation: sophisticated structure-based optimization

While they evolved chronologically, they are **complementary** rather than competitive, each with strengths in different scenarios.

### Implementation Relationship

- **AutoGraphBuilder** is a core class used by `jit` to build graphs from trace records
- **structural_jit** uses its own graph building approach via `_build_xcs_graph_from_structure`
- Both ultimately generate XCSGraph objects executed by the same engine
- **execution_options** context manager works with all approaches to control execution parameters

## Technical Integration Details

### Tracing System

The JIT decorator integrates with the tracing system through:

1. **TracerContext** - Captures operator execution traces
2. **TraceRecord** - Stores inputs, outputs, and metadata for each operation
3. **AutoGraphBuilder** - Analyzes trace records to identify dependencies between operators

### Dependency Analysis

Different approaches to dependency identification:

- **JIT**: Identifies dependencies based on observed data flow during execution
- **Structural JIT**: Identifies dependencies based on operator structure and composition

### Graph Execution

Both systems build XCSGraph objects that can be executed with different schedulers:

- **TopologicalScheduler** - Serial execution following dependencies
- **TopologicalSchedulerWithParallelDispatch** - Parallel execution where possible

## Usage Recommendations

For most users, we recommend using the decorators in this order of preference:

1. Start with `@jit` for most operators
2. Use `@structural_jit` for complex composite operators where structure is known
3. Use `autograph` only when you need explicit control over graph construction

For advanced users, consider:

- Combining approaches by using `@jit` for basic operators and `@structural_jit` for compositions
- Using `execution_options` to fine-tune execution behavior
- Testing different strategies to determine which works best for your specific operators

## Future Directions

Future improvements to the JIT system include:

1. Tighter integration between the three approaches
2. Better heuristics for automatic execution strategy selection
3. Improved integration with transforms (vmap, pmap)
4. Enhanced data dependency analysis
5. Advanced caching and plan reuse

## Examples

For practical demonstrations of these approaches, see:

- `src/ember/examples/xcs/jit_example.py` - Using the `@jit` decorator
- `src/ember/examples/xcs/enhanced_jit_example.py` - Advanced JIT features
- `src/ember/examples/xcs/auto_graph_example.py` - Using autograph for manual graph building
- `src/ember/examples/xcs/auto_graph_simplified.py` - Simple demonstration without LLM dependencies
  </code>

docs\xcs\PERFORMANCE_GUIDE.md:
<code>

# XCS Performance Optimization Guide

This guide provides recommendations for optimizing the performance of code using the XCS framework.

## Performance Principles

When optimizing code with XCS, follow these general principles:

1. **Identify Hot Spots**: Focus on optimizing the portions of your code that consume the most time
2. **Minimize Data Movement**: Reduce copying of large data structures between operations
3. **Increase Parallelism**: Make use of parallelization transforms where appropriate
4. **Batch Processing**: Process items in batches rather than individually
5. **Precompile Operations**: Use precompilation for frequently executed operations

## JIT Optimization

The `jit` decorator is a powerful tool for optimizing operator execution:

```python
from ember.api.xcs import jit, JITOptions

@jit(options=JITOptions(
    sample_input={"query": "example"},  # Precompile with sample input
    cache_size=100                      # Increase cache size for varied inputs
))
class MyOperator(Operator):
    def forward(self, *, inputs):
        # Complex computation
        return processed_result
```

### JIT Best Practices

1. **Precompile with Sample Inputs**: Provide sample inputs to precompile operations
2. **Set Appropriate Cache Size**: Increase cache size for operators with varied input patterns
3. **Minimize Side Effects**: Avoid operations with side effects in JIT-compiled code
4. **Move Invariant Computations**: Place invariant computations in `__init__` instead of `forward`
5. **Use Pure Functions**: Favor pure functions that depend only on their inputs

## Structural Analysis

For complex operators that compose multiple sub-operators, use `structural_jit`:

```python
from ember.api.xcs import structural_jit

@structural_jit(execution_strategy="parallel")
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()
        self.op3 = SubOperator3()

    def forward(self, *, inputs):
        # Operations automatically parallelized based on dependencies
        result1 = self.op1(inputs=inputs)
        result2 = self.op2(inputs=inputs)
        result3 = self.op3(inputs=result1, auxiliary=result2)
        return result3
```

### Structural JIT Best Practices

1. **Expose Parallelism**: Structure code to expose potential parallelism
2. **Keep Operators Small**: Use many small operators rather than few large ones
3. **Clear Dependencies**: Make data dependencies explicit in operator interfaces
4. **Avoid Circular Dependencies**: Structure operators to avoid circular dependencies
5. **Consider Granularity**: Balance parallelism with overhead (too fine-grained can be slower)

## Parallelization Transforms

Use transforms to enable parallel processing:

```python
from ember.api.xcs import vmap, pmap

# Vectorize for batch processing
batch_fn = vmap(single_item_function)
batch_results = batch_fn([item1, item2, item3])

# Parallelize across devices/cores
parallel_fn = pmap(heavy_computation, num_workers=8)
parallel_results = parallel_fn(large_dataset)
```

### Transforms Best Practices

1. **Right-Size Batches**: Choose batch sizes that balance parallelism with memory usage
2. **Tune Worker Count**: Adjust `num_workers` based on your workload and available cores
3. **Separate Constants**: Use `in_axes` to mark constant arguments in `vmap`
4. **Consider Work Size**: Use `pmap` only when work units are substantial enough
5. **Combine Transforms**: Compose transforms when appropriate (e.g., `pmap(vmap(f))`)

## Distributed Execution

For very large workloads, use mesh-based distribution:

```python
from ember.api.xcs import mesh_sharded, DeviceMesh, PartitionSpec

# Create a device mesh
mesh = DeviceMesh(shape=(2, 2))

# Define partitioning strategy
pspec = PartitionSpec(0, 1)

# Create a sharded function
sharded_fn = mesh_sharded(function, mesh=mesh, partition_spec=pspec)
results = sharded_fn(large_dataset)
```

### Distributed Execution Best Practices

1. **Choose Mesh Topology**: Select a mesh shape that matches your computation pattern
2. **Select Partition Strategy**: Choose partitioning that minimizes communication
3. **Data Preparation**: Organize data to align with your partitioning strategy
4. **Load Balancing**: Ensure work is distributed evenly across devices
5. **Minimize Communication**: Structure algorithms to reduce cross-device communication

## Execution Configuration

Fine-tune execution behavior with execution options:

```python
from ember.xcs.engine.execution_options import execution_options

with execution_options(
    use_parallel=True,      # Control parallel execution
    max_workers=8,          # Set concurrency level
    timeout_seconds=30.0    # Set execution timeout
):
    results = complex_operation(data)
```

### Execution Configuration Best Practices

1. **Match Execution Mode to Workload**: Use `use_parallel=True` for parallelizable work, `use_parallel=False` for linear work
2. **Control Concurrency**: Set `max_workers` based on your system's capabilities
3. **Set Timeouts**: Use `timeout_seconds` to prevent runaway computations
4. **Local Configuration**: Apply configuration at the smallest scope needed
5. **Monitoring**: Use `trace_execution=True` to identify performance bottlenecks

## Memory Optimization

Optimize memory usage for better performance:

1. **Avoid Redundant Copies**: Reuse data structures when possible
2. **Use Views/References**: Prefer views over copies for large data
3. **Clean Up Resources**: Release resources when no longer needed
4. **Preallocate**: Preallocate buffers for known-size operations
5. **Incremental Processing**: Process large datasets in chunks

## Common Bottlenecks

Watch for these common performance bottlenecks:

1. **Excessive Tracing**: Repeated tracing of the same operation patterns
2. **Serialization Overhead**: Converting between formats unnecessarily
3. **Unbalanced Work Distribution**: Some workers idle while others overloaded
4. **Memory Contention**: Multiple operations competing for memory bandwidth
5. **Dependency Chains**: Long chains of dependent operations that can't parallelize

## Performance Analysis

To identify performance issues:

1. **Use XCS Tracing**: Enable execution tracing to see operation timing
2. **Profile Time Distribution**: Identify which operations take the most time
3. **Analyze Parallelism**: Check if operations are executing in parallel
4. **Check Cache Hit Rates**: Monitor JIT compilation cache hit rates
5. **Examine Memory Usage**: Watch for excessive memory allocations
   </code>

docs\xcs\QUICKSTART.md:
<code>

# XCS Quickstart Guide

This guide provides a quick introduction to using the XCS module for high-performance execution of computational graphs in Ember.

## Installation

XCS is included with Ember. Make sure you have the latest version:

```bash
pip install -U ember
```

## Basic Usage

## XCS JIT System

Ember provides three complementary approaches to optimizing operator execution:

### 1. JIT Compilation with @jit

The `jit` decorator uses execution tracing to automatically optimize operators:

```python
from ember.api.xcs import jit
from ember.api.operator import Operator

@jit
class SimpleOperator(Operator):
    def forward(self, *, inputs):
        return {"result": inputs["text"].upper()}

# Create and use the operator
op = SimpleOperator()
result = op(inputs={"text": "hello world"})
print(result)  # {"result": "HELLO WORLD"}
```

The `jit` decorator:

- Traces actual execution to identify dependencies
- Builds optimized execution graphs
- Caches compiled graphs for repeated use
- Works best for operators with dynamic execution patterns

### 2. Structural JIT for Complex Operator Compositions

For operators with known internal structure, the `structural_jit` decorator provides optimizations without requiring execution:

```python
from ember.api.xcs import structural_jit
from ember.api.operator import Operator

@structural_jit(execution_strategy="parallel")
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = FirstOperator()
        self.op2 = SecondOperator()

    def forward(self, *, inputs):
        intermediate = self.op1(inputs=inputs)
        result = self.op2(inputs=intermediate)
        return result
```

The `structural_jit` decorator:

- Analyzes operator structure directly
- Identifies potential parallelism through structural analysis
- Supports multiple execution strategies (auto, parallel, sequential)
- Works best for complex composite operators

### 3. Explicit Graph Building with autograph

For maximum control, you can build execution graphs explicitly:

```python
from ember.api.xcs import autograph, execute

# Define some operators
op1 = FirstOperator()
op2 = SecondOperator()

# Build a graph
with autograph() as graph:
    intermediate = op1(inputs={"query": "Example"})
    result = op2(inputs=intermediate)

# Execute the graph with optimized scheduling
results = execute(graph)
print(results)
```

This separates graph definition from execution, enabling optimization across operator boundaries.

### Parallelization Transforms

XCS provides transforms for vectorization and parallelization:

```python
from ember.api.xcs import vmap, pmap

# Vectorize a function to process batches
def process_item(item):
    return item * 2

batch_fn = vmap(process_item)
batch_results = batch_fn([1, 2, 3])  # [2, 4, 6]

# Parallelize a function across cores
def slow_computation(x):
    # Heavy computation here
    return processed_result

parallel_fn = pmap(slow_computation)
parallel_results = parallel_fn([data1, data2, data3, data4])
```

## Advanced Features

### Structural JIT

For complex operators that compose multiple sub-operators:

```python
from ember.api.xcs import structural_jit

@structural_jit
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()

    def forward(self, *, inputs):
        # Operations automatically parallelized based on structure
        intermediate = self.op1(inputs=inputs)
        result = self.op2(inputs=intermediate)
        return result
```

The `structural_jit` decorator analyzes operator structure and automatically parallelizes independent operations.

### Configuration Options

Fine-tune behavior with configuration options:

```python
from ember.api.xcs import jit, JITOptions, XCSExecutionOptions

# Configure JIT with precompilation
@jit(options=JITOptions(
    sample_input={"query": "example"},
    cache_size=100
))
class OptimizedOperator(Operator):
    def forward(self, *, inputs):
        return process_complex_input(inputs)

# Configure execution environment
with XCSExecutionOptions(
    scheduler="parallel",
    max_workers=4,
    timeout=30.0
):
    results = complex_operation(data)
```

## Common Patterns

### Sequential Pipeline

```python
@jit
class PipelineOperator(Operator):
    def __init__(self):
        self.op1 = Stage1Operator()
        self.op2 = Stage2Operator()
        self.op3 = Stage3Operator()

    def forward(self, *, inputs):
        stage1_result = self.op1(inputs=inputs)
        stage2_result = self.op2(inputs=stage1_result)
        stage3_result = self.op3(inputs=stage2_result)
        return stage3_result
```

### Parallel Branches

```python
@structural_jit
class ParallelBranchOperator(Operator):
    def __init__(self):
        self.branch1 = Branch1Operator()
        self.branch2 = Branch2Operator()
        self.merger = MergeOperator()

    def forward(self, *, inputs):
        # These two operations run in parallel
        branch1_result = self.branch1(inputs=inputs)
        branch2_result = self.branch2(inputs=inputs)

        # Merge the results
        final_result = self.merger(
            inputs={"branch1": branch1_result, "branch2": branch2_result}
        )
        return final_result
```

### Batch Processing

```python
def process_item(item):
    # Process a single item
    return processed_item

# Create a batched version
batch_processor = vmap(process_item)

# Process a batch at once
results = batch_processor([item1, item2, item3, item4])
```

## Next Steps

- See the [API Reference](API_REFERENCE.md) for detailed function documentation
- Read the [JIT Overview](JIT_OVERVIEW.md) for a comprehensive explanation of the different JIT approaches
- Check the [Architecture Overview](ARCHITECTURE.md) for system design details
- Read the [Performance Guide](PERFORMANCE_GUIDE.md) for optimization tips
- Explore the [Transforms Documentation](TRANSFORMS.md) for advanced parallelization
  </code>

docs\xcs\README.md:
<code>

# Ember XCS: High-Performance Execution Framework

The Ember XCS (Accelerated Compound Systems) module provides a high-performance execution framework for computational graphs, enabling intelligent scheduling, just-in-time tracing, and advanced parallel execution transformations.

## Core Features

- **Just-In-Time Compilation**: Automatically optimize operator execution paths
- **Intelligent Graph Building**: Create and execute computational graphs
- **Parallelization Transforms**: Vectorize and parallelize operations
- **Structural Analysis**: Analyze operator structures for optimized execution
- **Distributed Computing**: Support for mesh-based distribution

## Architecture

XCS is designed with a clean, modular architecture consisting of these key components:

- **Tracer**: JIT compilation and structural analysis
- **Engine**: Efficient execution scheduling and runtime management
- **Graph**: Computational graph representation and manipulation
- **Transforms**: Function transformations for vectorization and parallelization
- **Utils**: Common utility functions and tree manipulation tools

The system is built on functional programming principles and follows SOLID design patterns throughout its implementation.

For a detailed explanation of the JIT system architecture and the relationship between autograph, jit, and structural_jit, see [JIT Overview](JIT_OVERVIEW.md).

## Usage Examples

### Just-In-Time Compilation

```python
from ember.api.xcs import jit

@jit
class MyOperator(Operator):
    def forward(self, *, inputs):
        # Complex computation
        return {"result": processed_data}
```

### Automatic Graph Building

```python
from ember.api.xcs import autograph, execute

with autograph() as graph:
    # Operations are traced, not executed
    result1 = op1(inputs={"query": "Example"})
    result2 = op2(inputs=result1)

# Execute the graph with optimized scheduling
results = execute(graph)
```

### Function Transforms

```python
from ember.api.xcs import vmap, pmap

# Vectorize a function to process batches
batch_fn = vmap(single_item_function)
batch_results = batch_fn([item1, item2, item3])

# Parallelize across multiple cores/devices
parallel_fn = pmap(heavy_computation)
parallel_results = parallel_fn(large_dataset)
```

### Structural JIT

```python
from ember.api.xcs import structural_jit

@structural_jit
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()

    def forward(self, *, inputs):
        # Operations automatically parallelized based on structure
        intermediate = self.op1(inputs=inputs)
        result = self.op2(inputs=intermediate)
        return result
```

### Advanced Configuration

```python
from ember.api.xcs import XCSExecutionOptions, JITOptions

# Configure JIT compilation with precompilation
@jit(options=JITOptions(
    sample_input={"query": "example"},
    cache_size=100
))
class OptimizedOperator(Operator):
    def forward(self, *, inputs):
        return process_complex_input(inputs)

# Configure execution environment
with XCSExecutionOptions(
    scheduler="parallel",
    max_workers=4,
    timeout=30.0
):
    results = complex_operation(data)
```

## Simplified Imports

XCS provides a clean, simplified import structure through the `ember.api.xcs` module. All key functionality is available with short, intuitive imports:

```python
from ember.api.xcs import jit, vmap, pmap, autograph, execute, structural_jit
```

For more details, see [Simplified Imports](SIMPLIFIED_IMPORTS.md).

## Additional Resources

- Check the [examples directory](../../src/ember/examples/xcs) for practical demonstrations
- See the source code for implementation details
- Refer to the project README for overall framework information
  </code>

docs\xcs\SIMPLIFIED_IMPORTS.md:
<code>

# Simplified XCS Import Structure

The XCS (Accelerated Compound Systems) module lays a groundwork for high-performance execution capabilities for operator graphs, just-in-time tracing, and parallel execution transformations.

## Before: Complex Import Paths

Previously, we needed to remember specific import paths:

```python
from ember.xcs.tracer.tracer_decorator import jit
from ember.xcs.tracer.autograph import AutoGraphBuilder
from ember.xcs.engine.xcs_engine import execute_graph
from ember.xcs.transforms.vmap import vmap
from ember.xcs.transforms.pmap import pmap
```

## After: Simpler, More Intuitive Imports

Now, all functionality is available through a top-level API:

```python
# Simple function imports
from ember.api.xcs import jit, vmap, pmap, autograph, execute

# Or use the unified API
from ember.api.xcs import xcs

result = xcs.execute(graph, inputs={"query": "Example"})
```

## Design Principles

The new import structure follows key design principles:

1. **Single Responsibility**: Each function has a clear, focused purpose
2. **Open for Extension**: The API can be extended without modifying existing code
3. **Liskov Substitution**: Types are properly defined with consistent interfaces
4. **Interface Segregation**: Clean, minimal interfaces for different use cases
5. **Dependency Inversion**: Components depend on abstractions rather than details

## Benefits

- **Discoverability**: Functions are easy to find and use
- **Consistency**: Follows the pattern of other Ember modules
- **Simplicity**: Hides implementation details behind a clean interface
- **Extensibility**: New functionality can be added without breaking changes
- **Backward Compatibility**: Old import paths still work

This redesign makes the XCS module more accessible while maintaining all its power and flexibility.
</code>

docs\xcs\TRANSFORMS.md:
<code>

# XCS Transforms

XCS provides powerful transformation primitives that enable vectorization, parallelization, and distributed execution of computations. This document provides a detailed overview of the available transforms and their usage.

## Core Transform Concepts

Transforms in XCS follow these core principles:

1. **Function Transformation**: Transforms take functions as input and return transformed functions
2. **Semantic Preservation**: Transformed functions preserve the semantics of the original function
3. **Composability**: Transforms can be composed to create more complex transformations
4. **Explicit Configuration**: Transforms provide clear configuration options

## Available Transforms

### Vectorized Mapping (vmap)

The `vmap` transform enables automatic vectorization of functions to process batched inputs:

```python
from ember.api.xcs import vmap

def process_item(x):
    return x * 2

# Create a vectorized version
batch_process = vmap(process_item)

# Process multiple items at once
results = batch_process([1, 2, 3])  # [2, 4, 6]
```

#### Advanced vmap Features

**Handling Multiple Arguments**:

```python
def process_pairs(x, y):
    return x + y

# Vectorize over both arguments
vectorized = vmap(process_pairs)
result = vectorized([1, 2, 3], [10, 20, 30])  # [11, 22, 33]
```

**Specifying Axes**:

```python
# Vectorize only the first argument
vectorized = vmap(process_pairs, in_axes=(0, None))
result = vectorized([1, 2, 3], 10)  # [11, 12, 13]
```

**Nested Batching**:

```python
# Vectorize in two dimensions
double_vectorized = vmap(vmap(process_item))
result = double_vectorized([[1, 2], [3, 4]])  # [[2, 4], [6, 8]]
```

### Parallel Mapping (pmap)

The `pmap` transform enables parallel execution across multiple cores:

```python
from ember.api.xcs import pmap
import time

def slow_computation(x):
    time.sleep(1)  # Simulate work
    return x * 2

# Create a parallelized version
parallel_process = pmap(slow_computation)

# Process items in parallel (much faster than sequential)
results = parallel_process([1, 2, 3, 4])  # [2, 4, 6, 8]
```

#### Advanced pmap Features

**Controlling Worker Count**:

```python
# Specify number of worker threads
parallel_process = pmap(slow_computation, num_workers=4)
```

**Error Handling**:

```python
from ember.api.xcs import TransformOptions

# Configure error handling
options = TransformOptions(propagate_errors=True)
parallel_process = pmap(risky_function, options=options)
```

**Timeouts**:

```python
# Add timeout to prevent hanging
options = TransformOptions(timeout=5.0)
parallel_process = pmap(slow_computation, options=options)
```

### Structural JIT (structural_jit)

The `structural_jit` decorator analyzes operator structure and constructs an optimized execution graph:

```python
from typing import Dict, Any, ClassVar
from ember.xcs.tracer.structural_jit import structural_jit
from ember.core.registry.specification import Specification

# Define a composite operator with proper field declarations
@structural_jit
class CompositeOperator(Operator[Dict[str, Any], Dict[str, Any]]):
    # Class-level specification
    specification: ClassVar[Specification] = CompositeSpecification()

    # Field declarations
    op1: SubOperator1
    op2: SubOperator2

    def __init__(self, *, config_param: str) -> None:
        # Initialize sub-operators
        self.op1 = SubOperator1(param=config_param)
        self.op2 = SubOperator2(param=config_param)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Multi-step computation that gets optimized
        intermediate = self.op1(inputs=inputs)
        return self.op2(inputs=intermediate)
```

#### Advanced structural_jit Features

**Execution Strategy Control**:

```python
# Control the execution strategy
@structural_jit(execution_strategy="parallel", max_workers=8)
class ParallelPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    # Implementation...
```

**Caching Control**:

```python
# Control graph caching behavior
@structural_jit(cache_graph=True)
class CachedPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    # Implementation...
```

**Runtime Control**:

```python
# Create an optimized operator
pipeline = OptimizedPipeline()

# Disable JIT for debugging
pipeline.disable_jit()

# Re-enable JIT
pipeline.enable_jit()

# Clear graph cache to force recompilation
pipeline.clear_graph_cache()
```

### Mesh Sharding (mesh_sharded)

The `mesh_sharded` transform enables distributed execution across a device mesh:

```python
from ember.api.xcs import mesh_sharded, DeviceMesh, PartitionSpec

# Create a 2D device mesh
mesh = DeviceMesh(shape=(2, 2))

# Specify how to partition the data
pspec = PartitionSpec(0, 1)  # Partition along both dimensions

# Create a sharded function
sharded_fn = mesh_sharded(heavy_computation, mesh=mesh, partition_spec=pspec)

# Execute with data distributed across the mesh
result = sharded_fn(large_data)
```

#### Advanced Mesh Features

**Custom Device Specification**:

```python
# Specify devices explicitly
mesh = DeviceMesh(
    devices=["gpu:0", "gpu:1", "cpu:0", "cpu:1"],
    shape=(2, 2)
)
```

**Partial Sharding**:

```python
# Only shard along first dimension
pspec = PartitionSpec(0, None)
```

**Nested Meshes**:

```python
# Create hierarchical meshes
outer_mesh = DeviceMesh(shape=(2,))
inner_mesh = DeviceMesh(shape=(2,))

# Compose sharding transformations
outer_sharded = mesh_sharded(inner_sharded_fn, mesh=outer_mesh, partition_spec=PartitionSpec(0))
```

## Combining Transforms

Transforms can be combined to create more powerful transformations. This section provides detailed examples of common integration patterns.

### vmap + pmap (Batch Processing with Parallelism)

```python
from typing import Dict, Any, ClassVar
from ember.api.xcs import vmap, pmap
from ember.core.registry.specification import Specification

# Define an operator with proper field declarations
class TextProcessor(Operator[Dict[str, Any], Dict[str, Any]]):
    specification: ClassVar[Specification] = TextProcessorSpecification()
    model_name: str

    def __init__(self, *, model_name: str) -> None:
        self.model_name = model_name

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        text = inputs.get("text", "")
        return {"processed": f"Processed with {self.model_name}: {text[:20]}..."}

# Create operator instance
processor = TextProcessor(model_name="nlp-model")

# First vectorize to handle batches
vectorized_processor = vmap(processor)

# Then parallelize to distribute across workers
distributed_processor = pmap(vectorized_processor, num_workers=4)

# Process a large batch
large_batch = {
    "text": [f"Document {i}" for i in range(100)],
    "options": {"format": "plain"}  # Non-batched parameter
}

# This will:
# 1. Divide the batch into chunks (pmap)
# 2. Process each chunk as a batch (vmap)
# 3. Combine all results
results = distributed_processor(inputs=large_batch)
```

### structural_jit + vmap (Optimized Batch Processing)

```python
from typing import Dict, Any, ClassVar
from ember.api.xcs import vmap
from ember.xcs.tracer.structural_jit import structural_jit
from ember.core.registry.specification import Specification

# Define component operators
class FeatureExtractor(Operator[Dict[str, Any], Dict[str, Any]]):
    specification: ClassVar[Specification] = ExtractorSpecification()
    extractor_type: str

    def __init__(self, *, extractor_type: str) -> None:
        self.extractor_type = extractor_type

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        text = inputs.get("text", "")
        return {"features": f"Features({self.extractor_type}): {text[:20]}..."}

class Classifier(Operator[Dict[str, Any], Dict[str, Any]]):
    specification: ClassVar[Specification] = ClassifierSpecification()
    model_name: str
    threshold: float

    def __init__(self, *, model_name: str, threshold: float = 0.5) -> None:
        self.model_name = model_name
        self.threshold = threshold

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        features = inputs.get("features", "")
        return {"classification": f"Classified({self.model_name}, {self.threshold}): {features}"}

# Define complex pipeline with optimization
@structural_jit(execution_strategy="auto", max_workers=8)
class ClassificationPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    specification: ClassVar[Specification] = PipelineSpecification()
    extractor: FeatureExtractor
    classifier: Classifier

    def __init__(self, *, model_name: str) -> None:
        self.extractor = FeatureExtractor(extractor_type="advanced")
        self.classifier = Classifier(model_name=model_name)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # This execution flow gets optimized by structural_jit
        features = self.extractor(inputs=inputs)
        return self.classifier(inputs=features)

# Create optimized pipeline
pipeline = ClassificationPipeline(model_name="bert-classifier")

# Add batch processing capability
vectorized_pipeline = vmap(pipeline)

# Process a batch through the optimized pipeline
batch_inputs = {
    "text": [
        "This product is excellent!",
        "Service was terrible.",
        "Average experience, could be better."
    ]
}

# This will:
# 1. Use the optimized execution graph from structural_jit
# 2. Apply that optimized execution to each item in the batch
results = vectorized_pipeline(inputs=batch_inputs)
```

### structural_jit + pmap (Distributed Complex Processing)

```python
from typing import Dict, Any, ClassVar
from ember.xcs.tracer.structural_jit import structural_jit
from ember.api.xcs import pmap
from ember.core.registry.specification import Specification

# Define a complex operator with optimization
@structural_jit(execution_strategy="parallel")
class ProcessingPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    specification: ClassVar[Specification] = ProcessingSpecification()
    stage1: Stage1Operator
    stage2: Stage2Operator
    stage3: Stage3Operator

    def __init__(self, *, config: str) -> None:
        self.stage1 = Stage1Operator(config=config)
        self.stage2 = Stage2Operator(config=config)
        self.stage3 = Stage3Operator(config=config)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Multi-stage execution optimized by structural_jit
        intermediate1 = self.stage1(inputs=inputs)
        intermediate2 = self.stage2(inputs=intermediate1)
        return self.stage3(inputs=intermediate2)

# Create pipeline
pipeline = ProcessingPipeline(config="high-throughput")

# Parallelize the entire pipeline
parallel_pipeline = pmap(pipeline, num_workers=4)

# Process multiple items through distributed pipelines
batch_inputs = {
    "data": [f"Data{i}" for i in range(16)],
    "config": {"quality": "high"}  # Shared config
}

# This will:
# 1. Distribute items across workers (pmap)
# 2. Each worker processes items using the optimized graph (structural_jit)
results = parallel_pipeline(inputs=batch_inputs)
```

### Integration with Execution Options

Use execution options to control transform behavior at runtime:

```python
from ember.api.xcs import vmap, pmap
from ember.xcs.engine.execution_options import execution_options
from ember.xcs.tracer.structural_jit import structural_jit

# Define optimized operator
@structural_jit
class ComplexOperator(Operator[Dict[str, Any], Dict[str, Any]]):
    # Implementation...

# Apply transforms
op = ComplexOperator()
vectorized_op = vmap(op)

# Fine-tune execution with options
with execution_options(
    use_parallel=True,         # Enable parallel execution
    max_workers=8,             # Set worker count
    enable_caching=True,       # Cache intermediate results
    timeout_seconds=30.0,      # Set timeout
    device_strategy="auto"     # Auto device selection
):
    results = vectorized_op(inputs=batch_inputs)
```

### Transformation Order

The order of transforms matters and affects performance characteristics:

- `pmap(vmap(f))`: Each worker processes a batch (often more efficient)

  - Lower overhead as each worker handles multiple items
  - Better for memory-bound operations
  - Good for large batches with moderate per-item processing

- `vmap(pmap(f))`: Each element processed in parallel (higher parallelism)
  - Higher overhead from more worker coordination
  - Better for compute-bound operations
  - Good for small batches with intensive per-item processing

### Integration with JIT System

Transforms can be combined with Ember's JIT system for additional optimizations:

```python
from ember.api.xcs import jit, vmap

# JIT-compiled vectorized function
@jit
def process_item(x):
    return expensive_computation(x)

# Vectorized version with JIT optimization
batch_process = vmap(process_item)
```

The relationship between transforms and the different JIT approaches (jit, structural_jit, autograph) is described in more detail in [JIT Overview](JIT_OVERVIEW.md).

## Performance Optimization Patterns

### Memory-Efficient Processing for Large Datasets

```python
from ember.api.xcs import vmap
from ember.xcs.engine.execution_options import execution_options

def process_in_chunks(large_dataset, chunk_size=64):
    vectorized_op = vmap(base_operator)
    all_results = []

    # Process in controlled-size chunks to manage memory
    for i in range(0, len(large_dataset), chunk_size):
        chunk = large_dataset[i:i+chunk_size]

        # Control parallelism and caching based on chunk
        with execution_options(max_workers=4, enable_caching=(i == 0)):
            # First chunk can be cached as a template
            chunk_results = vectorized_op(inputs={"data": chunk})
            all_results.append(chunk_results)

    # Combine all results
    return combine_results(all_results)
```

### JIT Warm-up for Latency-Sensitive Applications

```python
from ember.xcs.tracer.structural_jit import structural_jit

@structural_jit
class CriticalPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
    # Implementation...

# Application startup: warm up the JIT compilation
pipeline = CriticalPipeline()
_ = pipeline(inputs=warm_up_sample)  # Compilation happens here

# Later, during time-critical operation
def process_critical_request(request_data):
    # Uses cached compilation, minimal latency
    return pipeline(inputs=request_data)
```

## Transform Implementation Details

### vmap Implementation

The `vmap` transform works by:

1. Splitting the input batch into individual elements
2. Applying the original function to each element
3. Combining the results into a batched output

For optimization, it:

- Uses vectorized operations when available
- Employs batch processing primitives
- Handles nested data structures correctly

### pmap Implementation

The `pmap` transform works by:

1. Creating a thread pool of worker threads
2. Dividing work among workers
3. Executing the function on each worker
4. Combining results in the original order

It automatically handles:

- Thread creation and management
- Work distribution
- Result aggregation
- Error propagation

### structural_jit Implementation

The `structural_jit` transform works by:

1. Analyzing the operator's structure through attributes and composition
2. Building a directed graph of operations
3. Detecting execution dependencies
4. Creating an optimized execution plan based on the graph
5. Caching the plan for repeated execution

It provides:

- Dependency-aware execution
- Adaptive parallelization
- Operation reordering when safe
- Graph caching for performance

### mesh_sharded Implementation

The `mesh_sharded` transform works by:

1. Partitioning input data according to the partition spec
2. Mapping partitions to devices in the mesh
3. Executing the function on each device with its partition
4. Gathering and combining the results

## Best Practices

### Transform Selection Guide

| Scenario                        | Recommended Transform | Configuration                                      |
| ------------------------------- | --------------------- | -------------------------------------------------- |
| Batch vectorization             | `vmap`                | Use `in_axes` to control batching dimensions       |
| Parallel execution              | `pmap`                | Set `num_workers` based on CPU cores               |
| Complex operator optimization   | `structural_jit`      | Choose appropriate `execution_strategy`            |
| Distributing computation        | `mesh_sharded`        | Configure `PartitionSpec` for data distribution    |
| Large dataset processing        | `vmap` + `pmap`       | Apply `pmap(vmap(f))` with chunking                |
| Memory-constrained environments | Chunked processing    | Process in smaller batches with controlled workers |

### General Recommendations

1. **Start Simple**: Begin with individual transforms before combining them.

2. **Benchmark Configurations**: Test different transform combinations and parameters.

3. **Match to Hardware**:

   - For CPU-bound tasks: Use `pmap` with `num_workers` set to physical core count
   - For memory-bound tasks: Use controlled batch sizes with `vmap`
   - For complex pipelines: Use `structural_jit` with appropriate execution strategy

4. **Consider Overhead**:

   - Transforming very small operations may not be worth the overhead
   - For simple operations, batch processing may be more efficient than parallelism

5. **Control Resources**:

   - Use execution options to limit worker count for shared systems
   - Set appropriate timeouts for production environments
   - Enable caching selectively for repetitive operations

6. **Debug Transformed Code**:

   - Use `disable_structural_jit()` to temporarily disable JIT for debugging
   - Enable `trace_execution=True` to get execution information
   - Fall back to sequential execution to isolate issues

7. **Layer Transforms Appropriately**:
   - Optimize with `structural_jit` first for complex operations
   - Apply `vmap` at the appropriate level (whole pipeline vs. components)
   - Use `pmap` at the outermost level when possible to minimize thread creation

### Implementation Details and Performance Tuning

For more information on performance optimization with transforms, see [Performance Guide](PERFORMANCE_GUIDE.md).

For detailed execution control options, see [Execution Options](EXECUTION_OPTIONS.md).

For information about the JIT system that powers structural_jit, see [JIT Overview](JIT_OVERVIEW.md).
</code>

docs\CONTEXT_SYSTEM.md:
<code>

# Ember Context System

## Overview

The Ember Context System provides a lightweight, zero-overhead approach to component discovery and dependency management. This redesigned system eliminates circular dependencies, improves thread safety, and enhances performance while significantly reducing complexity.

## Key Concepts

### Registry

The `Registry` is the only global abstraction in the system. It provides a thread-local dictionary that components use to discover each other:

```python
from ember.core.context import Registry

# Get current thread's registry
registry = Registry.current()

# Register a component
registry.register("my_component", component_instance)

# Get a component
component = registry.get("my_component")
```

### Component

The `Component` base class provides common functionality for all components:

```python
from ember.core.context import Component

class MyComponent(Component):
    def _register(self):
        """Register in registry."""
        self._registry.register("my_component", self)

    def _initialize(self):
        """Initialize lazily."""
        # Initialization logic
```

### Core Components

The system includes these core components:

- **ConfigComponent**: Configuration management
- **ModelComponent**: Model discovery and creation
- **DataComponent**: Dataset management
- **MetricsComponent**: Performance metrics collection

## Migration Guide

### Old API vs New API

Old API (EmberContext):

```python
from ember.core.app_context import get_app_context

# Get context
context = get_app_context()

# Get model and dataset
model = context.get_model("my_model")
dataset = context.get_dataset("my_dataset")
```

New API (Component-based):

```python
from ember.core.context.model import ModelComponent
from ember.core.context.data import DataComponent

# Get components
model_component = ModelComponent()
data_component = DataComponent()

# Get model and dataset
model = model_component.get_model("my_model")
dataset = data_component.get_dataset("my_dataset")
```

### Step 1: Update Imports

Change your imports from:

```python
from ember.core.app_context import get_app_context, EmberContext
```

To:

```python
from ember.core.context.compatibility import current_context, EmberContext
```

This provides compatibility with the old API while using the new implementation.

### Step 2: Migrate to Component-Based API

For each usage of `get_app_context()`, consider migrating to the direct component API:

Before:

```python
context = get_app_context()
model = context.get_model("gpt-4")
```

After:

```python
from ember.core.context.model import ModelComponent
model_component = ModelComponent()
model = model_component.get_model("gpt-4")
```

### Step 3: Update Mocks in Tests

Update test mocks to mock the component directly rather than the context:

Before:

```python
# Mock EmberContext
context_mock = MagicMock()
context_mock.get_model.return_value = model_mock
monkeypatch.setattr("ember.core.app_context.get_app_context", lambda: context_mock)
```

After:

```python
# Mock ModelComponent
model_component_mock = MagicMock()
model_component_mock.get_model.return_value = model_mock
monkeypatch.setattr("ember.core.context.model.ModelComponent.get",
                    lambda cls: model_component_mock)
```

Or use the scoped registry for cleaner tests:

```python
from ember.core.context import scoped_registry
from ember.core.context.model import ModelComponent

# Create isolated registry for test
with scoped_registry() as registry:
    # Create component with explicit registry
    model = ModelComponent(registry)

    # Register mock model
    model.register_model("test_model", mock_model)

    # Test code that uses model
    result = test_function()  # Will use mock_model
```

## Best Practices

1. **Direct Component Usage**: Access components directly instead of through EmberContext
2. **Explicit Dependencies**: Pass components as arguments instead of implicitly accessing them
3. **Scoped Testing**: Use `scoped_registry()` for isolated tests
4. **Lazy Loading**: Let components initialize themselves when first accessed
5. **Thread Safety**: Design components to be thread-safe using the double-checked locking pattern

## Performance Considerations

The new context system is designed for optimal performance:

- Thread-local storage eliminates most contention
- Lazy initialization reduces startup costs
- Component caching accelerates repeated lookups
- Direct component interaction reduces indirection

## Thread Safety

Thread safety is achieved through:

1. **Thread-Local Storage**: Each thread has its own isolated registry
2. **Double-Checked Locking**: Efficient lazy initialization pattern
3. **Fine-Grained Locking**: Component-specific locks for minimal contention

With this approach, most operations require no locking, and only registration and initialization need synchronization.

## Advanced Usage

### Custom Components

Creating custom components is straightforward:

```python
from ember.core.context import Component, Registry

class MyCustomComponent(Component):
    def __init__(self, registry=None):
        super().__init__(registry)
        self._data = {}

    def _register(self):
        self._registry.register("my_custom", self)

    def _initialize(self):
        # Lazy initialization logic
        config = self._registry.get("config")
        if config:
            self._data = config.get_config("my_custom") or {}

    def get_item(self, key):
        self._ensure_initialized()
        return self._data.get(key)
```

### Registry Scoping

For isolated execution contexts:

```python
from ember.core.context import scoped_registry
from ember.core.context.config import ConfigComponent

def isolated_function():
    with scoped_registry() as registry:
        # Create components with isolated registry
        config = ConfigComponent(registry, config_data={"custom": {"key": "value"}})

        # Function logic using isolated components
        result = process_with_config(config)

        return result
```

This ensures that components created within the function don't affect the rest of the application.

## Architecture

The new architecture simplifies the design to a single abstraction with direct component interaction:

```
┌─────────────────────────────────────────────────────────────┐
│ User Code                                                   │
│                                                             │
│  ┌───────────────┐    ┌───────────────┐    ┌───────────────┐  │
│  │ ModelComponent│    │ DataComponent │    │ConfigComponent│  │
│  │ get_model()   │    │ get_dataset() │    │ get_config()  │  │
│  └───────────────┘    └───────────────┘    └───────────────┘  │
└─────────────────────────────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ Thread-Local Registry                                        │
│                                                             │
│  ┌───────────────┐    ┌───────────────┐    ┌───────────────┐  │
│  │ Thread 1      │    │ Thread 2      │    │ Thread 3      │  │
│  │ Registry      │    │ Registry      │    │ Registry      │  │
│  └───────────────┘    └───────────────┘    └───────────────┘  │
└─────────────────────────────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────────┐
│ Component Registration                                       │
│                                                             │
│  ┌───────────────┐    ┌───────────────┐    ┌───────────────┐  │
│  │ "model" →     │    │ "data" →      │    │ "config" →    │  │
│  │ ModelComponent│    │ DataComponent │    │ConfigComponent│  │
│  └───────────────┘    └───────────────┘    └───────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

This design:

- Eliminates circular dependencies
- Reduces indirection
- Improves component discoverability
- Enhances thread safety
- Optimizes performance

## Implementation Status

- ✅ Core thread-local registry - Fully implemented with `Registry` class
- ✅ Component base class - Common functionality for all components
- ✅ Core components - Config, Model, Data, and Metrics implementations
- ✅ Management utilities - Scoped registry and temporary components
- ✅ Compatibility layer - Integration with existing code
- ✅ Documentation - Migration guide and best practices
  </code>

docs\exceptions_guide.md:
<code>

# Ember Exception Architecture

## Overview

Ember uses a hierarchical exception system to provide precise error information. Each exception includes context data, error codes, and recovery hints to simplify debugging. All exceptions inherit from a base `EmberError` class and follow domain boundaries that match the framework's structure.

## Core Features

### 1. Contextual Information

Exceptions capture detailed context data:

```python
try:
    # Operation that fails
except EmberError as e:
    print(f"Error context: {e.get_context()}")
```

### 2. Error Codes

Exceptions use numeric codes organized by domain for consistent error identification:

```
[Error 3001] Provider API error for 'openai': Rate limit exceeded
```

### 3. Recovery Hints

Exceptions include actionable recovery suggestions:

```
[Error 6004] Missing configuration key 'model.api_key'
[Recovery: Add the missing configuration key to your config file]
```

### 4. Logging Integration

Exceptions integrate with the logging system:

```python
try:
    # Operation that fails
except EmberError as e:
    e.log_with_context(logger)
```

## Exception Hierarchy

The exception hierarchy is organized by domain:

```
EmberError
├── ErrorGroup - Aggregates multiple related errors
├── Core Framework (1000-1999)
│   ├── InvalidArgumentError
│   ├── ValidationError
│   ├── NotImplementedFeatureError
│   ├── DeprecationError
│   ├── IncompatibleTypeError
│   └── InitializationError
├── Registry Exceptions (1100-1199)
│   ├── RegistryError
│   ├── ItemNotFoundError
│   ├── DuplicateItemError
│   └── RegistrationError
├── Operator Framework (2000-2999)
│   ├── OperatorError
│   ├── OperatorSpecificationError
│   ├── OperatorExecutionError
│   ├── SpecificationValidationError
│   ├── TreeTransformationError
│   └── FlattenError
├── Model Framework (3000-3999)
│   ├── ModelError
│   ├── ModelProviderError
│   ├── ModelNotFoundError
│   ├── ProviderAPIError
│   ├── ProviderConfigError
│   ├── ModelDiscoveryError
│   ├── ModelRegistrationError
│   ├── MissingLMModuleError
│   └── InvalidPromptError
├── Data Framework (4000-4999)
│   ├── DataError
│   ├── DataValidationError
│   ├── DataTransformationError
│   ├── DataLoadError
│   └── DatasetNotFoundError
├── XCS Framework (5000-5999)
│   ├── XCSError
│   ├── TraceError
│   ├── CompilationError
│   ├── ExecutionError
│   ├── TransformError
│   ├── ParallelExecutionError
│   ├── DataFlowError
│   └── SchedulerError
├── Configuration (6000-6999)
│   ├── ConfigError
│   ├── ConfigValidationError
│   ├── ConfigFileError
│   ├── ConfigValueError
│   └── MissingConfigError
├── API (7000-7999)
│   └── APIError
├── CLI (8000-8999)
│   └── CLIError
└── Plugin System (9000-9999)
    ├── PluginError
    ├── PluginNotFoundError
    ├── PluginConfigError
    └── PluginLoadError
```

## Usage Patterns

### Type-Specific Exceptions

Use the most specific exception type for the error condition:

```python
if model_name not in available_models:
    raise ModelNotFoundError.for_model(
        model_name=model_name,
        provider_name=provider
    )
```

### Context Enrichment

Include relevant context data:

```python
try:
    # Operation
except Exception as e:
    raise DataLoadError(
        message="Failed to load dataset",
        context={"dataset_name": name, "format": format},
        cause=e
    )
```

### Factory Methods

Use factory methods to create exceptions with consistent context:

```python
# Instead of:
raise ItemNotFoundError(f"Item '{item_name}' not found in {registry_name}")

# Use:
raise ItemNotFoundError.for_item(item_name, registry_name)
```

### Error Aggregation

Group related errors when appropriate:

```python
validation_errors = []
# Collect multiple validation errors
if validation_errors:
    raise ErrorGroup(
        message="Multiple validation errors occurred",
        errors=validation_errors
    )
```

## Standard Python Exception Conversion

Replace standard Python exceptions with Ember equivalents:

```python
# Instead of:
if not isinstance(inputs, dict):
    raise ValueError("Inputs must be a dictionary")

# Use:
if not isinstance(inputs, dict):
    raise ValidationError(
        "Inputs must be a dictionary",
        context={"input_type": type(inputs).__name__}
    )
```

## Creating Custom Exceptions

Define domain-specific exceptions by extending the base classes:

```python
from ember.core.exceptions import EmberError

class MyDomainError(EmberError):
    """Base exception for my domain."""
    DEFAULT_ERROR_CODE = 9500  # Use a unique range
    DEFAULT_RECOVERY_HINT = "Check domain configuration"
```

</code>

src\ember\core\utils\data\examples\usage_example.py:
<code>
"""
Usage:
python -m ember.core.utils.data.examples.usage_example

This example demonstrates the high-level dataset loading and preparation pipeline.
It shows how to retrieve processed dataset entries using two different methods:

1. Loading MMLU dataset entries using a configuration dictionary.
2. Loading HaluEval dataset entries using a configuration instance.
   """

import logging
from typing import Any, List

# Import for the high-level one-liner API function.

from ember.core.utils.data import load_dataset_entries

# Import for configuration classes.

from ember.core.utils.data.datasets_registry.halueval import HaluEvalConfig
from ember.core.utils.data.datasets_registry.mmlu import MMLUConfig

def main() -> None:
"""Demonstrate usage of the high-level dataset loading pipeline.

    Raises:
        Exception: Propagates any errors encountered during the dataset loading process.
    """
    # Configure logging at the INFO level.
    logging.basicConfig(level=logging.INFO)
    logger: logging.Logger = logging.getLogger(__name__)

    # Example 1: Load and prepare MMLU dataset entries using a configuration dictionary.
    try:
        logger.info("Loading MMLU dataset entries with configuration dictionary.")
        mmlu_entries: List[Any] = load_dataset_entries(
            dataset_name="mmlu",
            config=MMLUConfig(
                config_name="abstract_algebra",
                split="dev",
            ),
            num_samples=5,
        )
        logger.info("Successfully loaded %d entries for MMLU.", len(mmlu_entries))
        for idx, entry in enumerate(mmlu_entries, start=1):
            logger.info("MMLU Entry #%d:\n%s", idx, entry.model_dump_json(indent=2))
    except Exception as error:
        logger.exception("Error loading MMLU dataset entries: %s", error)

    # Example 2: Load and prepare HaluEval dataset entries using a configuration instance.
    try:
        logger.info("Loading HaluEval dataset entries with configuration instance.")
        halu_config: HaluEvalConfig = (
            HaluEvalConfig()
        )  # Defaults: config_name="qa", split="data"
        halu_entries: List[Any] = load_dataset_entries(
            dataset_name="halueval",
            config=halu_config,
            num_samples=3,
        )
        logger.info("Successfully loaded %d entries for HaluEval.", len(halu_entries))
        for idx, entry in enumerate(halu_entries, start=1):
            logger.info("HaluEval Entry #%d:\n%s", idx, entry.model_dump_json(indent=2))
    except Exception as error:
        logger.exception("Error loading HaluEval dataset entries: %s", error)

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\clean_jit_example.py:
<code>
"""Clean JIT API Example.

This module demonstrates the unified JIT API for Ember, focusing on:

1. JIT decoration of individual operators with @jit for tracing
2. Using execution_options for controlling execution behavior
3. Automatic graph construction and optimized parallel execution

To run:
uv run python src/ember/examples/advanced/clean_jit_example.py
"""

import logging
import time
from typing import ClassVar, List, Type

from ember.api import models

# ember imports

from ember.core.registry.operator.base.operator_base import Operator, Specification
from ember.core.types.ember_model import EmberModel, Field
from ember.xcs import execution_options, explain_jit_selection, get_jit_stats, jit

###############################################################################

# Input/Output Models

###############################################################################

class GenerationInput(EmberModel):
"""Input model for LLM operator.

    Attributes:
        query: The query to be processed.
    """

    query: str = Field(description="The query to be processed")

class GenerationOutput(EmberModel):
"""Output model for LLM operator.

    Attributes:
        responses: List of responses from the language model.
    """

    responses: List[str] = Field(
        description="List of responses from the language model"
    )

class AggregatorInput(EmberModel):
"""Input model for aggregator operator.

    Attributes:
        responses: List of responses to aggregate.
    """

    responses: List[str] = Field(description="List of responses to aggregate")

class AggregatorOutput(EmberModel):
"""Output model for aggregator operator.

    Attributes:
        final_answer: The aggregated result.
        confidence: Confidence score for the aggregation.
    """

    final_answer: str = Field(description="The aggregated result")
    confidence: float = Field(description="Confidence score for the aggregation")

###############################################################################

# Specifications

###############################################################################

class GenerationSpecification(Specification):
"""Specification for LLM operator."""

    input_model: Type[EmberModel] = GenerationInput
    structured_output: Type[EmberModel] = GenerationOutput

class AggregatorSpecification(Specification):
"""Specification for aggregator operator."""

    input_model: Type[EmberModel] = AggregatorInput
    structured_output: Type[EmberModel] = AggregatorOutput

###############################################################################

# JIT-Decorated Operators

###############################################################################

@jit
class LLMOperator(Operator):
"""A language model operator that generates responses to a query."""

    specification: ClassVar[Specification] = GenerationSpecification()
    model_id: str

    def __init__(self, *, model_id: str = "anthropic:claude-3-5-sonnet") -> None:
        """Initialize the LLM operator.

        Args:
            model_id: The model identifier to use for generation.
        """
        self.model_id = model_id
        # Initialize the registry to get model
        self.registry = models.get_registry()

    def forward(self, *, inputs: GenerationInput) -> GenerationOutput:
        """Generate responses using the language model.

        Args:
            inputs: The input containing the query.

        Returns:
            Generated responses from the language model.
        """
        # Handle both EmberModel and dict inputs (for JIT compatibility)
        if isinstance(inputs, dict) and "query" in inputs:
            # Convert dict to EmberModel for processing
            query = inputs["query"]
            inputs_obj = GenerationInput(query=query)
        elif hasattr(inputs, "query"):
            # Already proper EmberModel
            inputs_obj = inputs
            query = inputs.query
        else:
            # Cannot process this input
            logging.error(f"Invalid input type: {type(inputs)}")
            return GenerationOutput(responses=["Error: Invalid input format"])

        logging.info(f"Processing query with {self.model_id}: {query}")

        try:
            # Check if the model is available
            if self.registry.is_registered(self.model_id):
                # In a real application, we would call the model
                # model = self.registry.get_model(self.model_id)
                # response = model(prompt=query)

                # For demo purposes, we'll simulate a response to avoid API costs
                logging.info(f"Model {self.model_id} found, simulating response")
                time.sleep(0.1)  # Simulate API call latency
                simulated_responses = [
                    f"Mock answer about '{query}' from {self.model_id}",
                    f"Alternative perspective on '{query}'",
                    f"Additional context for '{query}'",
                ]
                return GenerationOutput(responses=simulated_responses)
            else:
                logging.warning(f"Model {self.model_id} not found in registry")
                return GenerationOutput(
                    responses=[f"Error: Model {self.model_id} not available"]
                )
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            return GenerationOutput(responses=[f"Error: {str(e)}"])

@jit
class AggregatorOperator(Operator):
"""An aggregator that combines multiple responses."""

    specification: ClassVar[Specification] = AggregatorSpecification()

    def forward(self, *, inputs: AggregatorInput) -> AggregatorOutput:
        """Aggregate responses into a final answer.

        Args:
            inputs: The responses to aggregate.

        Returns:
            Aggregated result with confidence score.
        """
        # Handle both EmberModel and dict inputs (for JIT compatibility)
        if isinstance(inputs, dict) and "responses" in inputs:
            # Convert dict to EmberModel for processing
            responses = inputs["responses"]
            inputs_obj = AggregatorInput(responses=responses)
        elif hasattr(inputs, "responses"):
            # Already proper EmberModel
            inputs_obj = inputs
            responses = inputs.responses
        else:
            # Cannot process this input
            logging.error(f"Invalid input type: {type(inputs)}")
            return AggregatorOutput(
                final_answer="Error: Invalid input format", confidence=0.0
            )

        logging.info(f"Aggregating {len(responses)} responses")
        time.sleep(0.05)  # Simulate processing

        # In a real application, this would implement a more sophisticated
        # aggregation strategy, potentially using another LLM call
        if not responses:
            return AggregatorOutput(
                final_answer="No responses to aggregate", confidence=0.0
            )

        return AggregatorOutput(
            final_answer=responses[0],  # Just take the first one for this demo
            confidence=0.95,
        )

def run_pipeline(query: str, num_workers: int = 3) -> AggregatorOutput:
"""Run a simple pipeline with JIT-enabled operators.

    Args:
        query: The query to process
        num_workers: Number of worker threads

    Returns:
        The pipeline result

    Raises:
        RuntimeError: If there is an error running the pipeline
    """
    # Create the operators
    llm_op = LLMOperator(model_id="anthropic:claude-3-5-sonnet")
    aggregator = AggregatorOperator()

    try:
        # Use execution_options to set parallel execution with specific workers
        with execution_options(scheduler="parallel", max_workers=num_workers):
            # Step 1: Run the LLM operator with a proper GenerationInput instance
            llm_input = GenerationInput(query=query)
            llm_response = llm_op(inputs=llm_input)
            logging.info(f"LLM operator output: {llm_response}")

            # Step 2: Feed the responses to the aggregator with a proper AggregatorInput instance
            aggregator_input = AggregatorInput(responses=llm_response["responses"])
            final_result = aggregator(inputs=aggregator_input)
            logging.info(f"Aggregator output: {final_result}")

        # Display JIT compilation statistics
        stats = get_jit_stats()
        logging.info(f"JIT compilation stats: {stats}")

        return final_result
    except Exception as e:
        logging.error(f"Error in run_pipeline: {e}")
        # Create a fallback response in case of error
        error_result = AggregatorOutput(
            final_answer=f"Error processing query: {str(e)}", confidence=0.0
        )
        return error_result

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run demonstration of clean JIT API."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    logging.info("=== Clean JIT API Demonstration ===")

    # Example query
    query = "What is the capital of France?"

    logging.info(f"Processing query: {query}")
    result = run_pipeline(query=query, num_workers=5)

    logging.info(f"Final answer: {result['final_answer']}")
    logging.info(f"Confidence: {result['confidence']:.2f}")

    # Show explanation of JIT strategy selection
    for op_name in ["LLMOperator", "AggregatorOperator"]:
        explanation = explain_jit_selection(op_name)
        if explanation:
            logging.info(f"JIT strategy for {op_name}: {explanation}")

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\context_performance_example.py:
<code>
"""Example demonstrating the high-performance EmberContext system.

This example showcases:

1. Thread-local context access with near-zero overhead
2. Cache-optimized component lookup
3. Thread safety and isolation for concurrent applications
4. Configuration access through context
5. Performance measurements

The example follows the core architectural principles of the Ember framework
and demonstrates how the unified context system improves performance.
"""

import threading
import time

# Use absolute imports following the Google Python Style Guide

from ember.core.context import current_context, temp_component
from ember.core.context.config_integration import config_override

class SimpleModel:
"""Simple model for example purposes."""

    def __init__(self, name: str, temperature: float = 0.7):
        self.name = name
        self.temperature = temperature

    def generate(self, prompt: str) -> str:
        """Simulate text generation."""
        return f"Output from {self.name} (temp={self.temperature}): {prompt}"

def process_with_model(model_name: str, prompt: str) -> str:
"""Process text with a model.""" # Get current context (thread-local, zero-overhead)
ctx = current_context()

    # Get model from context (efficient lookup)
    model = ctx.get_model(model_name)
    if model is None:
        return f"Model {model_name} not found"

    # Use model to generate output
    return model.generate(prompt)

def benchmark_context_access(iterations: int = 1_000_000) -> float:
"""Benchmark context access performance.

    Args:
        iterations: Number of iterations

    Returns:
        Average time per operation in nanoseconds
    """
    # Register test model
    ctx = current_context()
    ctx.register("model", "test_model", SimpleModel("test_model"))

    # Warm-up
    for _ in range(10000):
        current_context()

    # Benchmark
    start = time.time()
    for _ in range(iterations):
        _ = current_context()
    end = time.time()

    # Calculate average time
    total_ns = (end - start) * 1e9
    return total_ns / iterations

def benchmark_model_lookup(iterations: int = 1_000_000) -> float:
"""Benchmark model lookup performance.

    Args:
        iterations: Number of iterations

    Returns:
        Average time per lookup in nanoseconds
    """
    # Register test model
    ctx = current_context()
    ctx.register("model", "test_model", SimpleModel("test_model"))

    # Warm-up
    for _ in range(10000):
        ctx.get_model("test_model")

    # Benchmark
    start = time.time()
    for _ in range(iterations):
        _ = ctx.get_model("test_model")
    end = time.time()

    # Calculate average time
    total_ns = (end - start) * 1e9
    return total_ns / iterations

def benchmark_config_access(iterations: int = 1_000_000) -> float:
"""Benchmark configuration access performance.

    Args:
        iterations: Number of iterations

    Returns:
        Average time per access in nanoseconds
    """
    # Get context
    ctx = current_context()

    # Warm-up
    for _ in range(10000):
        _ = ctx.config

    # Benchmark
    start = time.time()
    for _ in range(iterations):
        _ = ctx.config
    end = time.time()

    # Calculate average time
    total_ns = (end - start) * 1e9
    return total_ns / iterations

def demo_thread_isolation() -> None:
"""Demonstrate thread isolation with thread-local contexts."""

    def worker(thread_id: int) -> None:
        """Worker function showing thread isolation."""
        # Each thread gets its own context
        ctx = current_context()

        # Register thread-specific model
        model = SimpleModel(f"thread-{thread_id}", temperature=thread_id / 10)
        ctx.register("model", "my-model", model)

        # Use model
        result = process_with_model("my-model", "Hello from thread")
        print(f"Thread {thread_id}: {result}")

        # Now show that another thread's model is separate
        other_result = process_with_model("thread-model", "Test isolation")
        print(f"Thread {thread_id} accessing 'thread-model': {other_result}")

    # Create threads
    threads = []
    for i in range(5):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)

    # Start threads
    for thread in threads:
        thread.start()

    # Register a global model
    ctx = current_context()
    ctx.register("model", "thread-model", SimpleModel("thread-model", temperature=0.5))

    # Wait for threads to complete
    for thread in threads:
        thread.join()

def demo_config_override() -> None:
"""Demonstrate configuration overrides.""" # Get current context
ctx = current_context()

    # Register model
    ctx.register("model", "demo-model", SimpleModel("demo-model"))

    # Get output with default config
    print("Default configuration:")
    result = process_with_model("demo-model", "Hello world")
    print(result)

    # Override configuration
    print("\nWith configuration override:")
    with config_override({"model": {"temperature": 0.2}}):
        # Register model with config temperature
        temperature = ctx.config.get("model", {}).get("temperature", 0.7)
        ctx.register("model", "demo-model", SimpleModel("demo-model", temperature))

        # Get output with overridden config
        result = process_with_model("demo-model", "Hello world")
        print(result)

    # Back to default configuration
    print("\nBack to default configuration:")
    result = process_with_model("demo-model", "Hello world")
    print(result)

def demo_temporary_components() -> None:
"""Demonstrate temporary component registration.""" # Get current context
ctx = current_context()

    # Register permanent model
    ctx.register("model", "permanent", SimpleModel("permanent-model"))

    # Use permanent model
    print("Using permanent model:")
    result = process_with_model("permanent", "Hello")
    print(result)

    # Use temporary model
    print("\nUsing temporary model:")
    with temp_component(
        "model", "temporary", SimpleModel("temporary-model", 0.3)
    ) as model:
        result = process_with_model("temporary", "Hello")
        print(result)

    # Check if temporary model is gone
    print("\nAfter temporary scope:")
    result = process_with_model("temporary", "Hello")
    print(result)

    # Permanent model still exists
    print("\nPermanent model still exists:")
    result = process_with_model("permanent", "Hello")
    print(result)

def demo_xcs_integration() -> None:
"""Demonstrate XCS integration with the unified context.

    This demonstrates how the context system conceptually integrates with XCS
    execution, following patterns shown in the API guidelines.
    """
    # When actually running this example with a proper setup,
    # the XCS integration would work as described below.
    # For demonstration purposes, we show the pattern without
    # actually executing potentially uninitialized XCS code.

    # Get current context
    ctx = current_context()

    print("XCS Integration Pattern")
    print("----------------------")
    print("The unified context system enables seamless XCS integration.")
    print("With a properly initialized system, you could use:")

    print("\n1. JIT execution with context awareness:")
    print("   @jit")
    print("   def process(data):")
    print("       # Access context inside JIT-compiled function")
    print("       ctx = current_context()")
    print("       model = ctx.get_model('my-model')")
    print("       return model.generate(data)")

    print("\n2. Execution options from context configuration:")
    print("   with execution_options(scheduler='parallel'):")
    print("       result = process(data)")

    print("\n3. Thread-local execution contexts:")
    print("   # Each thread has its own execution context")
    print("   # with proper resource isolation")

    print("\nThis integration enables high-performance, thread-safe execution")

def run_example() -> None:
"""Run the complete context system example."""
print("EmberContext Performance Example")
print("================================\n")

    # Register example model
    ctx = current_context()
    ctx.register("model", "example", SimpleModel("example-model"))

    # Run performance benchmarks
    print("Performance Benchmarks:")
    ctx_ns = benchmark_context_access()
    print(f"Context access:   {ctx_ns:.2f} ns per operation")

    model_ns = benchmark_model_lookup()
    print(f"Model lookup:     {model_ns:.2f} ns per operation")

    config_ns = benchmark_config_access()
    print(f"Config access:    {config_ns:.2f} ns per operation")

    print("\nFeature Demonstrations:\n")

    # Demo thread isolation
    print("\n1. Thread Isolation")
    print("------------------")
    demo_thread_isolation()

    # Demo configuration overrides
    print("\n\n2. Configuration Overrides")
    print("-------------------------")
    demo_config_override()

    # Demo temporary components
    print("\n\n3. Temporary Components")
    print("---------------------")
    demo_temporary_components()

    # Demo XCS integration
    print("\n\n4. XCS Integration")
    print("-----------------")
    try:
        demo_xcs_integration()
    except (ImportError, AttributeError) as e:
        print(f"XCS integration demo skipped: {e}")

if **name** == "**main**":
run_example()

</code>

src\ember\examples\advanced\custom_component_example.py:
<code>
"""Example of creating custom components with the new context system.

This example demonstrates how to create custom components
that integrate with the context system.
"""

from typing import Any, Dict, List, Optional, Type

from ember.core.context import Component, Registry
from ember.core.context.config import ConfigComponent

class CacheComponent(Component):
"""Example custom component for caching.

    This component demonstrates how to create a custom component
    that works with the context system.
    """

    def __init__(self, registry: Optional[Registry] = None):
        """Initialize with registry.

        Args:
            registry: Registry to use (current thread's if None)
        """
        super().__init__(registry)
        self._cache: Dict[str, Any] = {}
        self._hits = 0
        self._misses = 0

    def _register(self) -> None:
        """Register in registry as 'cache'."""
        self._registry.register("cache", self)

    def _initialize(self) -> None:
        """Initialize cache component.

        In this case, we'll load cache configuration from the config component.
        """
        # Check if there's a config component
        config = self._registry.get("config")
        if config:
            # Get cache configuration
            cache_config = config.get_config("cache")

            # Apply any configuration options
            self._max_size = cache_config.get("max_size", 1000)
            if "initial_values" in cache_config:
                self._cache.update(cache_config["initial_values"])

    def get(self, key: str, default: Any = None) -> Any:
        """Get item from cache.

        Args:
            key: Cache key
            default: Default value if not found

        Returns:
            Cached value or default
        """
        self._ensure_initialized()

        if key in self._cache:
            self._hits += 1
            return self._cache[key]
        else:
            self._misses += 1
            return default

    def set(self, key: str, value: Any) -> None:
        """Set item in cache.

        Args:
            key: Cache key
            value: Value to cache
        """
        self._ensure_initialized()

        # Basic cache management - remove oldest if too many items
        if len(self._cache) >= self._max_size:
            # Remove first item (oldest)
            if self._cache:
                del self._cache[next(iter(self._cache))]

        self._cache[key] = value

    def clear(self) -> None:
        """Clear the cache."""
        self._cache.clear()
        self._hits = 0
        self._misses = 0

    def get_stats(self) -> Dict[str, int]:
        """Get cache statistics.

        Returns:
            Dictionary with cache statistics
        """
        return {
            "hits": self._hits,
            "misses": self._misses,
            "size": len(self._cache),
            "hit_ratio": self._hits / (self._hits + self._misses)
            if (self._hits + self._misses) > 0
            else 0,
        }

class LoggerComponent(Component):
"""Example custom logging component.

    This component demonstrates dependency injection with
    the context system.
    """

    def __init__(self, registry: Optional[Registry] = None):
        """Initialize with registry.

        Args:
            registry: Registry to use (current thread's if None)
        """
        super().__init__(registry)
        self._logs: List[str] = []
        self._level = "INFO"

    def _register(self) -> None:
        """Register in registry as 'logger'."""
        self._registry.register("logger", self)

    def _initialize(self) -> None:
        """Initialize logger component.

        Loads configuration from config component.
        """
        # Check if there's a config component
        config = self._registry.get("config")
        if config:
            # Get logger configuration
            logger_config = config.get_config("logger")

            # Apply any configuration options
            self._level = logger_config.get("level", "INFO").upper()

    def log(self, level: str, message: str) -> None:
        """Log a message.

        Args:
            level: Log level
            message: Message to log
        """
        self._ensure_initialized()

        level = level.upper()
        if self._should_log(level):
            log_entry = f"[{level}] {message}"
            self._logs.append(log_entry)
            print(log_entry)

    def _should_log(self, level: str) -> bool:
        """Check if message should be logged at this level.

        Args:
            level: Log level to check

        Returns:
            True if message should be logged
        """
        levels = {"DEBUG": 0, "INFO": 1, "WARNING": 2, "ERROR": 3, "CRITICAL": 4}

        message_level = levels.get(level.upper(), 0)
        current_level = levels.get(self._level, 1)

        return message_level >= current_level

    def debug(self, message: str) -> None:
        """Log debug message.

        Args:
            message: Debug message
        """
        self.log("DEBUG", message)

    def info(self, message: str) -> None:
        """Log info message.

        Args:
            message: Info message
        """
        self.log("INFO", message)

    def warning(self, message: str) -> None:
        """Log warning message.

        Args:
            message: Warning message
        """
        self.log("WARNING", message)

    def error(self, message: str) -> None:
        """Log error message.

        Args:
            message: Error message
        """
        self.log("ERROR", message)

    def critical(self, message: str) -> None:
        """Log critical message.

        Args:
            message: Critical message
        """
        self.log("CRITICAL", message)

    def get_logs(self) -> List[str]:
        """Get all logs.

        Returns:
            List of log entries
        """
        return self._logs.copy()

def demonstrate_custom_components() -> None:
"""Demonstrate custom components working together.""" # Clear any existing registry
Registry.clear()

    # Create configuration
    config = ConfigComponent(
        config_data={
            "cache": {"max_size": 100, "initial_values": {"greeting": "Hello, world!"}},
            "logger": {"level": "DEBUG"},
        }
    )

    # Create custom components
    cache = CacheComponent()
    logger = LoggerComponent()

    # Use logger
    logger.debug("Initializing application")
    logger.info("Application started")

    # Use cache
    greeting = cache.get("greeting")
    logger.info(f"Greeting from cache: {greeting}")

    # Add more items to cache
    cache.set("answer", 42)
    logger.debug("Added answer to cache")

    # Get cache stats
    stats = cache.get_stats()
    logger.info(f"Cache stats: {stats}")

    # Get non-existent item
    value = cache.get("nonexistent")
    logger.warning(f"Attempted to access nonexistent key, got: {value}")

    # Update stats
    stats = cache.get_stats()
    logger.info(f"Updated cache stats: {stats}")

    # Demonstrate component discovery
    registry = Registry.current()
    components = registry.keys()
    logger.info(f"Registered components: {components}")

    # Log final message
    logger.info("Custom component demonstration completed")

    # Print all logs
    print("\nAll logs:")
    for log in logger.get_logs():
        print(f"  {log}")

if **name** == "**main**":
demonstrate_custom_components()

</code>

src\ember\examples\advanced\diagnose_model_discovery.py:
<code>
"""Diagnose Model Discovery Issues in Ember

This script is designed to pinpoint exactly why model auto-discovery isn't working.
It adds detailed logging and performs targeted tests to identify issues in the model
discovery process.

To run:
uv run python src/ember/examples/advanced/diagnose_model_discovery.py
"""

import importlib
import inspect
import logging
import os
import sys
from pathlib import Path

# Configure logging

logging.basicConfig(level=logging.DEBUG, format="%(levelname)s: %(name)s: %(message)s")
logger = logging.getLogger("model_discovery_diagnostics")

# Add src to path if needed

src_path = str(Path(**file**).parent.parent.parent.parent)
if src_path not in sys.path:
sys.path.append(src_path)
logger.debug(f"Added {src_path} to sys.path")

# Import with careful error handling

try:
from ember.api import models
from ember.core.registry.model.base.registry.discovery import ModelDiscoveryService
from ember.core.registry.model.base.registry.model_registry import ModelRegistry
from ember.core.registry.model.providers.base_discovery import BaseDiscoveryProvider
from ember.core.registry.model.providers.registry import PROVIDER_REGISTRY

    logger.debug("Successfully imported core model registry modules")

except ImportError as e:
logger.error(f"Failed to import required modules: {e}")
sys.exit(1)

def check_api_keys():
"""Check if API keys are set in environment variables."""
keys_to_check = {"OPENAI_API_KEY": "OpenAI", "ANTHROPIC_API_KEY": "Anthropic"}

    logger.info("Checking for API keys...")
    for key, provider in keys_to_check.items():
        value = os.environ.get(key)
        if value:
            logger.info(f"✅ {key} is set for {provider}")
        else:
            logger.warning(f"❌ {key} is NOT set for {provider}")

def check_config_paths():
"""Check if config files exist at expected paths."""
logger.info("Checking configuration file paths...")

    # Define paths to check
    config_paths = [
        "src/ember/core/registry/model/config/model_registry_config.yaml",
        "src/ember/core/registry/model/providers/openai/openai_config.yaml",
        "src/ember/core/registry/model/providers/anthropic/anthropic_config.yaml",
    ]

    base_dir = Path(__file__).parent.parent.parent.parent.parent

    for path in config_paths:
        full_path = base_dir / path
        if full_path.exists():
            logger.debug(f"✅ Config file exists: {full_path}")
            # Check the content of the config file for path references
            with open(full_path, "r") as f:
                content = f.read()
                logger.debug(f"Config content for {full_path}:\n{content}")
        else:
            logger.error(f"❌ Config file NOT found: {full_path}")

            # Try to find similar files
            parent_dir = full_path.parent
            if parent_dir.exists():
                similar_files = list(parent_dir.glob("*.yaml"))
                if similar_files:
                    logger.info(f"Similar files in {parent_dir}:")
                    for file in similar_files:
                        logger.info(f"  - {file.name}")

def check_provider_registry():
"""Check the provider registry population."""
logger.info("Checking provider registry...")

    try:
        # Check the PROVIDER_REGISTRY dictionary
        logger.debug(f"PROVIDER_REGISTRY contents: {PROVIDER_REGISTRY}")

        if not PROVIDER_REGISTRY:
            logger.error("❌ PROVIDER_REGISTRY is empty")

            # Try to find provider decorators
            try:
                from ember.core.registry.model.providers import (
                    anthropic_provider,
                    openai_provider,
                )

                logger.debug(f"Anthropic provider module: {anthropic_provider}")
                logger.debug(f"OpenAI provider module: {openai_provider}")

                # Check if providers are correctly decorated
                for name, module in [
                    ("Anthropic", anthropic_provider),
                    ("OpenAI", openai_provider),
                ]:
                    for item_name in dir(module):
                        item = getattr(module, item_name)
                        if inspect.isclass(item):
                            logger.debug(f"Found class {item_name} in {name} provider")
                            if hasattr(item, "__provider_name__"):
                                logger.debug(
                                    f"Class {item_name} has __provider_name__ = {item.__provider_name__}"
                                )
            except ImportError as e:
                logger.error(f"Failed to import provider modules: {e}")
        else:
            logger.info(
                f"✅ PROVIDER_REGISTRY contains {len(PROVIDER_REGISTRY)} providers"
            )
            for provider_name, provider_class in PROVIDER_REGISTRY.items():
                logger.info(f"  - {provider_name}: {provider_class}")
    except Exception as e:
        logger.error(f"❌ Error checking provider registry: {e}")

def test_discovery_imports():
"""Test import paths for discovery modules."""
logger.info("Testing discovery module imports...")

    import_paths = [
        "ember.core.registry.model.providers",
        "ember.core.registry.model.providers.anthropic",
        "ember.core.registry.model.providers.openai",
        "ember.core.registry.model.providers.anthropic.anthropic_discovery",
        "ember.core.registry.model.providers.openai.openai_discovery",
    ]

    for path in import_paths:
        try:
            module = importlib.import_module(path)
            logger.debug(f"✅ Successfully imported {path}")

            # For discovery modules, check what they contain
            if "discovery" in path:
                for item_name in dir(module):
                    item = getattr(module, item_name)
                    if inspect.isclass(item) and issubclass(
                        item, BaseDiscoveryProvider
                    ):
                        logger.debug(f"Found discovery class: {item_name}")
                        # Print the fetch_models method source
                        if hasattr(item, "fetch_models"):
                            fetch_method = item.fetch_models
                            logger.debug(f"fetch_models method: {fetch_method}")
                            if hasattr(fetch_method, "__code__"):
                                source_lines = inspect.getsourcelines(fetch_method)
                                logger.debug(f"fetch_models source: {source_lines[0]}")
        except ImportError as e:
            logger.error(f"❌ Failed to import {path}: {e}")

            # Try to find similar modules
            parent_path = ".".join(path.split(".")[:-1])
            try:
                parent_module = importlib.import_module(parent_path)
                logger.debug(f"Parent module {parent_path} exists")
                logger.debug(f"Contents of {parent_path}: {dir(parent_module)}")
            except ImportError:
                logger.error(f"Parent module {parent_path} doesn't exist")

def test_individual_discovery():
"""Test each discovery provider individually."""
logger.info("Testing individual discovery providers...")

    discovery_classes = [
        (
            "ember.core.registry.model.providers.anthropic.anthropic_discovery",
            "AnthropicDiscovery",
        ),
        (
            "ember.core.registry.model.providers.openai.openai_discovery",
            "OpenAIDiscovery",
        ),
    ]

    for module_path, class_name in discovery_classes:
        try:
            module = importlib.import_module(module_path)
            discovery_class = getattr(module, class_name, None)

            if discovery_class:
                logger.debug(f"Found discovery class {class_name} in {module_path}")

                # Try instantiating the discovery class
                try:
                    discovery_instance = discovery_class()
                    logger.debug(f"Successfully instantiated {class_name}")

                    # Try fetching models
                    try:
                        logger.debug(f"Calling fetch_models() on {class_name}...")
                        models = discovery_instance.fetch_models()
                        logger.debug(f"fetch_models() returned: {models}")
                    except Exception as e:
                        logger.error(
                            f"❌ Error calling fetch_models() on {class_name}: {e}"
                        )
                except Exception as e:
                    logger.error(f"❌ Error instantiating {class_name}: {e}")
            else:
                logger.error(f"❌ Could not find class {class_name} in {module_path}")
        except ImportError as e:
            logger.error(f"❌ Could not import {module_path}: {e}")

def test_model_registry_initialization():
"""Test the full model registry initialization process."""
logger.info("Testing model registry initialization...")

    try:
        # First check the API approach
        logger.debug("Testing models.initialize_registry with auto_discover=True...")
        try:
            registry = models.initialize_registry(auto_discover=True)
            logger.debug(f"Registry initialized: {registry}")

            # Check which models were discovered
            model_ids = registry.list_models()
            logger.debug(f"Discovered models: {model_ids}")

            if not model_ids:
                logger.warning("❌ No models were discovered automatically")
        except Exception as e:
            logger.error(f"❌ Error initializing registry through API: {e}")

        # Test direct instantiation
        logger.debug("Testing direct ModelRegistry instantiation...")
        try:
            registry = ModelRegistry()
            logger.debug(f"Registry directly instantiated: {registry}")

            # Try to discover models manually
            logger.debug("Manually running discovery process...")
            registry.discover_models()

            # Check which models were discovered
            model_ids = registry.list_models()
            logger.debug(f"Manually discovered models: {model_ids}")

            if not model_ids:
                logger.warning("❌ No models were discovered through manual discovery")
        except Exception as e:
            logger.error(f"❌ Error with direct registry initialization: {e}")
    except Exception as e:
        logger.error(f"❌ Unexpected error in registry initialization test: {e}")

def check_model_registry_implementation():
"""Check the implementation of model discovery in ModelRegistry."""
logger.info("Examining ModelRegistry implementation...")

    try:
        # Get source for discover_models method
        from ember.core.registry.model.base.registry.model_registry import ModelRegistry

        discover_method = ModelRegistry.discover_models

        if hasattr(discover_method, "__code__"):
            source_lines, line_no = inspect.getsourcelines(discover_method)
            logger.debug(
                f"discover_models source (line {line_no}):\n{''.join(source_lines)}"
            )

        # Check the factory implementation
        from ember.core.registry.model.base.registry.factory import (
            create_provider_instance,
        )

        factory_method = create_provider_instance

        if hasattr(factory_method, "__code__"):
            source_lines, line_no = inspect.getsourcelines(factory_method)
            logger.debug(
                f"create_provider_instance source (line {line_no}):\n{''.join(source_lines)}"
            )
    except Exception as e:
        logger.error(f"❌ Error examining ModelRegistry implementation: {e}")

def main():
"""Run all diagnostic tests."""
logger.info("======= STARTING MODEL DISCOVERY DIAGNOSTICS =======")

    # Run all diagnostic checks
    check_api_keys()
    check_config_paths()
    check_provider_registry()
    test_discovery_imports()
    check_model_registry_implementation()
    test_individual_discovery()
    test_model_registry_initialization()

    logger.info("======= DIAGNOSTICS COMPLETE =======")

    # Provide a summary of findings
    logger.info("\n===== DIAGNOSTIC SUMMARY =====")
    logger.info(
        "Review the log output above to identify where the model discovery process is failing."
    )
    logger.info("Key areas to check:")
    logger.info("1. API keys - are they present and correctly formatted?")
    logger.info("2. Configuration files - do they exist at the expected paths?")
    logger.info("3. Provider registry - is it being populated correctly?")
    logger.info("4. Import paths - are all modules being found correctly?")
    logger.info("5. Discovery implementations - are they properly implemented?")
    logger.info("6. ModelRegistry initialization - is it properly configured?")

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\ensemble_judge_mmlu.py:
<code>
"""
Advanced example demonstrating a varied ensemble operator with a judge for MMLU evaluation.

This example:

1. Creates a varied ensemble operator using multiple models with different configurations
2. Feeds ensemble responses to a judge operator
3. Evaluates on MMLU dataset with selectable subjects
4. Compares to a baseline single model operator
5. Optimizes execution using XCS transforms and enhanced dependency analysis
6. Visualizes performance results

Usage: # Basic usage (2 subjects, 2 samples each, 2 models)
python -m ember.examples.advanced.ensemble_judge_mmlu

    # Custom configuration through environment variables
    SAMPLE_SIZE=5 MMLU_SUBJECTS="high_school_mathematics,philosophy" MODEL_COUNT=3 \
        python -m ember.examples.advanced.ensemble_judge_mmlu

    # Quick test with minimal API calls
    SAMPLE_SIZE=1 MMLU_SUBJECTS="high_school_mathematics" MODEL_COUNT=1 \
        python -m ember.examples.advanced.ensemble_judge_mmlu

Environment variables:
SAMPLE_SIZE: Number of samples per subject (default: 2)
MMLU_SUBJECTS: Comma-separated list of subjects (default: first 2 subjects)
MODEL_COUNT: Number of models to use in the ensemble (default: 2)
"""

import logging
import time
from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type

import matplotlib.pyplot as plt
import numpy as np
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# Set up logger for this module

logger = logging.getLogger(**name**)

# Import necessary modules

from ember.api import DatasetBuilder
from ember.api.operators import Operator
from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.operator.base.operator_base import Specification
from ember.core.registry.specification.specification import (
Specification as CoreSpecification,
)
from ember.core.types.ember_model import EmberModel
from ember.xcs import jit
from ember.xcs.engine.execution_options import execution_options

# Set up console for rich output

console = Console()

# Available MMLU subjects for evaluation

MMLU_SUBJECTS = [
"high_school_mathematics",
"high_school_computer_science",
"high_school_physics",
"professional_medicine",
"college_biology",
"philosophy",
]

class MCQInput(EmberModel):
"""Input for multiple-choice question evaluation."""

    question: str
    choices: Dict[str, str]

class MCQOutput(EmberModel):
"""Output for multiple-choice question evaluation."""

    answer: str
    reasoning: str = ""
    confidence: float = 0.0

class EnsembleJudgeInput(EmberModel):
"""Input for the judge operator."""

    question: str
    choices: Dict[str, str]
    candidate_responses: List[MCQOutput]

class EnsembleJudgeOutput(EmberModel):
"""Output for the judge operator."""

    question: str
    choices: Dict[str, str]
    candidate_responses: List[MCQOutput]
    selected_answer: str
    confidence: float
    justification: str

class MMLUDataset:
"""Dataset loader for MMLU evaluations with subject selection.

    Uses the Ember dataset API to load MMLU datasets by subject.
    """

    def __init__(self, subject: str = "high_school_mathematics"):
        """Initialize MMLU dataset loader with specified subject.

        Args:
            subject: The MMLU subject to load (config_name in MMLU)
        """
        self.subject = subject

    def load(self, max_samples: int = 10) -> List[Dict[str, Any]]:
        """Load samples from the specified MMLU subject.

        Args:
            max_samples: Maximum number of samples to load

        Returns:
            List of question dictionaries formatted for operators
        """
        try:
            # Use the standard DatasetBuilder pattern to load MMLU data
            dataset = (
                DatasetBuilder()
                .from_registry("mmlu")
                .subset(self.subject)  # MMLU uses subset as the subject name
                .split("test")  # Test split for evaluation
                .sample(max_samples)  # Limit number of samples
                .build()
            )

            # Convert DatasetEntry objects to our expected format
            result = []
            for entry in dataset.entries:
                result.append(
                    {
                        "question": entry.query,
                        "choices": entry.choices,
                        "answer": entry.metadata.get("correct_answer", ""),
                    }
                )
            return result

        except Exception as e:
            # Fallback to mock data if dataset loading fails
            console.print(f"[yellow]Using mock data: {str(e)}[/yellow]")
            return self._get_mock_data(max_samples)

    def _get_mock_data(self, max_samples: int) -> List[Dict[str, Any]]:
        """Get mock data for testing when real dataset cannot be loaded.

        Args:
            max_samples: Maximum number of samples to return

        Returns:
            List of mock question dictionaries
        """
        # Mock data by subject
        mock_datasets = {
            "high_school_mathematics": [
                {
                    "question": "What is the derivative of f(x) = x^2?",
                    "choices": {
                        "A": "f'(x) = x",
                        "B": "f'(x) = 2x",
                        "C": "f'(x) = 2",
                        "D": "f'(x) = x^2",
                    },
                    "answer": "B",
                },
                {
                    "question": "Solve for x: 2x + 5 = 13",
                    "choices": {"A": "x = 3", "B": "x = 4", "C": "x = 5", "D": "x = 6"},
                    "answer": "B",
                },
                {
                    "question": "What is the area of a circle with radius 5?",
                    "choices": {"A": "25π", "B": "10π", "C": "5π", "D": "100π"},
                    "answer": "A",
                },
            ],
            "high_school_physics": [
                {
                    "question": "What is the SI unit of force?",
                    "choices": {
                        "A": "Joule",
                        "B": "Newton",
                        "C": "Watt",
                        "D": "Pascal",
                    },
                    "answer": "B",
                },
                {
                    "question": "Which law states that energy cannot be created or destroyed?",
                    "choices": {
                        "A": "Newton's First Law",
                        "B": "Law of Conservation of Energy",
                        "C": "Ohm's Law",
                        "D": "Boyle's Law",
                    },
                    "answer": "B",
                },
            ],
            "philosophy": [
                {
                    "question": "Who wrote 'Critique of Pure Reason'?",
                    "choices": {
                        "A": "Friedrich Nietzsche",
                        "B": "Immanuel Kant",
                        "C": "René Descartes",
                        "D": "John Locke",
                    },
                    "answer": "B",
                },
                {
                    "question": "The statement 'I think, therefore I am' is associated with which philosopher?",
                    "choices": {
                        "A": "Socrates",
                        "B": "Aristotle",
                        "C": "René Descartes",
                        "D": "Plato",
                    },
                    "answer": "C",
                },
            ],
        }

        # Get the specified subject or default to mathematics
        data = mock_datasets.get(self.subject, mock_datasets["high_school_mathematics"])
        return data[:max_samples]

class BaselineMCQSpecification(CoreSpecification):
"""Specification for baseline MCQ operator."""

    # Define both input and output models explicitly
    input_model: Type[EmberModel] = MCQInput
    structured_output: Type[EmberModel] = MCQOutput

    prompt_template: str = """Answer the following multiple-choice question:

Question: {question}

Options:
{choices}

First, think through this step-by-step to determine the correct answer.
Then, respond with just one of the option letters (A, B, C, or D).
Explain your reasoning after selecting your answer.

Format:
Answer: [Selected Option Letter]
Reasoning: [Your detailed explanation]
"""

class BaselineMCQOperator(Operator[MCQInput, MCQOutput]):
"""Baseline operator using a single model for MCQ evaluation."""

    specification: ClassVar[Specification] = BaselineMCQSpecification()
    model_name: str
    temperature: float
    max_tokens: int
    lm_module: LMModule

    def __init__(
        self,
        model_name: str = "anthropic:claude-3-sonnet-20240229",
        temperature: float = 0.0,
        max_tokens: int = 1024,
    ) -> None:
        """Initialize the baseline operator with model configuration.

        Args:
            model_name: Name of the model to use
            temperature: Temperature parameter for generation
            max_tokens: Maximum tokens to generate
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens

        # Initialize the LM module
        self.lm_module = LMModule(
            config=LMModuleConfig(
                id=model_name, temperature=temperature, max_tokens=max_tokens
            )
        )

    def forward(self, *, inputs: MCQInput) -> MCQOutput:
        """Process a multiple-choice question with the model.

        Args:
            inputs: Question and choices

        Returns:
            Model's answer and reasoning
        """
        # Pre-format choices as text for template insertion
        choices_text = "\n".join(
            [f"{key}. {value}" for key, value in inputs.choices.items()]
        )

        # Create a modified copy of inputs with the formatted choices
        template_vars = {
            "question": inputs.question,
            "choices": choices_text,  # Pre-formatted choices text passed as "choices"
        }

        # Fill in the template with standard Python format strings
        prompt = self.specification.prompt_template.format(**template_vars)

        # Call LMModule with explicit named parameter to make it JIT-friendly
        response = self.lm_module(prompt=prompt)

        # Parse the response to extract answer and reasoning
        answer, reasoning = self._parse_response(response, inputs.choices)

        return MCQOutput(
            answer=answer,
            reasoning=reasoning,
            confidence=1.0,  # Default confidence for baseline
        )

    def _parse_response(
        self, response: str, choices: Dict[str, str]
    ) -> Tuple[str, str]:
        """Parse the model response to extract answer and reasoning.

        Args:
            response: The full text response from the model
            choices: The available choices

        Returns:
            Tuple of (selected answer, reasoning)
        """
        if not response:
            return "Unknown", "No response"

        response = response.strip()
        answer_letter = None
        reasoning = response

        # Try to extract answer using "Answer:" format
        if "Answer:" in response:
            parts = response.split("Answer:", 1)
            if len(parts) > 1:
                answer_part = parts[1].strip().split("\n", 1)[0].strip()
                # Extract just the letter part
                for letter in choices:
                    if letter in answer_part:
                        answer_letter = letter
                        break

                # Extract reasoning if it's in the expected format
                if "Reasoning:" in response:
                    reasoning = response.split("Reasoning:", 1)[1].strip()
                else:
                    # If no explicit reasoning section, use everything after the answer
                    reasoning_parts = parts[1].strip().split("\n", 1)
                    reasoning = (
                        reasoning_parts[1].strip()
                        if len(reasoning_parts) > 1
                        else "No explicit reasoning provided."
                    )

        # If no answer found with "Answer:" format, try other approaches
        if not answer_letter:
            for letter in choices:
                if (
                    f"The answer is {letter}" in response
                    or f"answer is {letter}" in response
                ):
                    answer_letter = letter
                    break

        # If still no answer, look for letter at beginning of lines
        if not answer_letter:
            lines = response.split("\n")
            for line in lines:
                line = line.strip()
                for letter in choices:
                    if line.startswith(f"{letter}") or line.startswith(f"({letter})"):
                        answer_letter = letter
                        break
                if answer_letter:
                    break

        # Last resort: Just look for the first occurrence of any valid choice letter
        if not answer_letter:
            for letter in choices:
                if letter in response:
                    answer_letter = letter
                    break

        # Default to first choice if we couldn't extract an answer
        if not answer_letter and choices:
            answer_letter = next(iter(choices), None)

        # Get the full answer text
        answer = choices.get(answer_letter, "Unknown")

        return answer, reasoning

class VariedEnsembleSpecification(CoreSpecification):
"""Specification for varied ensemble MCQ operator."""

    # Explicit models for proper type handling
    input_model: Type[MCQInput] = MCQInput
    # We'll let the system handle the conversion of lists
    structured_output: Any = None

    # Different prompt templates for each model strategy
    prompt_templates: List[str] = [
        # Template for analytical approach
        """Analyze this multiple-choice question from a logical perspective:

Question: {question}

Options:
{choices}

Use logical analysis to determine the correct answer.
First, analyze each option systematically.
Then, select your answer and explain your reasoning.

Answer: [Option Letter]
Reasoning: [Explanation]
""", # Template for example-based approach
"""Answer this multiple-choice question by comparing with examples:

Question: {question}

Options:
{choices}

Think about similar examples or cases to help solve this.
Compare the options to known concepts or facts.
Select your answer and explain your reasoning.

Answer: [Option Letter]
Reasoning: [Explanation]
""", # Template for comprehensive approach
"""Consider all aspects of this multiple-choice question:

Question: {question}

Options:
{choices}

Evaluate each option thoroughly and eliminate incorrect choices.
Consider all relevant facts and concepts.
Select your answer and provide comprehensive reasoning.

Answer: [Option Letter]
Reasoning: [Explanation]
""",
]

class VariedEnsembleMCQOperator(Operator[MCQInput, List[MCQOutput]]):
"""Ensemble operator using multiple model configurations for MCQ evaluation.

    This operator implements a varied ensemble approach where multiple language models
    with different configurations evaluate the same question.
    """

    specification: ClassVar[Specification] = VariedEnsembleSpecification()
    lm_modules: List[LMModule]

    def __init__(
        self,
        model_configs: Optional[List[Dict[str, Any]]] = None,
    ) -> None:
        """Initialize the varied ensemble operator with multiple model configurations.

        Args:
            model_configs: List of model configuration dictionaries
        """
        if model_configs is None:
            # Default model configurations with different models and parameters
            model_configs = [
                {"model_name": "anthropic:claude-3-opus-20240229", "temperature": 0.0},
                {
                    "model_name": "anthropic:claude-3-sonnet-20240229",
                    "temperature": 0.7,
                },
                # Use only available models to ensure functionality
            ]

        # Create LM modules for each configuration
        self.lm_modules = []
        for config in model_configs:
            self.lm_modules.append(
                LMModule(
                    config=LMModuleConfig(
                        id=config["model_name"],
                        temperature=config["temperature"],
                        max_tokens=config.get("max_tokens", 1024),
                    )
                )
            )

    def _process_with_model(
        self, question: str, choices: Dict[str, str], lm_module: LMModule, template: str
    ) -> MCQOutput:
        """Process a question with a single model.

        This method is separated to make it clear to the JIT optimizer that
        each model inference can be parallelized.

        Args:
            question: The question text
            choices: Original choices dictionary
            lm_module: The language model module to use
            template: Prompt template to format

        Returns:
            Processed output with answer and reasoning
        """

        # Pre-format choices as text for template insertion
        choices_text = "\n".join([f"{key}. {value}" for key, value in choices.items()])

        # Create template variables with pre-formatted choices
        template_vars = {
            "question": question,
            "choices": choices_text,  # Pre-formatted choices text passed as "choices"
        }

        # Fill in the template with standard Python format strings
        prompt = template.format(**template_vars)

        # Call LMModule with explicit named parameter to make it JIT-friendly
        response = lm_module(prompt=prompt)

        # Parse the response
        answer, reasoning = self._parse_response(response, choices)

        return MCQOutput(
            answer=answer,
            reasoning=reasoning,
            confidence=0.8,  # Default confidence, could be improved with calibration
        )

    def forward(self, *, inputs: MCQInput) -> List[MCQOutput]:
        """Process a question with multiple models in the ensemble.

        This implementation is structured with explicit dependency declaration
        to enable the enhanced JIT to automatically parallelize the model calls.

        Args:
            inputs: Question and choices

        Returns:
            List of answers and reasoning from each model
        """
        templates = self.specification.prompt_templates

        # Get responses from each model with different prompts
        responses = []
        for i, lm_module in enumerate(self.lm_modules):
            # Select a prompt template (cycling through available templates)
            template_idx = i % len(templates)
            template = templates[template_idx]

            # Process with this model - enhanced JIT will parallelize these
            responses.append(
                self._process_with_model(
                    question=inputs.question,
                    choices=inputs.choices,
                    lm_module=lm_module,
                    template=template,
                )
            )

        return responses

    def _parse_response(
        self, response: str, choices: Dict[str, str]
    ) -> Tuple[str, str]:
        """Parse the model response to extract answer and reasoning.

        Args:
            response: The full text response from the model
            choices: The available choices

        Returns:
            Tuple of (selected answer, reasoning)
        """
        if not response:
            return "Unknown", "No response"

        response = response.strip()
        answer_letter = None
        reasoning = response

        # Try to extract answer using "Answer:" format
        if "Answer:" in response:
            parts = response.split("Answer:", 1)
            if len(parts) > 1:
                answer_part = parts[1].strip().split("\n", 1)[0].strip()
                # Extract just the letter part
                for letter in choices:
                    if letter in answer_part:
                        answer_letter = letter
                        break

                # Extract reasoning if it's in the expected format
                if "Reasoning:" in response:
                    reasoning = response.split("Reasoning:", 1)[1].strip()
                else:
                    # If no explicit reasoning section, use everything after the answer
                    reasoning_parts = parts[1].strip().split("\n", 1)
                    reasoning = (
                        reasoning_parts[1].strip()
                        if len(reasoning_parts) > 1
                        else "No explicit reasoning provided."
                    )

        # If no answer found with "Answer:" format, try other approaches
        if not answer_letter:
            for letter in choices:
                if (
                    f"The answer is {letter}" in response
                    or f"answer is {letter}" in response
                ):
                    answer_letter = letter
                    break

        # Default to first choice if we couldn't extract an answer
        if not answer_letter and choices:
            answer_letter = next(iter(choices), None)

        # Get the full answer text
        answer = choices.get(answer_letter, "Unknown")

        return answer, reasoning

class JudgeOperatorSpecification(CoreSpecification):
"""Specification for MCQ judge operator."""

    # Explicit models for proper type handling
    input_model: Type[EnsembleJudgeInput] = EnsembleJudgeInput
    structured_output: Type[EnsembleJudgeOutput] = EnsembleJudgeOutput

    prompt_template: str = """You are a judge evaluating different candidate answers to a multiple-choice question.

Your task is to analyze the reasoning of each candidate and select the most accurate answer.

Question: {question}

Options:
{choices}

Candidate responses:
{candidate_responses}

Step 1: Analyze each candidate's reasoning objectively and identify strengths and weaknesses.
Step 2: Determine which reasoning is most sound and leads to the correct answer.
Step 3: Select the answer you believe is correct based on the strongest reasoning.
Step 4: Provide your justification for this selection.

Your response should follow this format:
Selected Answer: [selected answer text]
Confidence: [number between 0-1 representing certainty]
Justification: [your detailed reasoning justifying this selection]
"""

class JudgeOperator(Operator[EnsembleJudgeInput, EnsembleJudgeOutput]):
"""Judge operator that selects the best answer from ensemble responses."""

    specification: ClassVar[Specification] = JudgeOperatorSpecification()
    lm_module: LMModule

    def __init__(
        self,
        model_name: str = "anthropic:claude-3-sonnet-20240229",
        temperature: float = 0.0,
        max_tokens: int = 1024,
    ) -> None:
        """Initialize the judge operator with model configuration.

        Args:
            model_name: Name of the model to use
            temperature: Temperature parameter for generation
            max_tokens: Maximum tokens to generate
        """
        self.model_name = model_name
        self.lm_module = LMModule(
            config=LMModuleConfig(
                id=model_name, temperature=temperature, max_tokens=max_tokens
            )
        )

    def forward(self, *, inputs: EnsembleJudgeInput) -> EnsembleJudgeOutput:
        """Judge ensemble responses and select the best answer.

        Args:
            inputs: Question, choices, and candidate responses

        Returns:
            The selected answer with justification
        """
        # Format candidate responses
        candidate_responses_text = ""
        for i, response in enumerate(inputs.candidate_responses):
            candidate_responses_text += f"Candidate {i+1}:\n"
            candidate_responses_text += f"Answer: {response.answer}\n"
            candidate_responses_text += f"Reasoning: {response.reasoning}\n\n"

        # Pre-format choices as text for template insertion
        choices_text = "\n".join(
            [f"{key}. {value}" for key, value in inputs.choices.items()]
        )

        # Create template variables with pre-formatted choices
        template_vars = {
            "question": inputs.question,
            "choices": choices_text,  # Pre-formatted choices text passed as "choices"
            "candidate_responses": candidate_responses_text,
        }

        # Fill in the template with standard Python format strings
        prompt = self.specification.prompt_template.format(**template_vars)

        # Call LMModule with explicit named parameter to make it JIT-friendly
        response = self.lm_module(prompt=prompt)

        # Parse the judge's response
        selected_answer, confidence, justification = self._parse_judge_response(
            response, inputs.choices
        )

        return EnsembleJudgeOutput(
            question=inputs.question,
            choices=inputs.choices,
            candidate_responses=inputs.candidate_responses,
            selected_answer=selected_answer,
            confidence=confidence,
            justification=justification,
        )

    def _parse_judge_response(
        self, response: str, choices: Dict[str, str]
    ) -> Tuple[str, float, str]:
        """Parse the judge's response to extract the selected answer and justification.

        Args:
            response: The full text response from the judge
            choices: The available choices

        Returns:
            Tuple of (selected answer, confidence, justification)
        """
        if not response:
            return "Unknown", 0.5, "No response"

        response = response.strip()

        # Default values
        selected_answer = "Unknown"
        confidence = 0.5
        justification = "No justification provided."

        # Try to extract the selected answer using the expected format
        if "Selected Answer:" in response:
            parts = response.split("Selected Answer:", 1)
            if len(parts) > 1:
                answer_part = parts[1].split("\n", 1)[0].strip()

                # Check if the answer matches any of the choices
                for letter, choice_text in choices.items():
                    if letter in answer_part or choice_text in answer_part:
                        selected_answer = choice_text
                        break

        # Extract confidence if available
        if "Confidence:" in response:
            parts = response.split("Confidence:", 1)
            if len(parts) > 1:
                confidence_part = parts[1].split("\n", 1)[0].strip()
                try:
                    confidence_value = float(confidence_part)
                    if 0 <= confidence_value <= 1:
                        confidence = confidence_value
                except (ValueError, TypeError):
                    pass

        # Extract justification if available
        if "Justification:" in response:
            parts = response.split("Justification:", 1)
            if len(parts) > 1:
                justification = parts[1].strip()

        return selected_answer, confidence, justification

class EnsembleJudgePipeline(Operator[MCQInput, EnsembleJudgeOutput]):
"""JIT-optimized pipeline combining ensemble and judge operators.

    This pipeline uses the enhanced JIT system with automatic parallelization
    detection to provide substantial performance benefits for ensemble-based workflows.
    """

    # Complete specification with input/output models
    specification: ClassVar[Specification] = CoreSpecification(
        input_model=MCQInput, structured_output=EnsembleJudgeOutput
    )
    ensemble_operator: VariedEnsembleMCQOperator
    judge_operator: JudgeOperator

    def __init__(
        self,
        *,  # Add explicit keyword-only marker to ensure parameters are passed by name
        model_configs: Optional[List[Dict[str, Any]]] = None,
        judge_model: str = "anthropic:claude-3-sonnet-20240229",
    ) -> None:
        """Initialize the pipeline with ensemble and judge operators.

        Args:
            model_configs: List of model configuration dictionaries for the ensemble
            judge_model: Name of the model to use for judging
        """
        self.ensemble_operator = VariedEnsembleMCQOperator(model_configs=model_configs)
        self.judge_operator = JudgeOperator(model_name=judge_model)

    def forward(self, *, inputs: MCQInput) -> EnsembleJudgeOutput:
        """Process a question through the ensemble and judge pipeline.

        The enhanced JIT optimizer automatically identifies parallelization
        opportunities within the operator. No explicit hints needed.

        Args:
            inputs: Question and choices

        Returns:
            The judge's selected answer and justification
        """
        # Get ensemble responses - JIT will automatically parallelize model calls internally
        # Pass MCQInput as an object, not a dict
        ensemble_outputs = self.ensemble_operator(inputs=inputs)

        # Prepare input for judge
        judge_input = EnsembleJudgeInput(
            question=inputs.question,
            choices=inputs.choices,
            candidate_responses=ensemble_outputs,
        )

        # Get judge decision
        return self.judge_operator(inputs=judge_input)

# Apply JIT manually after class definition to avoid initialization issues

EnsembleJudgePipeline = jit(EnsembleJudgePipeline)

# Simplified pipeline creation function

def create_pipeline(
model_configs: Optional[List[Dict[str, Any]]] = None,
judge_model: str = "anthropic:claude-3-sonnet-20240229",
) -> EnsembleJudgePipeline:
"""Create a pipeline combining ensemble and judge operators.

    Args:
        model_configs: Model configurations for the ensemble
        judge_model: Model to use for judging

    Returns:
        Ensemble-judge pipeline instance
    """
    # Now using explicit keyword arguments to match the updated __init__ method
    return EnsembleJudgePipeline(model_configs=model_configs, judge_model=judge_model)

class MMLUExperiment:
"""Experiment class for comparing baseline vs. ensemble-judge approaches on MMLU."""

    def __init__(
        self,
        subject: str = "high_school_mathematics",
        sample_size: int = 3,
        model_configs: Optional[List[Dict[str, Any]]] = None,
        baseline_model: str = "anthropic:claude-3-sonnet-20240229",
        judge_model: str = "anthropic:claude-3-sonnet-20240229",
        use_acceleration: bool = True,
    ) -> None:
        """Initialize the experiment.

        Args:
            subject: MMLU subject to evaluate
            sample_size: Number of samples to use
            model_configs: Model configurations for the ensemble
            baseline_model: Model for the baseline operator
            judge_model: Model for the judge operator
            use_acceleration: Whether to use XCS acceleration
        """
        self.subject = subject
        self.sample_size = sample_size

        # Initialize operators
        self.baseline_operator = BaselineMCQOperator(model_name=baseline_model)

        # Create a standard pipeline - use execution_options to control acceleration
        # Using keyword-only parameters for the EnsembleJudgePipeline
        self.ensemble_judge_operator = EnsembleJudgePipeline(
            model_configs=model_configs, judge_model=judge_model
        )
        # Note: acceleration is controlled at execution time using execution_options

        # Load MMLU dataset
        self.mmlu_dataset = MMLUDataset(subject=subject)

    def run(self) -> Dict[str, Any]:
        """Run the experiment and return results.

        Returns:
            Dictionary of experiment results
        """
        # Create models for evaluation

        # Load data samples
        mmlu_data = self.mmlu_dataset.load(max_samples=self.sample_size)

        # Initialize results
        results = {
            "subject": self.subject,
            "sample_size": len(mmlu_data),
            "baseline": {
                "correct": 0,
                "total": len(mmlu_data),
                "time": 0,
                "detailed": [],
            },
            "ensemble_judge": {
                "correct": 0,
                "total": len(mmlu_data),
                "time": 0,
                "detailed": [],
            },
        }

        # Run baseline
        console.print(Panel(f"Running baseline on {self.subject}...", style="blue"))
        baseline_start = time.time()

        for item in mmlu_data:
            # Convert dict choices to key-value format
            choices = {k: v for k, v in item["choices"].items()}
            correct_answer = item["answer"]

            input_data = MCQInput(question=item["question"], choices=choices)
            output = self.baseline_operator(inputs=input_data)

            # Find which letter corresponds to the selected answer
            selected_letter = None
            for letter, text in choices.items():
                if text == output.answer:
                    selected_letter = letter
                    break

            # Check if the selected letter matches the correct answer letter
            is_correct = selected_letter == correct_answer

            results["baseline"]["detailed"].append(
                {
                    "question": item["question"],
                    "choices": choices,
                    "correct_answer": correct_answer,
                    "predicted": output.answer,
                    "is_correct": is_correct,
                    "reasoning": output.reasoning,
                }
            )

            if is_correct:
                results["baseline"]["correct"] += 1

        results["baseline"]["time"] = time.time() - baseline_start
        results["baseline"]["accuracy"] = (
            results["baseline"]["correct"] / results["baseline"]["total"]
        )

        # Run ensemble-judge with enhanced JIT + explicit parallel execution
        console.print(
            Panel(f"Running ensemble-judge on {self.subject}...", style="green")
        )
        ensemble_start = time.time()

        # Use wave scheduler for optimal parallelization with the enhanced JIT system
        for item in mmlu_data:
            # Convert dict choices to key-value format
            choices = {k: v for k, v in item["choices"].items()}
            correct_answer = item["answer"]

            input_data = MCQInput(question=item["question"], choices=choices)

            # Use execution_options context manager with enhanced settings
            # The wave scheduler is optimized for this kind of parallel ensemble workload
            with execution_options(
                scheduler="wave",
                max_workers=len(
                    self.ensemble_judge_operator.ensemble_operator.lm_modules
                ),
                enable_caching=True,  # Enable caching for better performance
                device_strategy="auto",  # Let system choose the best device strategy
            ):
                output = self.ensemble_judge_operator(inputs=input_data)

            # Find which letter corresponds to the selected answer
            selected_letter = None
            for letter, text in choices.items():
                if text == output.selected_answer:
                    selected_letter = letter
                    break

            # Check if the selected letter matches the correct answer letter
            is_correct = selected_letter == correct_answer

            results["ensemble_judge"]["detailed"].append(
                {
                    "question": item["question"],
                    "choices": choices,
                    "correct_answer": correct_answer,
                    "predicted": output.selected_answer,
                    "is_correct": is_correct,
                    "justification": output.justification,
                }
            )

            if is_correct:
                results["ensemble_judge"]["correct"] += 1

        results["ensemble_judge"]["time"] = time.time() - ensemble_start
        results["ensemble_judge"]["accuracy"] = (
            results["ensemble_judge"]["correct"] / results["ensemble_judge"]["total"]
        )

        # Record the number of models used in the ensemble for reference
        num_models = len(self.ensemble_judge_operator.ensemble_operator.lm_modules)
        results["num_models"] = num_models

        # We'll simply report the measured time for both approaches without
        # trying to calculate a theoretical speedup, as real-world API calls
        # have complex queueing, network effects, and provider-side processing
        # that make simple multipliers inaccurate

        return results

class ExperimentVisualizer:
"""Utilities for visualizing experiment results."""

    @staticmethod
    def print_summary(results: Dict[str, Any]) -> None:
        """Print a summary of the experiment results.

        Args:
            results: Dictionary of experiment results
        """
        console.print("\n")
        table = Table(title=f"MMLU Results: {results['subject']}")

        table.add_column("Metric", style="cyan")
        table.add_column("Baseline", style="blue")
        table.add_column("Ensemble+Judge", style="green")

        table.add_row(
            "Accuracy",
            f"{results['baseline']['accuracy']:.2%}",
            f"{results['ensemble_judge']['accuracy']:.2%}",
        )

        table.add_row(
            "Execution Time",
            f"{results['baseline']['time']:.2f}s",
            f"{results['ensemble_judge']['time']:.2f}s",
        )

        if results.get("speedup"):
            relative_performance = (
                "↑"
                if results["ensemble_judge"]["accuracy"]
                > results["baseline"]["accuracy"]
                else "↓"
            )
            accuracy_diff = (
                abs(
                    results["ensemble_judge"]["accuracy"]
                    - results["baseline"]["accuracy"]
                )
                * 100
            )

            table.add_row(
                "Improvement",
                "Baseline",
                f"{relative_performance} {accuracy_diff:.2f}% accuracy",
            )

            # Show the number of models used, not a theoretical speedup
            table.add_row("Models Used", "1", f"{results.get('num_models', 'N/A')}")

        console.print(table)

    @staticmethod
    def plot_results(
        all_results: List[Dict[str, Any]], output_path: Optional[str] = None
    ) -> None:
        """Plot comparison of baseline vs ensemble-judge across subjects.

        Args:
            all_results: List of result dictionaries from experiments
            output_path: Path to save the plot image
        """
        subjects = [r["subject"].replace("_", " ").title() for r in all_results]
        baseline_accuracy = [r["baseline"]["accuracy"] for r in all_results]
        ensemble_accuracy = [r["ensemble_judge"]["accuracy"] for r in all_results]

        # Create figure with subplots
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

        # Accuracy comparison
        x = np.arange(len(subjects))
        width = 0.35

        ax1.bar(
            x - width / 2, baseline_accuracy, width, label="Baseline", color="royalblue"
        )
        ax1.bar(
            x + width / 2,
            ensemble_accuracy,
            width,
            label="Ensemble+Judge",
            color="forestgreen",
        )

        ax1.set_ylabel("Accuracy")
        ax1.set_title("Accuracy by Subject")
        ax1.set_xticks(x)
        ax1.set_xticklabels(subjects, rotation=45, ha="right")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Time comparison
        baseline_time = [r["baseline"]["time"] for r in all_results]
        ensemble_time = [r["ensemble_judge"]["time"] for r in all_results]

        ax2.bar(
            x - width / 2, baseline_time, width, label="Baseline", color="royalblue"
        )
        ax2.bar(
            x + width / 2,
            ensemble_time,
            width,
            label="Ensemble+Judge",
            color="forestgreen",
        )

        ax2.set_ylabel("Execution Time (s)")
        ax2.set_title("Execution Time by Subject")
        ax2.set_xticks(x)
        ax2.set_xticklabels(subjects, rotation=45, ha="right")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # Speedup comparison
        speedups = [r.get("speedup", 1.0) for r in all_results]

        ax3.bar(x, speedups, width * 1.5, color="orange")
        ax3.set_ylabel("Speedup Factor")
        ax3.set_title("Acceleration Speedup vs Non-Accelerated")
        ax3.set_xticks(x)
        ax3.set_xticklabels(subjects, rotation=45, ha="right")
        ax3.grid(True, alpha=0.3)

        plt.tight_layout()

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches="tight")
            console.print(f"Plot saved to {output_path}")

        plt.show()

    @staticmethod
    def plot_acceleration_comparison(
        benchmark_results: Dict[str, Any], output_path: Optional[str] = None
    ) -> None:
        """Plot comparison of different acceleration strategies.

        Args:
            benchmark_results: Dictionary containing benchmark measurements
            output_path: Path to save the plot image
        """
        # Create figure with three subplots
        fig = plt.figure(figsize=(18, 10))
        gs = plt.GridSpec(2, 3, figure=fig, height_ratios=[3, 2])

        ax1 = fig.add_subplot(
            gs[0, 0:2]
        )  # Execution time (top left, spanning 2 columns)
        ax2 = fig.add_subplot(gs[0, 2])  # Relative speedup (top right)
        ax3 = fig.add_subplot(gs[1, :])  # JIT metrics (bottom, spanning all columns)

        # Setup data - ordered from slowest to fastest for better visual comparison
        strategies = ["Sequential", "Wave", "Parallel", "Auto"]

        # Use a color scheme that shows progression from sequential (red) to fastest (green)
        colors = ["firebrick", "darkorange", "royalblue", "forestgreen"]

        # Extract times
        seq_time = benchmark_results["sequential_time"]
        wave_time = benchmark_results["wave_time"]
        parallel_time = benchmark_results["parallel_time"]
        auto_time = benchmark_results["auto_time"]

        times = [seq_time, wave_time, parallel_time, auto_time]

        # Calculate speedups relative to sequential
        wave_speedup = seq_time / max(wave_time, 1e-6)
        parallel_speedup = seq_time / max(parallel_time, 1e-6)
        auto_speedup = seq_time / max(auto_time, 1e-6)

        speedups = [1.0, wave_speedup, parallel_speedup, auto_speedup]

        # Plot execution times - use horizontal bars for better readability
        bars1 = ax1.barh(strategies, times, color=colors)
        ax1.set_xlabel("Execution Time (seconds)")
        ax1.set_title("Execution Time by Strategy")
        ax1.grid(axis="x", alpha=0.3)
        ax1.invert_yaxis()  # Put sequential at the top

        # Add time values at end of bars
        for bar in bars1:
            width = bar.get_width()
            ax1.text(
                width + 0.01,
                bar.get_y() + bar.get_height() / 2,
                f"{width:.4f}s",
                va="center",
                fontsize=10,
            )

        # Plot speedups - use horizontal bars for consistency
        bars2 = ax2.barh(strategies, speedups, color=colors)
        ax2.set_xlabel("Speedup Factor (vs Sequential)")
        ax2.set_title("Relative Speedup by Strategy")
        ax2.axvline(
            x=1.0, color="black", linestyle="--", alpha=0.5
        )  # Add reference line at 1.0
        ax2.grid(axis="x", alpha=0.3)
        ax2.invert_yaxis()  # Keep same order as first plot

        # Add speedup values at end of bars
        for bar in bars2:
            width = bar.get_width()
            ax2.text(
                width + 0.05,
                bar.get_y() + bar.get_height() / 2,
                f"{width:.2f}x",
                va="center",
                fontweight="bold",
            )

        # Plot JIT metrics if available
        jit_metrics = benchmark_results.get("jit_metrics", {})
        if jit_metrics:
            # Convert metrics to cleaner format for display
            metrics_to_show = [
                ("Cache Hit Rate", jit_metrics.get("cache_hit_rate", 0) * 100, "%"),
                (
                    "Avg Compilation",
                    jit_metrics.get("avg_compilation_time_ms", 0),
                    "ms",
                ),
                ("Avg Execution", jit_metrics.get("avg_execution_time_ms", 0), "ms"),
                ("Cache Hits", jit_metrics.get("cache_hits", 0), ""),
                ("Cache Misses", jit_metrics.get("cache_misses", 0), ""),
                ("Compilation Count", jit_metrics.get("compilation_count", 0), ""),
                ("Execution Count", jit_metrics.get("execution_count", 0), ""),
            ]

            # Create bar chart of metrics
            metric_names = [m[0] for m in metrics_to_show]
            metric_values = [m[1] for m in metrics_to_show]
            metric_units = [m[2] for m in metrics_to_show]

            bars3 = ax3.bar(metric_names, metric_values, alpha=0.7, color="steelblue")
            ax3.set_title("JIT Performance Metrics")
            ax3.set_ylabel("Value")
            ax3.grid(axis="y", alpha=0.3)

            # Add values on top of bars
            for i, bar in enumerate(bars3):
                height = bar.get_height()
                unit = metric_units[i]
                ax3.text(
                    bar.get_x() + bar.get_width() / 2,
                    height + 0.1,
                    f"{height:.2f}{unit}",
                    ha="center",
                    fontsize=9,
                )

            # Add log scale if needed for large values
            if any(v > 1000 for v in metric_values):
                ax3.set_yscale("log")
                ax3.set_ylabel("Value (log scale)")
        else:
            ax3.text(
                0.5,
                0.5,
                "No JIT metrics available",
                ha="center",
                va="center",
                fontsize=14,
                transform=ax3.transAxes,
            )

        # Add a note about the best strategy
        best_strategy = benchmark_results.get("best_strategy", "auto").capitalize()
        speedup = benchmark_results.get("speedup", 1.0)

        plt.figtext(
            0.5,
            0.01,
            f"Best strategy: {best_strategy} ({speedup:.2f}x speedup vs Sequential)",
            ha="center",
            fontsize=12,
            bbox={"facecolor": "lightyellow", "alpha": 0.5, "pad": 5},
        )

        plt.tight_layout(rect=[0, 0.05, 1, 0.95])

        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches="tight")
            console.print(f"Acceleration comparison saved to {output_path}")

        plt.show()

def run_acceleration_benchmark(
subject: str, sample_size: int, model_configs: List[Dict[str, Any]]
) -> Dict[str, Any]:
"""Benchmark various scheduler strategies for parallelization.

    Compares execution strategies:
    1. Sequential execution (using execution_options with scheduler="sequential")
    2. Wave scheduler execution (using execution_options with scheduler="wave")
    3. Parallel scheduler execution (using execution_options with scheduler="parallel")
    4. Auto scheduler selection (using execution_options with scheduler="auto")

    Args:
        subject: MMLU subject to evaluate
        sample_size: Number of samples to use
        model_configs: Model configurations for the ensemble

    Returns:
        Dictionary of benchmark results
    """
    from ember.xcs.jit import get_jit_stats

    console.print(
        Panel(f"Benchmarking execution strategies on {subject}...", style="yellow")
    )

    # Constants for benchmark configuration
    WARMUP_RUNS = 2  # Number of warmup runs to stabilize JIT
    MEASURE_RUNS = 3  # Number of measurement runs to average

    # Return empty results on error
    empty_results = {
        "subject": subject,
        "sequential_time": 0,
        "wave_time": 0,
        "parallel_time": 0,
        "auto_time": 0,
        "speedup": 1.0,
        "jit_metrics": {},
    }

    try:
        mmlu_data = MMLUDataset(subject=subject).load(max_samples=sample_size)

        # Prepare test input
        if not mmlu_data:
            console.print("[red]No test data available![/red]")
            return empty_results

        sample_item = mmlu_data[0]
        test_input = MCQInput(
            question=sample_item["question"],
            choices={k: v for k, v in sample_item["choices"].items()},
        )

        # Create pipeline for benchmarking with multiple models
        pipeline = EnsembleJudgePipeline(model_configs=model_configs)

        # Number of worker threads for parallel execution
        max_workers = len(model_configs)

        # Define scheduler configurations to test
        scheduler_configs = [
            {
                "name": "sequential",
                "options": {"scheduler": "sequential", "enable_caching": False},
            },
            {
                "name": "wave",
                "options": {
                    "scheduler": "wave",
                    "max_workers": max_workers,
                    "enable_caching": False,
                },
            },
            {
                "name": "parallel",
                "options": {
                    "scheduler": "parallel",
                    "max_workers": max_workers,
                    "enable_caching": False,
                },
            },
            {
                "name": "auto",
                "options": {
                    "scheduler": "auto",
                    "max_workers": max_workers,
                    "enable_caching": False,
                },
            },
        ]

        results = {}

        # Run benchmarks for each scheduler configuration
        for config in scheduler_configs:
            name = config["name"]
            options = config["options"]

            console.print(f"Running {name} scheduler benchmark...")

            # Perform warmup runs to stabilize JIT
            console.print(f"  Performing {WARMUP_RUNS} warmup runs...")
            for _ in range(WARMUP_RUNS):
                with execution_options(**options):
                    _ = pipeline(inputs=test_input)

            # Collect actual measurements
            times = []
            console.print(f"  Collecting {MEASURE_RUNS} measurement runs...")
            for run in range(MEASURE_RUNS):
                run_start = time.perf_counter()
                with execution_options(**options):
                    _ = pipeline(inputs=test_input)
                run_time = time.perf_counter() - run_start
                times.append(run_time)
                console.print(f"    Run {run+1}: {run_time:.4f}s")

            # Calculate statistics
            avg_time = sum(times) / len(times)
            results[f"{name}_time"] = avg_time

            # Get JIT metrics after last run
            if (
                name == "auto"
            ):  # Only collect metrics for auto mode to avoid duplication
                results["jit_metrics"] = get_jit_stats(pipeline)

            console.print(f"  {name.capitalize()} avg time: {avg_time:.4f}s")

        # Find the fastest parallel strategy (excluding sequential)
        parallel_times = {
            "wave": results["wave_time"],
            "parallel": results["parallel_time"],
            "auto": results["auto_time"],
        }
        best_strategy = min(parallel_times.items(), key=lambda x: x[1])[0]
        best_time = parallel_times[best_strategy]

        # Calculate speedup ratio against sequential execution
        sequential_time = results["sequential_time"]
        speedup = sequential_time / max(best_time, 1e-6)

        # Return comprehensive benchmark results
        return {
            "subject": subject,
            "sequential_time": results["sequential_time"],
            "wave_time": results["wave_time"],
            "parallel_time": results["parallel_time"],
            "auto_time": results["auto_time"],
            "best_strategy": best_strategy,
            "best_time": best_time,
            "speedup": speedup,
            "jit_metrics": results["jit_metrics"],
        }

    except Exception as e:
        console.print(f"[yellow]Benchmark setup error: {e}[/yellow]")
        return empty_results

def main() -> None:
"""Main function to run the ensemble-judge MMLU evaluation example with enhanced JIT.

    Environment variables:
      SAMPLE_SIZE: Number of samples per subject (default: 2)
      MMLU_SUBJECTS: Comma-separated list of subjects (default: first 2)
    """
    # Get configuration from environment variables
    import os

    sample_size = int(os.environ.get("SAMPLE_SIZE", "2"))
    console.print(
        Panel.fit(
            "MMLU Evaluation: Baseline vs. Ensemble+Judge Pipeline with Enhanced JIT Optimization",
            title="Ember Advanced Example",
            subtitle="Demonstrating LLM Ensemble Techniques and Enhanced Parallelization",
            style="bold green",
        )
    )

    # Check if API keys are set
    import os

    api_keys = {
        "ANTHROPIC_API_KEY": os.environ.get("ANTHROPIC_API_KEY"),
        "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY"),
        "GOOGLE_API_KEY": os.environ.get("GOOGLE_API_KEY"),
    }

    if not any(api_keys.values()):
        # No API keys found, provide clear instructions
        console.print(
            Panel(
                "⚠️ No API keys found for language model providers.\n\n"
                "To run this example with real models, please set at least one of these environment variables:\n"
                "  - ANTHROPIC_API_KEY\n"
                "  - OPENAI_API_KEY\n"
                "  - GOOGLE_API_KEY\n\n"
                "Example:\n"
                "  export ANTHROPIC_API_KEY=sk_ant_xxxx\n\n"
                "This example will showcase the code structure and architecture without making actual API calls.",
                title="API Keys Required",
                style="yellow",
            )
        )
        return

    # Define model configurations
    model_configs = [
        {"model_name": "anthropic:claude-3-opus-20240229", "temperature": 0.0},
        {"model_name": "anthropic:claude-3-sonnet-20240229", "temperature": 0.7},
    ]

    # Get subjects from environment with sensible default
    subjects_env = os.environ.get("MMLU_SUBJECTS", "")
    if subjects_env:
        subject_list = [s.strip() for s in subjects_env.split(",")]
        subjects_to_evaluate = [s for s in subject_list if s in MMLU_SUBJECTS]
        # If no valid subjects found, fall back to default
        if not subjects_to_evaluate:
            subjects_to_evaluate = MMLU_SUBJECTS[:2]
    else:
        subjects_to_evaluate = MMLU_SUBJECTS[:2]  # Default: first 2 subjects

    try:
        # Run experiments on selected subjects
        all_results = []

        for subject in subjects_to_evaluate:
            console.print(f"\n[bold]Running experiment on {subject}...[/bold]")

            # Use environment variable to control model count with a sensible default
            model_count = min(
                int(os.environ.get("MODEL_COUNT", "7")), len(model_configs)
            )

            experiment = MMLUExperiment(
                subject=subject,
                sample_size=sample_size,
                model_configs=model_configs[:model_count],
                use_acceleration=True,
            )

            results = experiment.run()
            all_results.append(results)

            # Print summary for this subject
            ExperimentVisualizer.print_summary(results)

        # Run acceleration benchmarks to compare different strategies
        console.print(
            "\n[bold]Benchmarking enhanced JIT acceleration strategies:[/bold]"
        )
        # For benchmark, use same model count as experiment
        benchmark_results = run_acceleration_benchmark(
            subject=subjects_to_evaluate[0],  # Use first subject
            sample_size=sample_size,
            model_configs=model_configs[:model_count],
        )

        # Print comparison
        acc_table = Table(title="Enhanced JIT Acceleration Strategy Comparison")
        acc_table.add_column("Metric", style="cyan")
        acc_table.add_column("Sequential", style="red")
        acc_table.add_column("Wave", style="yellow")
        acc_table.add_column("Parallel", style="blue")
        acc_table.add_column("Auto", style="green")

        acc_table.add_row(
            "Execution Time",
            f"{benchmark_results['sequential_time']:.4f}s",
            f"{benchmark_results['wave_time']:.4f}s",
            f"{benchmark_results['parallel_time']:.4f}s",
            f"{benchmark_results['auto_time']:.4f}s",
        )

        # Calculate speedups
        wave_speedup = benchmark_results["sequential_time"] / max(
            benchmark_results["wave_time"], 1e-6
        )
        parallel_speedup = benchmark_results["sequential_time"] / max(
            benchmark_results["parallel_time"], 1e-6
        )
        auto_speedup = benchmark_results["sequential_time"] / max(
            benchmark_results["auto_time"], 1e-6
        )

        acc_table.add_row(
            "Speedup",
            "Baseline",
            f"{wave_speedup:.2f}x",
            f"{parallel_speedup:.2f}x",
            f"{auto_speedup:.2f}x",
        )

        # Highlight the best strategy
        best_strategy = benchmark_results.get("best_strategy", "auto").capitalize()
        acc_table.add_row(
            "Best Strategy",
            "",
            "[bold green]✓[/bold green]" if best_strategy == "Wave" else "",
            "[bold green]✓[/bold green]" if best_strategy == "Parallel" else "",
            "[bold green]✓[/bold green]" if best_strategy == "Auto" else "",
        )

        # Display JIT metrics
        jit_metrics = benchmark_results.get("jit_metrics", {})
        if jit_metrics:
            # Create a table for JIT metrics
            jit_table = Table(title="JIT Performance Metrics")
            jit_table.add_column("Metric", style="cyan")
            jit_table.add_column("Value", style="magenta")

            # Add key metrics
            jit_table.add_row(
                "Cache Hit Rate", f"{jit_metrics.get('cache_hit_rate', 0)*100:.2f}%"
            )
            jit_table.add_row(
                "Avg Compilation Time",
                f"{jit_metrics.get('avg_compilation_time_ms', 0):.2f}ms",
            )
            jit_table.add_row(
                "Avg Execution Time",
                f"{jit_metrics.get('avg_execution_time_ms', 0):.2f}ms",
            )
            jit_table.add_row("Cache Hits", str(jit_metrics.get("cache_hits", 0)))
            jit_table.add_row("Cache Misses", str(jit_metrics.get("cache_misses", 0)))
            jit_table.add_row(
                "Compilation Count", str(jit_metrics.get("compilation_count", 0))
            )
            jit_table.add_row(
                "Execution Count", str(jit_metrics.get("execution_count", 0))
            )

            # Display both tables
            console.print("\n[bold]JIT Performance Metrics:[/bold]")
            console.print(jit_table)

        # Visualize acceleration benchmark results using our specialized visualization
        console.print("\n[bold]Acceleration Benchmark Visualization:[/bold]")
        try:
            ExperimentVisualizer.plot_acceleration_comparison(
                benchmark_results=benchmark_results,
                output_path="acceleration_strategies.png",
            )
            console.print(
                "[green]Acceleration visualization saved to acceleration_strategies.png[/green]"
            )
        except Exception as e:
            console.print(f"[yellow]Visualization error: {e}[/yellow]")

        console.print(acc_table)
        # Skip visualizing results if matplotlib is not available
        try:
            # Visualize results if we have enough data
            if len(all_results) > 1:
                console.print("\n[bold]Generating visualization...[/bold]")
                ExperimentVisualizer.plot_results(all_results)
        except Exception as e:
            console.print(f"[yellow]Skipping visualization: {e}[/yellow]")

        console.print("\n[bold green]Example completed successfully![/bold green]")

    except Exception as e:
        console.print(
            Panel(
                f"Error running example: {str(e)}\n\n"
                "This example requires properly configured API keys and model availability.\n"
                "Please check your API keys and available models.",
                title="Execution Error",
                style="red",
            )
        )

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\example_architectures.py:
<code>
"""Example architectures demonstrating clean Ember operator composition patterns.

This module showcases best practices for defining and composing operators
in Ember.
"""

import logging
from typing import ClassVar, Type

# Import the non module directly from ember core

from ember.core import non
from ember.core.context import current_context
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel, Field

# Configure logging

logger = logging.getLogger(**name**)

class NetworkInput(EmberModel):
"""Input model for network operators.

    Attributes:
        query: The query to process
    """

    query: str = Field(description="The query to process")

class NetworkOutput(EmberModel):
"""Output model for network operators.

    Attributes:
        final_answer: The final processed answer
    """

    final_answer: str = Field(description="The final processed answer")

class SubNetworkSpecification(Specification):
"""Specification for SubNetwork operator."""

    # Use the pattern established in NestedNetworkSpecification
    input_model: Type[EmberModel] = NetworkInput
    structured_output: Type[EmberModel] = NetworkOutput

class SubNetwork(Operator[NetworkInput, NetworkOutput]):
"""SubNetwork that composes an ensemble with verification.

    This operator first processes inputs through an ensemble of models and subsequently verifies
    the output based on the initial ensemble's response.

    Attributes:
        specification: The operator's input/output contract
        ensemble: A uniform ensemble with N units of the specified model
        verifier: A verification operator using the specified model
    """

    specification: ClassVar[Specification] = SubNetworkSpecification()
    ensemble: non.UniformEnsemble
    verifier: non.Verifier

    def __init__(
        self, *, model_name: str = "openai:gpt-4o", num_units: int = 2
    ) -> None:
        """Initialize the SubNetwork with configurable components.

        Args:
            model_name: The model to use for both ensemble and verification
            num_units: Number of ensemble units to run in parallel
        """
        logger.debug(
            f"Initializing SubNetwork with model={model_name}, units={num_units}"
        )
        self.ensemble = non.UniformEnsemble(
            num_units=num_units, model_name=model_name, temperature=0.0
        )
        self.verifier = non.Verifier(model_name=model_name, temperature=0.0)
        logger.debug("SubNetwork initialization complete")

    def forward(self, *, inputs: NetworkInput) -> NetworkOutput:
        """Process the input through the ensemble and verify the results.

        Args:
            inputs: The validated input containing the query

        Returns:
            A NetworkOutput with the verified answer
        """
        logger.debug("Processing input through SubNetwork ensemble")
        ensemble_result = self.ensemble(query=inputs.query)

        # Extract the first response for verification
        candidate_answer = ensemble_result["responses"][0]
        logger.debug("Selected candidate answer from ensemble")

        logger.debug("Verifying candidate answer")
        verified_result = self.verifier(
            query=inputs.query, candidate_answer=candidate_answer
        )

        # Return structured output
        logger.debug("SubNetwork processing complete")
        return NetworkOutput(final_answer=verified_result["revised_answer"])

class NestedNetworkSpecification(Specification):
"""Specification for NestedNetwork operator."""

    input_model: Type[EmberModel] = NetworkInput
    structured_output: Type[EmberModel] = NetworkOutput

class NestedNetwork(Operator[NetworkInput, NetworkOutput]):
"""Nested network that aggregates results from multiple sub-networks and applies judgment.

    This operator executes two subnetwork branches and uses a judge operator to synthesize the outputs.

    Attributes:
        specification: The operator's input/output contract
        sub1: The first sub-network instance
        sub2: The second sub-network instance
        judge: A judge synthesis operator
    """

    specification: ClassVar[Specification] = NestedNetworkSpecification()
    sub1: SubNetwork
    sub2: SubNetwork
    judge: non.JudgeSynthesis

    def __init__(self, *, model_name: str = "openai:gpt-4o") -> None:
        """Initialize the NestedNetwork with sub-networks and a judge.

        Args:
            model_name: The model to use for all components
        """
        logger.debug(f"Initializing NestedNetwork with model={model_name}")
        self.sub1 = SubNetwork(model_name=model_name)
        self.sub2 = SubNetwork(model_name=model_name)
        self.judge = non.JudgeSynthesis(model_name=model_name, temperature=0.0)
        logger.debug("NestedNetwork initialization complete")

    def forward(self, *, inputs: NetworkInput) -> NetworkOutput:
        """Execute the nested network by processing through sub-networks and judging results.

        Args:
            inputs: The validated input containing the query

        Returns:
            A NetworkOutput with the final judged answer
        """
        logger.debug("Starting NestedNetwork execution")

        # Process through parallel sub-networks
        logger.debug("Processing through first sub-network")
        s1_out = self.sub1(inputs=inputs)

        logger.debug("Processing through second sub-network")
        s2_out = self.sub2(inputs=inputs)

        logger.debug("Applying judge synthesis to sub-network outputs")
        judged_result = self.judge(
            query=inputs.query, responses=[s1_out.final_answer, s2_out.final_answer]
        )

        # Return structured output
        logger.debug("NestedNetwork execution complete")
        return NetworkOutput(final_answer=judged_result["synthesized_response"])

def create_nested_network(\*, model_name: str = "gpt-4o") -> NestedNetwork:
"""Create a nested network with the specified model.

    Args:
        model_name: The model to use throughout the network

    Returns:
        A configured NestedNetwork operator
    """
    logger.info(f"Creating nested network with model: {model_name}")
    return NestedNetwork(model_name=model_name)

def create_pipeline(\*, model_name: str = "gpt-4o") -> non.Sequential:
"""Create a declarative pipeline using the Sequential NON operator.

    This demonstrates a more declarative approach to building operator pipelines
    using the Sequential operator, which chains operators together automatically.

    Args:
        model_name: The model to use throughout the pipeline

    Returns:
        A callable pipeline accepting standardized inputs
    """
    logger.info(f"Creating declarative pipeline with model: {model_name}")

    # Create a pipeline using Sequential operator for cleaner composition
    pipeline = non.Sequential(
        operators=[
            # Generate 3 responses with the same model
            non.UniformEnsemble(
                num_units=3,
                model_name=model_name,
                temperature=0.7,  # Using higher temperature for diversity
            ),
            # Pass the ensemble responses to a judge for synthesis
            non.JudgeSynthesis(model_name=model_name, temperature=0.0),
            # Verify the synthesized response
            non.Verifier(model_name=model_name, temperature=0.0),
        ]
    )

    logger.debug("Pipeline created successfully")
    return pipeline

if **name** == "**main**": # Use the centralized logging configuration with reduced verbosity
from ember.core.utils.logging import configure_logging

    configure_logging(verbose=False)

    # Initialize the ember context
    context = current_context()
    logger.info("Ember context initialized")

    # Example 1: Using the object-oriented approach
    logger.info("=== Object-Oriented Style ===")
    network = NestedNetwork(model_name="openai:gpt-4o")
    test_input = NetworkInput(
        query="What are three key principles of functional programming?"
    )
    logger.info(f"Running network with query: {test_input.query}")
    test_result = network(inputs=test_input)
    logger.info(f"Answer: {test_result.final_answer}")

    # Example 2: Using the declarative non.Sequential "pipeline" style
    logger.info("=== Declarative Pipeline Style ===")
    pipeline = create_pipeline(model_name="openai:gpt-4o")
    query = "What are three key principles of functional programming?"

    # For consistency, use kwargs pattern for pipeline invocation too
    logger.info(f"Running pipeline with query: {query}")
    result = pipeline(query=query)
    logger.info(f"Answer: {result['revised_answer']}")

</code>

src\ember\examples\advanced\model_benchmark_specialized_datasets.py:
<code>
"""Benchmark models on specialized datasets (AIME, GPQA, Codeforces).

This example demonstrates how to:

1. Load and prepare specialized datasets
2. Evaluate multiple models on these datasets
3. Compare performance across different types of tasks
4. Generate performance reports

Usage:
python -m ember.examples.advanced.model_benchmark_specialized_datasets [--dataset DATASET] [--samples N]
"""

import argparse
import logging
import re
import sys
import time
from typing import Any, Dict, List, Tuple

# Set up logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

try:
import matplotlib.pyplot as plt
import numpy as np

    HAS_VISUALIZATION = True

except ImportError:
logger.warning("Matplotlib not available. Visualizations will be skipped.")
HAS_VISUALIZATION = False

from ember.api import DatasetBuilder, datasets, models
from ember.core.exceptions import GatedDatasetAuthenticationError
from ember.core.utils.eval.evaluators import MultipleChoiceEvaluator
from ember.core.utils.eval.numeric_answer import AIMEAnswerEvaluator

def evaluate_aime(
models_config: List[Tuple[str, Any]], sample_size: int = 5
) -> Dict[str, Any]:
"""Evaluate models on AIME dataset.

    Args:
        models_config: List of (name, model) tuples to evaluate
        sample_size: Number of problems to sample

    Returns:
        Results dictionary
    """
    logger.info("Evaluating models on AIME dataset...")

    try:
        # Load dataset
        aime_data = datasets("aime")
        problems = aime_data.sample(sample_size)

        if len(problems) == 0:
            logger.error("AIME dataset loaded but contains no problems")
            return {"success": False, "error": "Empty dataset"}

        # Initialize evaluator
        evaluator = AIMEAnswerEvaluator()

        # Track results
        results = {"success": True, "model_results": {}, "problems": []}

        # Run evaluation
        for i, problem in enumerate(problems):
            logger.info(f"\nProblem {i+1}: {problem.query[:100]}...")
            logger.info(f"Expected answer: {problem.metadata['correct_answer']}")

            problem_result = {
                "id": problem.metadata.get("problem_id", f"problem_{i}"),
                "query": problem.query,
                "answer": problem.metadata["correct_answer"],
                "model_responses": {},
            }

            # Evaluate each model
            for name, model in models_config:
                if name not in results["model_results"]:
                    results["model_results"][name] = {
                        "correct": 0,
                        "total": 0,
                        "time": 0,
                    }

                start_time = time.time()
                response = model(problem.query)
                inference_time = time.time() - start_time

                result = evaluator.evaluate(
                    response, problem.metadata["correct_answer"]
                )

                # Update results
                problem_result["model_responses"][name] = {
                    "response": (
                        response[:200] + "..." if len(response) > 200 else response
                    ),
                    "is_correct": result.is_correct,
                    "extracted_value": result.metadata.get(
                        "extracted_value", "Not found"
                    ),
                    "method": result.metadata.get("extracted_method", "Unknown"),
                    "time": inference_time,
                }

                if result.is_correct:
                    results["model_results"][name]["correct"] += 1
                results["model_results"][name]["total"] += 1
                results["model_results"][name]["time"] += inference_time

                logger.info(
                    f"{name}: {'✓' if result.is_correct else '✗'} "
                    + f"(Found: {result.metadata.get('extracted_value', 'Not found')}) "
                    + f"[{inference_time:.2f}s]"
                )

            results["problems"].append(problem_result)

        # Calculate accuracy
        for name in results["model_results"]:
            model_data = results["model_results"][name]
            model_data["accuracy"] = (
                model_data["correct"] / model_data["total"]
                if model_data["total"] > 0
                else 0
            )
            model_data["avg_time"] = (
                model_data["time"] / model_data["total"]
                if model_data["total"] > 0
                else 0
            )

        return results

    except Exception as e:
        logger.error(f"AIME evaluation error: {e}")
        return {"success": False, "error": str(e)}

def evaluate_gpqa(
models_config: List[Tuple[str, Any]], sample_size: int = 5
) -> Dict[str, Any]:
"""Evaluate models on GPQA dataset.

    Args:
        models_config: List of (name, model) tuples to evaluate
        sample_size: Number of problems to sample

    Returns:
        Results dictionary
    """
    logger.info("Evaluating models on GPQA dataset...")

    try:
        # Load dataset
        gpqa_data = datasets("gpqa")
        problems = gpqa_data.sample(sample_size)

        if len(problems) == 0:
            logger.error("GPQA dataset loaded but contains no questions")
            return {"success": False, "error": "Empty dataset"}

        # Initialize evaluator
        evaluator = MultipleChoiceEvaluator()

        # Track results
        results = {"success": True, "model_results": {}, "problems": []}

        # Run evaluation
        for i, problem in enumerate(problems):
            logger.info(f"\nQuestion {i+1}: {problem.query[:100]}...")
            logger.info(f"Choices: {list(problem.choices.keys())}")
            logger.info(f"Expected answer: {problem.metadata['correct_answer']}")

            # Format prompt
            prompt = problem.query + "\n\n"
            for key, choice in problem.choices.items():
                prompt += f"{key}. {choice}\n"

            problem_result = {
                "id": problem.metadata.get("id", f"question_{i}"),
                "query": problem.query,
                "choices": problem.choices,
                "answer": problem.metadata["correct_answer"],
                "subject": problem.metadata.get("subject", "Unknown"),
                "model_responses": {},
            }

            # Evaluate each model
            for name, model in models_config:
                if name not in results["model_results"]:
                    results["model_results"][name] = {
                        "correct": 0,
                        "total": 0,
                        "time": 0,
                    }

                start_time = time.time()
                response = model(prompt)
                inference_time = time.time() - start_time

                result = evaluator.evaluate(
                    response, problem.metadata["correct_answer"]
                )

                # Update results
                problem_result["model_responses"][name] = {
                    "response": (
                        response[:200] + "..." if len(response) > 200 else response
                    ),
                    "is_correct": result.is_correct,
                    "time": inference_time,
                }

                if result.is_correct:
                    results["model_results"][name]["correct"] += 1
                results["model_results"][name]["total"] += 1
                results["model_results"][name]["time"] += inference_time

                logger.info(
                    f"{name}: {'✓' if result.is_correct else '✗'} [{inference_time:.2f}s]"
                )

            results["problems"].append(problem_result)

        # Calculate accuracy
        for name in results["model_results"]:
            model_data = results["model_results"][name]
            model_data["accuracy"] = (
                model_data["correct"] / model_data["total"]
                if model_data["total"] > 0
                else 0
            )
            model_data["avg_time"] = (
                model_data["time"] / model_data["total"]
                if model_data["total"] > 0
                else 0
            )

        return results

    except GatedDatasetAuthenticationError as e:
        logger.error(f"Authentication required: {e.recovery_hint}")
        logger.info(
            "Request access at: https://huggingface.co/datasets/Idavidrein/gpqa"
        )
        return {
            "success": False,
            "error": "Authentication required",
            "recovery": e.recovery_hint,
        }
    except Exception as e:
        logger.error(f"GPQA evaluation error: {e}")
        return {"success": False, "error": str(e)}

def evaluate_codeforces(
models_config: List[Tuple[str, Any]], sample_size: int = 3
) -> Dict[str, Any]:
"""Evaluate models on Codeforces dataset.

    Args:
        models_config: List of (name, model) tuples to evaluate
        sample_size: Number of problems to sample

    Returns:
        Results dictionary
    """
    logger.info("Evaluating models on Codeforces dataset...")

    try:
        # Test with difficulty filtering
        cf_data = (
            DatasetBuilder()
            .from_registry("codeforces")
            .configure(difficulty_range=(800, 1200))
            .sample(sample_size)
            .build()
        )

        if len(cf_data) == 0:
            logger.error("Codeforces dataset loaded but contains no problems")
            return {"success": False, "error": "Empty dataset"}

        # Track results
        results = {"success": True, "model_results": {}, "problems": []}

        # Run evaluation (solution generation only, no execution)
        for i, problem in enumerate(problems := cf_data):
            logger.info(f"\nProblem {i+1}: {problem.query[:100]}...")
            logger.info(f"Difficulty: {problem.metadata.get('difficulty')}")

            problem_result = {
                "id": problem.metadata.get("id", f"problem_{i}"),
                "query": (
                    problem.query[:500] + "..."
                    if len(problem.query) > 500
                    else problem.query
                ),
                "difficulty": problem.metadata.get("difficulty"),
                "tags": problem.metadata.get("tags", []),
                "model_responses": {},
            }

            # Generate solutions with each model (no evaluation)
            for name, model in models_config:
                if name not in results["model_results"]:
                    results["model_results"][name] = {
                        "solutions": 0,
                        "total": 0,
                        "time": 0,
                    }

                prompt = f"Solve this programming problem and provide a Python solution:\n\n{problem.query}\n\nReturn only the Python code solution inside ```python ``` code blocks."

                start_time = time.time()
                response = model(prompt)
                inference_time = time.time() - start_time

                # Extract code from response
                code_match = re.search(r"```python\n(.*?)```", response, re.DOTALL)
                has_solution = bool(code_match)

                # Update results
                problem_result["model_responses"][name] = {
                    "response_preview": (
                        response[:200] + "..." if len(response) > 200 else response
                    ),
                    "has_solution": has_solution,
                    "time": inference_time,
                }

                if has_solution:
                    results["model_results"][name]["solutions"] += 1
                results["model_results"][name]["total"] += 1
                results["model_results"][name]["time"] += inference_time

                logger.info(
                    f"{name}: {'✓' if has_solution else '✗'} solution generated [{inference_time:.2f}s]"
                )

            results["problems"].append(problem_result)

        # Calculate metrics
        for name in results["model_results"]:
            model_data = results["model_results"][name]
            model_data["solution_rate"] = (
                model_data["solutions"] / model_data["total"]
                if model_data["total"] > 0
                else 0
            )
            model_data["avg_time"] = (
                model_data["time"] / model_data["total"]
                if model_data["total"] > 0
                else 0
            )

        return results

    except Exception as e:
        logger.error(f"Codeforces evaluation error: {e}")
        return {"success": False, "error": str(e)}

def display_results(dataset_name: str, results: Dict[str, Any]) -> None:
"""Display evaluation results.

    Args:
        dataset_name: Name of the dataset
        results: Results dictionary
    """
    if not results.get("success", False):
        logger.error(
            f"{dataset_name} evaluation failed: {results.get('error', 'Unknown error')}"
        )
        if "recovery" in results:
            logger.info(f"Recovery: {results['recovery']}")
        return

    logger.info(f"\n----- {dataset_name} Results -----")
    logger.info(f"Problems evaluated: {len(results.get('problems', []))}")

    model_results = results.get("model_results", {})

    # Display model accuracy
    for name, data in model_results.items():
        if dataset_name.lower() == "codeforces":
            solution_rate = data.get("solution_rate", 0)
            solutions = data.get("solutions", 0)
            total = data.get("total", 0)
            avg_time = data.get("avg_time", 0)
            logger.info(
                f"{name}: {solutions}/{total} solutions generated ({solution_rate:.1%}) - {avg_time:.2f}s avg"
            )
        else:
            accuracy = data.get("accuracy", 0)
            correct = data.get("correct", 0)
            total = data.get("total", 0)
            avg_time = data.get("avg_time", 0)
            logger.info(
                f"{name}: {correct}/{total} correct ({accuracy:.1%}) - {avg_time:.2f}s avg"
            )

def visualize_comparison(all_results: Dict[str, Dict[str, Any]]) -> None:
"""Visualize performance comparison across datasets.

    Args:
        all_results: Dictionary with dataset results
    """
    if not HAS_VISUALIZATION:
        logger.warning("Skipping visualization - matplotlib not available")
        return

    # Extract data for visualization
    datasets = []
    model_names = set()

    # First, identify all models and valid datasets
    for dataset_name, result in all_results.items():
        if result.get("success", False):
            datasets.append(dataset_name)
            for model in result.get("model_results", {}):
                model_names.add(model)

    if not datasets or not model_names:
        logger.warning("No valid data for visualization")
        return

    # Convert to sorted lists for consistent ordering
    datasets = sorted(datasets)
    model_names = sorted(model_names)

    # Create accuracy matrix
    accuracy_data = []
    time_data = []

    for model in model_names:
        model_accuracy = []
        model_time = []
        for dataset in datasets:
            result = all_results.get(dataset, {})
            if result.get("success", False):
                model_result = result.get("model_results", {}).get(model, {})

                # Handle Codeforces differently (solution rate vs accuracy)
                if dataset.lower() == "codeforces":
                    accuracy = model_result.get("solution_rate", 0)
                else:
                    accuracy = model_result.get("accuracy", 0)

                model_accuracy.append(accuracy * 100)  # Convert to percentage
                model_time.append(model_result.get("avg_time", 0))
            else:
                model_accuracy.append(0)
                model_time.append(0)

        accuracy_data.append(model_accuracy)
        time_data.append(model_time)

    # Create visualization
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Performance comparison
    bar_width = 0.8 / len(model_names)
    x = np.arange(len(datasets))

    for i, (model, accuracy) in enumerate(zip(model_names, accuracy_data)):
        offset = (i - len(model_names) / 2 + 0.5) * bar_width
        ax1.bar(x + offset, accuracy, bar_width, label=model)

    # Customize performance chart
    ax1.set_ylabel("Accuracy (%)")
    ax1.set_title("Model Performance by Dataset")
    ax1.set_xticks(x)
    ax1.set_xticklabels([d.upper() for d in datasets])
    ax1.legend(loc="upper center", bbox_to_anchor=(0.5, -0.15), ncol=len(model_names))
    ax1.grid(True, linestyle="--", alpha=0.7)
    ax1.set_ylim(0, 105)  # Make room for labels

    # Add data labels
    for i, model_data in enumerate(accuracy_data):
        for j, v in enumerate(model_data):
            offset = (i - len(model_names) / 2 + 0.5) * bar_width
            ax1.text(
                j + offset, v + 1, f"{v:.1f}%", ha="center", va="bottom", fontsize=8
            )

    # Time comparison
    for i, (model, times) in enumerate(zip(model_names, time_data)):
        offset = (i - len(model_names) / 2 + 0.5) * bar_width
        ax2.bar(x + offset, times, bar_width, label=model)

    # Customize time chart
    ax2.set_ylabel("Average Time (seconds)")
    ax2.set_title("Model Inference Time by Dataset")
    ax2.set_xticks(x)
    ax2.set_xticklabels([d.upper() for d in datasets])
    ax2.legend(loc="upper center", bbox_to_anchor=(0.5, -0.15), ncol=len(model_names))
    ax2.grid(True, linestyle="--", alpha=0.7)

    # Add data labels
    for i, model_data in enumerate(time_data):
        for j, v in enumerate(model_data):
            offset = (i - len(model_names) / 2 + 0.5) * bar_width
            ax2.text(
                j + offset, v + 0.1, f"{v:.1f}s", ha="center", va="bottom", fontsize=8
            )

    plt.tight_layout()
    plt.savefig("model_benchmark_results.png", dpi=300, bbox_inches="tight")
    logger.info("Visualization saved to model_benchmark_results.png")
    plt.show()

def main() -> None:
"""Run the model benchmark on specialized datasets."""
parser = argparse.ArgumentParser(
description="Benchmark models on specialized datasets"
)
parser.add_argument(
"--dataset",
choices=["aime", "gpqa", "codeforces", "all"],
default="all",
help="Dataset to evaluate",
)
parser.add_argument(
"--samples", type=int, default=5, help="Number of samples to evaluate"
)
args = parser.parse_args()

    # Configure models to evaluate
    models_config = [
        ("gpt-4o", models.openai.gpt4o()),
        ("claude-3-opus", models.anthropic.claude_3_opus()),
        ("claude-3-sonnet", models.anthropic.claude_3_sonnet()),
    ]

    results = {}

    # Run evaluations
    if args.dataset in ["aime", "all"]:
        results["AIME"] = evaluate_aime(models_config, args.samples)
        display_results("AIME", results["AIME"])

    if args.dataset in ["gpqa", "all"]:
        results["GPQA"] = evaluate_gpqa(models_config, args.samples)
        display_results("GPQA", results["GPQA"])

    if args.dataset in ["codeforces", "all"]:
        results["Codeforces"] = evaluate_codeforces(models_config, min(args.samples, 3))
        display_results("Codeforces", results["Codeforces"])

    # Generate visualization if multiple datasets were evaluated
    if len(results) > 1:
        visualize_comparison(results)

    # Check if any evaluations succeeded
    any_success = any(r.get("success", False) for r in results.values())
    if not any_success:
        logger.error("All evaluations failed")
        sys.exit(1)
    else:
        logger.info("\nBenchmark completed successfully")
        logger.info("See model_benchmark_results.png for visualization")

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\parallel_benchmark.py:
<code>
"""
Advanced Benchmark: Measuring Parallelization Impact in Ensemble LLM Execution

This dedicated benchmark script measures the performance difference between:

1. Sequential execution
2. Auto-parallelized execution
3. Explicitly parallelized execution

It uses the ensemble pattern from ensemble_judge_mmlu.py but focuses purely on
measuring execution strategies with precise instrumentation.

Usage:
MODEL_COUNT=10 uv run python -m ember.examples.advanced.parallel_benchmark

Environment variables:
MODEL_COUNT: Number of models to use in the ensemble (default: 10)
REPEAT: Number of benchmark repetitions (default: 3)
"""

import time
from typing import Any, ClassVar, Dict, List, Type

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# Import necessary modules

from ember.api.operators import Operator
from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.operator.base.operator_base import Specification
from ember.core.registry.specification.specification import (
Specification as CoreSpecification,
)
from ember.core.types.ember_model import EmberModel
from ember.xcs import jit
from ember.xcs.engine.execution_options import execution_options

# Set up console for rich output

console = Console()

# Define input/output data structures using EmberModel for compatibility with XCS

class BenchmarkInput(EmberModel):
"""Input for benchmark."""

    prompt: str

class BenchmarkOutput(EmberModel):
"""Output for benchmark."""

    result: str
    model_name: str

class BenchmarkEnsembleOutput(EmberModel):
"""Output from ensemble."""

    results: List[BenchmarkOutput]

class BenchmarkSpecification(CoreSpecification):
"""Specification for benchmark operator."""

    # Define both input and output models explicitly
    input_model: Type[BenchmarkInput] = BenchmarkInput
    structured_output: Type[BenchmarkOutput] = BenchmarkOutput

    prompt_template: str = """Respond with the exact phrase: 'This is a benchmark response from {model_name}'

Here is the input prompt: {prompt}

Your response must contain only the requested phrase.
"""

class SingleModelOperator(Operator[BenchmarkInput, BenchmarkOutput]):
"""Baseline operator using a single model for benchmarking."""

    specification: ClassVar[Specification] = BenchmarkSpecification()
    model_name: str
    temperature: float
    max_tokens: int
    lm_module: LMModule

    def __init__(
        self,
        model_name: str = "anthropic:claude-3-haiku",
        temperature: float = 0.0,
        max_tokens: int = 16,
    ) -> None:
        """Initialize the benchmark operator with model configuration."""
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens

        # Initialize the LM module
        self.lm_module = LMModule(
            config=LMModuleConfig(
                model_name=model_name, temperature=temperature, max_tokens=max_tokens
            )
        )

    def forward(self, *, inputs: BenchmarkInput) -> BenchmarkOutput:
        """Process a prompt with the model."""
        # Create template variables with the model name included
        template_vars = {"prompt": inputs.prompt, "model_name": self.model_name}

        # Fill in the template with standard Python format strings
        prompt = self.specification.prompt_template.format(**template_vars)

        # Get model response
        response = self.lm_module(prompt=prompt)

        return BenchmarkOutput(result=response, model_name=self.model_name)

class EnsembleSpecification(CoreSpecification):
"""Specification for ensemble operator."""

    # Explicit models for proper type handling
    input_model: Type[BenchmarkInput] = BenchmarkInput
    structured_output: Type[BenchmarkEnsembleOutput] = BenchmarkEnsembleOutput

@jit
class EnsembleOperator(Operator[BenchmarkInput, BenchmarkEnsembleOutput]):
"""JIT-optimized parallel ensemble operator.

    This operator runs multiple model workers in parallel using the same input.
    The JIT optimization automatically parallelizes the model calls when possible.
    """

    specification: ClassVar[Specification] = EnsembleSpecification()
    models: List[SingleModelOperator]

    def __init__(self, model_configs: List[Dict[str, Any]]) -> None:
        """Initialize the ensemble with multiple model operators."""
        self.models = []
        for config in model_configs:
            self.models.append(
                SingleModelOperator(
                    model_name=config["model_name"],
                    temperature=config.get("temperature", 0.0),
                    max_tokens=config.get("max_tokens", 16),
                )
            )

    def forward(self, *, inputs: BenchmarkInput) -> BenchmarkEnsembleOutput:
        """Process the input through all models in parallel."""
        results = []

        # Process with each model - the JIT optimizer can recognize these as independent operations
        for model_op in self.models:
            # Each call can be parallelized
            result = model_op(inputs=inputs)
            results.append(result)

        return BenchmarkEnsembleOutput(results=results)

def run_benchmarks(
model_configs: List[Dict[str, Any]], repeats: int = 3
) -> Dict[str, Any]:
"""Run benchmarks comparing sequential vs parallel execution.

    Performs multiple timing runs to ensure statistical significance.

    Args:
        model_configs: List of model configurations to use
        repeats: Number of benchmark repetitions

    Returns:
        Dictionary of benchmark results
    """
    # Create test input
    test_input = BenchmarkInput(prompt="What are compound AI systems?")

    # Create ensemble operator
    ensemble = EnsembleOperator(model_configs)

    # Statistics for each approach
    sequential_times = []
    auto_parallel_times = []
    explicit_parallel_times = []

    # Print benchmark configuration
    num_models = len(model_configs)
    console.print(
        f"[bold]Running benchmark with {num_models} models, {repeats} repetitions[/bold]"
    )
    model_table = Table(title="Models Used")
    model_table.add_column("Index", style="cyan")
    model_table.add_column("Model", style="green")
    model_table.add_column("Temperature", style="yellow")

    for i, config in enumerate(model_configs):
        model_table.add_row(
            str(i + 1), config["model_name"], str(config.get("temperature", 0.0))
        )
    console.print(model_table)

    # Run benchmarks
    console.print("\n[bold]Starting benchmark runs...[/bold]")

    for i in range(repeats):
        console.print(f"\n[bold]Benchmark Run {i+1}/{repeats}[/bold]")

        # 1. Sequential execution (force sequential with execution_options)
        console.print("  Measuring sequential execution...")
        start_time = time.perf_counter()
        with execution_options(use_parallel=False):
            sequential_result = ensemble(inputs=test_input)
        sequential_time = time.perf_counter() - start_time
        sequential_times.append(sequential_time)
        console.print(f"    Sequential time: {sequential_time:.4f}s")

        # 2. Auto-parallel execution (default JIT behavior)
        console.print("  Measuring auto-parallel execution...")
        start_time = time.perf_counter()
        auto_result = ensemble(inputs=test_input)
        auto_time = time.perf_counter() - start_time
        auto_parallel_times.append(auto_time)
        console.print(f"    Auto-parallel time: {auto_time:.4f}s")

        # 3. Explicit parallel execution
        console.print("  Measuring explicit parallel execution...")
        start_time = time.perf_counter()
        with execution_options(use_parallel=True):
            explicit_result = ensemble(inputs=test_input)
        explicit_time = time.perf_counter() - start_time
        explicit_parallel_times.append(explicit_time)
        console.print(f"    Explicit parallel time: {explicit_time:.4f}s")

    # Calculate averages
    avg_sequential = sum(sequential_times) / len(sequential_times)
    avg_auto = sum(auto_parallel_times) / len(auto_parallel_times)
    avg_explicit = sum(explicit_parallel_times) / len(explicit_parallel_times)

    # Calculate speedups
    auto_speedup = avg_sequential / avg_auto if avg_auto > 0 else 0
    explicit_speedup = avg_sequential / avg_explicit if avg_explicit > 0 else 0

    # Calculate time reductions as percentages
    auto_reduction = (
        ((avg_sequential - avg_auto) / avg_sequential) * 100
        if avg_sequential > 0
        else 0
    )
    explicit_reduction = (
        ((avg_sequential - avg_explicit) / avg_sequential) * 100
        if avg_sequential > 0
        else 0
    )

    # Print clean, elegant results
    console.print("\n[bold]Benchmark Results[/bold]")

    results_table = Table(
        title=f"JIT Optimization Performance ({len(model_configs)} models)"
    )
    results_table.add_column("Execution Strategy", style="cyan")
    results_table.add_column("Avg Time", style="yellow")
    results_table.add_column("Speedup", style="green")

    results_table.add_row("Sequential", f"{avg_sequential:.4f}s", "1.00x (baseline)")

    results_table.add_row("Auto Parallel", f"{avg_auto:.4f}s", f"{auto_speedup:.2f}x")

    results_table.add_row(
        "Explicit Parallel", f"{avg_explicit:.4f}s", f"{explicit_speedup:.2f}x"
    )

    console.print(results_table)

    # Show which strategy performed best, in a simple way
    best_strategy = (
        "Auto Parallel"
        if auto_speedup > explicit_speedup
        else (
            "Explicit Parallel"
            if explicit_speedup > auto_speedup
            else "Equal Performance"
        )
    )
    console.print(f"\n[bold]Best strategy:[/bold] {best_strategy}")

    # Show raw run data
    run_table = Table(title="Individual Run Times")
    run_table.add_column("Run", style="cyan")
    run_table.add_column("Sequential", style="red")
    run_table.add_column("Auto Parallel", style="yellow")
    run_table.add_column("Explicit Parallel", style="green")

    for i in range(len(sequential_times)):
        run_table.add_row(
            f"Run {i+1}",
            f"{sequential_times[i]:.4f}s",
            f"{auto_parallel_times[i]:.4f}s",
            f"{explicit_parallel_times[i]:.4f}s",
        )

    console.print(run_table)

    # Return detailed results for further analysis
    return {
        "sequential_times": sequential_times,
        "auto_parallel_times": auto_parallel_times,
        "explicit_parallel_times": explicit_parallel_times,
        "avg_sequential": avg_sequential,
        "avg_auto": avg_auto,
        "avg_explicit": avg_explicit,
        "auto_speedup": auto_speedup,
        "explicit_speedup": explicit_speedup,
        "auto_reduction": auto_reduction,
        "explicit_reduction": explicit_reduction,
        "num_models": len(model_configs),
    }

def main() -> None:
"""Main function to run the parallelization benchmark."""
import os

    # Print header
    console.print(
        Panel.fit(
            "Measuring JIT Parallelization Impact in Ensemble LLM Execution",
            title="Ember Parallelization Benchmark",
            subtitle="Comparing Sequential vs Auto-Parallel vs Explicit Parallel",
            style="bold cyan",
        )
    )

    # Check if API keys are available
    api_keys = {
        "ANTHROPIC_API_KEY": os.environ.get("ANTHROPIC_API_KEY"),
        "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY"),
        "GOOGLE_API_KEY": os.environ.get("GOOGLE_API_KEY"),
    }

    if not any(api_keys.values()):
        console.print(
            Panel(
                "⚠️ No API keys found for language model providers.\n\n"
                "To run this benchmark, please set at least one of these environment variables:\n"
                "  - ANTHROPIC_API_KEY\n"
                "  - OPENAI_API_KEY\n"
                "  - GOOGLE_API_KEY\n\n"
                "Example:\n"
                "  export ANTHROPIC_API_KEY=sk_ant_xxxx",
                title="API Keys Required",
                style="yellow",
            )
        )
        return

    # Basic model configurations - prefer lightweight models for benchmarking
    model_configs = [
        {
            "model_name": "anthropic:claude-3-haiku",
            "temperature": 0.0,
            "max_tokens": 16,
        },
        {
            "model_name": "anthropic:claude-3-haiku",
            "temperature": 0.7,
            "max_tokens": 16,
        },
    ]

    # Add OpenAI model if available
    if os.environ.get("OPENAI_API_KEY"):
        model_configs.append(
            {"model_name": "openai:gpt-3.5-turbo", "temperature": 0.0, "max_tokens": 16}
        )

    # Get number of models to use
    model_count = min(int(os.environ.get("MODEL_COUNT", "2")), len(model_configs))
    repeats = int(os.environ.get("REPEAT", "3"))

    try:
        # Run benchmarks
        results = run_benchmarks(
            model_configs=model_configs[:model_count], repeats=repeats
        )

        # Final summary
        console.print("\n[bold green]Benchmark completed successfully![/bold green]")
        best_speedup = max(results["auto_speedup"], results["explicit_speedup"])
        console.print(
            f"[bold]JIT parallelization achieved {best_speedup:.2f}x speedup[/bold]"
        )

    except Exception as e:
        console.print(
            Panel(
                f"Error running benchmark: {str(e)}\n\n"
                "This benchmark requires properly configured API keys and model availability.\n"
                "Please check your API keys and available models.",
                title="Execution Error",
                style="red",
            )
        )

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\parallel_pipeline_example.py:
<code>
"""Parallel Pipeline Example with JIT.

This example demonstrates building a more complex pipeline with
branching paths and parallel execution using JIT-enabled operators.

Note: In the current implementation, each operator needs to be
decorated with @jit separately, and the graph must be built manually.
Future versions will simplify this process.

To run:
uv run python src/ember/examples/advanced/parallel_pipeline_example.py
"""

import logging
import time
from typing import Any, Dict, Optional

# Use direct non import for ensemble

from ember.core.non import UniformEnsemble

# ember imports

from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.operator.core.synthesis_judge import JudgeSynthesisOperator
from ember.xcs.engine.xcs_engine import (
TopologicalSchedulerWithParallelDispatch,
execute_graph,
)
from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.tracer.tracer_decorator import jit

###############################################################################

# JIT-Decorated Ensemble Operators

###############################################################################

@jit()
class FactEnsemble(UniformEnsemble):
"""Ensemble focused on factual information."""

    pass

@jit()
class CreativeEnsemble(UniformEnsemble):
"""Ensemble focused on creative responses."""

    pass

@jit()
class DetailedEnsemble(UniformEnsemble):
"""Ensemble focused on detailed explanations."""

    pass

###############################################################################

# Pipeline Construction and Execution

###############################################################################
def build_multi_branch_pipeline(
\*, query: str, max_workers: Optional[int] = None
) -> Dict[str, Any]:
"""Build and execute a multi-branch pipeline with parallel execution.

    This creates a pipeline with three parallel branches (fact, creative, detailed),
    each with its own ensemble of models. The results are then synthesized by a judge.

    Args:
        query: The query to process
        max_workers: Maximum number of worker threads

    Returns:
        The final pipeline result
    """
    # Create the ensemble operators
    fact_ensemble = FactEnsemble(
        num_units=3,
        model_name="openai:gpt-4o-mini",
        temperature=0.3,  # Lower temperature for facts
    )

    creative_ensemble = CreativeEnsemble(
        num_units=3,
        model_name="openai:gpt-4o-mini",
        temperature=0.9,  # Higher temperature for creativity
    )

    detailed_ensemble = DetailedEnsemble(
        num_units=3,
        model_name="openai:gpt-4o-mini",
        temperature=0.5,  # Medium temperature
    )

    # Create the judge operator directly from core implementation
    lm_module = LMModule(
        config=LMModuleConfig(
            model_name="openai:gpt-4o",
            temperature=0.0,
        )
    )
    judge = JudgeSynthesisOperator(lm_module=lm_module)

    # Build the graph
    graph = XCSGraph()

    # Add nodes
    fact_id = graph.add_node(operator=fact_ensemble, node_id="fact_ensemble")
    creative_id = graph.add_node(
        operator=creative_ensemble, node_id="creative_ensemble"
    )
    detailed_id = graph.add_node(
        operator=detailed_ensemble, node_id="detailed_ensemble"
    )
    judge_id = graph.add_node(operator=judge, node_id="judge")

    # Connect all ensembles to the judge
    graph.add_edge(from_id=fact_id, to_id=judge_id)
    graph.add_edge(from_id=creative_id, to_id=judge_id)
    graph.add_edge(from_id=detailed_id, to_id=judge_id)

    # Use parallel execution
    workers = (
        max_workers or 12
    )  # Default to 12 workers (3 ensembles × 3 units + overhead)
    scheduler = TopologicalSchedulerWithParallelDispatch(max_workers=workers)

    # Execute the graph
    start_time = time.perf_counter()
    result = execute_graph(
        graph=graph, global_input={"query": query}, scheduler=scheduler
    )
    end_time = time.perf_counter()

    logging.info(f"Pipeline execution completed in {end_time - start_time:.4f}s")

    return result

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run demonstration of parallel pipeline with JIT-enabled operators."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("\n=== Parallel Pipeline with JIT ===\n")

    # Example query that benefits from multiple perspectives
    query = "Explain the impact of artificial intelligence on society."

    print(f"Processing query: {query}")
    result = build_multi_branch_pipeline(query=query)

    print("\n=== Final Synthesized Answer ===")
    # Handle the actual output format from JudgeSynthesisOperator
    if "final_answer" in result:
        print(result["final_answer"])
    else:
        print("No final answer found in result")
        print(f"Available keys: {list(result.keys())}")

    # Print reasoning if available
    if "reasoning" in result:
        print("\n=== Judge's Reasoning ===")
        print(result["reasoning"])

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\README.md:
<code>

# Advanced Ember Examples

This directory contains advanced examples demonstrating complex Ember systems, sophisticated AI architectures, and performance optimization techniques.

## Examples

- `parallel_pipeline_example.py` - Building and optimizing parallel execution pipelines
- `clean_jit_example.py` - Advanced JIT patterns for complex systems
- `example_architectures.py` - Reference architectures for complex AI systems
- `diagnose_model_discovery.py` - Debugging model discovery issues
- `test_auto_discovery.py` - Testing automatic model discovery

## Running Examples

To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/advanced/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/advanced/example_name.py
```

Replace `example_name.py` with the desired example file.

## Advanced Concepts

These examples demonstrate advanced Ember capabilities such as:

- Complex nested operator architectures
- Performance optimization techniques
- Multi-model ensemble patterns
- Sophisticated execution flow control
- Advanced error handling and debugging

## Next Steps

After mastering these advanced examples, you're ready to build complex AI systems with Ember! Check out the full documentation at https://pyember.ai.

</code>

src\ember\examples\advanced\reasoning_system.py:
<code>
"""Advanced Reasoning System Example

This example demonstrates building a sophisticated reasoning system using Ember's
advanced features, including ensembles, verification, and synthesis of responses.

This is an advanced example that showcases:

1. Complex operator composition
2. Multiple LLM reasoning paths
3. Verification of reasoning steps
4. Synthesis of final response
5. Automated parallelization

To run:
uv run python src/ember/examples/advanced/reasoning_system.py
"""

from typing import Any, ClassVar, Dict, List, Optional, Type

# Ember API imports

from ember.api.xcs import execution_options, jit

# Keep non import for UniformEnsemble

from ember.core import non
from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.operator.core.synthesis_judge import JudgeSynthesisOperator
from ember.core.registry.operator.core.verifier import VerifierOperator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel, Field

###############################################################################

# Input/Output Models

###############################################################################

class ReasoningInput(EmberModel):
"""Input for the advanced reasoning system."""

    query: str = Field(description="The reasoning query to process")
    context: Optional[str] = Field(
        default=None, description="Optional context to provide background knowledge"
    )

class ReasoningSteps(EmberModel):
"""Structured representation of reasoning steps."""

    steps: List[str] = Field(description="Individual reasoning steps")
    conclusion: str = Field(description="Conclusion based on reasoning")
    confidence: float = Field(description="Confidence score (0-1)", ge=0, le=1)

class VerificationResult(EmberModel):
"""Verification results for reasoning paths."""

    verified_steps: List[str] = Field(description="Verified reasoning steps")
    accuracy_score: float = Field(description="Accuracy score (0-1)", ge=0, le=1)
    coherence_score: float = Field(description="Coherence score (0-1)", ge=0, le=1)
    completeness_score: float = Field(
        description="Completeness score (0-1)", ge=0, le=1
    )

class ReasoningOutput(EmberModel):
"""Output from the advanced reasoning system."""

    query: str = Field(description="The original query")
    final_answer: str = Field(description="The synthesized final answer")
    confidence: float = Field(
        description="Overall confidence in the answer (0-1)", ge=0, le=1
    )
    reasoning_paths: List[ReasoningSteps] = Field(
        description="Multiple reasoning paths that were considered"
    )
    verification_results: List[VerificationResult] = Field(
        description="Verification results for each reasoning path"
    )

# Specification for the reasoning system

class ReasoningSpecification(Specification):
"""Specification for the advanced reasoning system."""

    input_model: Type[EmberModel] = ReasoningInput
    structured_output: Type[EmberModel] = ReasoningOutput

###############################################################################

# Reasoning Ensemble Operator

###############################################################################

@jit
class ReasoningEnsemble(Operator[ReasoningInput, Dict[str, Any]]):
"""Generates multiple reasoning paths for a query using different models."""

    # Class-level field declaration
    ensemble: non.UniformEnsemble

    def __init__(self, num_units: int = 3):
        """Initialize with configurable number of ensemble units."""
        self.ensemble = non.UniformEnsemble(
            num_units=num_units, model_name="openai:gpt-4o", temperature=0.7
        )

    def forward(self, *, inputs: ReasoningInput) -> Dict[str, Any]:
        """Generate multiple reasoning paths for the input query."""
        ensemble_inputs = {"query": inputs.query}

        if inputs.context:
            ensemble_inputs["context"] = inputs.context

        # This will be automatically parallelized
        ensemble_results = self.ensemble(inputs=ensemble_inputs)

        # Parse raw responses into structured reasoning steps
        reasoning_paths = []
        for response in ensemble_results.get("responses", []):
            # In a real implementation, this would use proper parsing
            steps = response.split("\n\n")[:3]  # Simplified parsing
            conclusion = response.split("\n\n")[-1] if "\n\n" in response else response

            reasoning_paths.append(
                ReasoningSteps(
                    steps=steps,
                    conclusion=conclusion,
                    confidence=0.8,  # Simplified confidence assignment
                )
            )

        return {"reasoning_paths": reasoning_paths, "query": inputs.query}

###############################################################################

# Verification Operator

###############################################################################

@jit
class ReasoningVerifier(Operator[Dict[str, Any], Dict[str, Any]]):
"""Verifies each reasoning path for accuracy, coherence, and completeness."""

    # Class-level field declaration
    verifier: VerifierOperator

    def __init__(self, model_name: str = "anthropic:claude-3-sonnet"):
        """Initialize with configurable model."""
        # Create LM module for the verifier
        lm_module = LMModule(
            config=LMModuleConfig(
                model_name=model_name,
                temperature=0.2,
            )
        )

        # Use the core VerifierOperator directly
        self.verifier = VerifierOperator(lm_module=lm_module)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Verify each reasoning path in the inputs."""
        query = inputs.get("query", "")
        reasoning_paths = inputs.get("reasoning_paths", [])

        verification_results = []
        for path in reasoning_paths:
            # Use the verifier operator to check this reasoning path
            verification_input = {
                "query": query,
                "reasoning": path.conclusion,
                "steps": path.steps,
            }

            # This would be automatically parallelized across paths
            result = self.verifier(inputs=verification_input)

            # Parse verification results
            verification_results.append(
                VerificationResult(
                    verified_steps=path.steps,  # Simplified - would be filtered in real implementation
                    accuracy_score=result.get("accuracy_score", 0.0),
                    coherence_score=result.get("coherence_score", 0.0),
                    completeness_score=result.get("completeness_score", 0.0),
                )
            )

        return {
            "verification_results": verification_results,
            "reasoning_paths": reasoning_paths,
            "query": query,
        }

###############################################################################

# Synthesis Operator

###############################################################################

@jit
class ReasoningSynthesizer(Operator[Dict[str, Any], ReasoningOutput]):
"""Synthesizes a final answer from verified reasoning paths."""

    # Class-level field declaration
    synthesizer: JudgeSynthesisOperator

    def __init__(self, model_name: str = "anthropic:claude-3-opus"):
        """Initialize with configurable model."""
        # Create LM module for the synthesizer
        lm_module = LMModule(
            config=LMModuleConfig(
                model_name=model_name,
                temperature=0.2,
            )
        )

        # Use the core JudgeSynthesisOperator directly
        self.synthesizer = JudgeSynthesisOperator(lm_module=lm_module)

    def forward(self, *, inputs: Dict[str, Any]) -> ReasoningOutput:
        """Synthesize a final answer from multiple verified reasoning paths."""
        query = inputs.get("query", "")
        reasoning_paths = inputs.get("reasoning_paths", [])
        verification_results = inputs.get("verification_results", [])

        # Extract conclusions from reasoning paths
        conclusions = [path.conclusion for path in reasoning_paths]

        # Calculate verification scores
        verification_scores = []
        for result in verification_results:
            avg_score = (
                result.accuracy_score
                + result.coherence_score
                + result.completeness_score
            ) / 3.0
            verification_scores.append(avg_score)

        # Use the synthesizer to create the final answer
        synthesis_input = {
            "query": query,
            "conclusions": conclusions,
            "verification_scores": verification_scores,
        }

        synthesis_result = self.synthesizer(inputs=synthesis_input)

        # Calculate overall confidence as weighted average of verification scores
        # In a real implementation, this would be more sophisticated
        overall_confidence = (
            sum(verification_scores) / len(verification_scores)
            if verification_scores
            else 0.0
        )

        # Return structured output
        return ReasoningOutput(
            query=query,
            final_answer=synthesis_result.get("final_answer", "No answer generated"),
            confidence=overall_confidence,
            reasoning_paths=reasoning_paths,
            verification_results=verification_results,
        )

###############################################################################

# Complete Reasoning System

###############################################################################

@jit
class AdvancedReasoningSystem(Operator[ReasoningInput, ReasoningOutput]):
"""A sophisticated reasoning system with: 1. Parallel LLM reasoning with different models 2. Verification of reasoning steps 3. Synthesis of final response
"""

    # Class-level specification declaration
    specification: ClassVar[Specification] = ReasoningSpecification()

    # Class-level field declarations with types
    reasoning_ensemble: ReasoningEnsemble
    verifier: ReasoningVerifier
    synthesizer: ReasoningSynthesizer

    def __init__(self, num_reasoning_paths: int = 3):
        """Initialize the advanced reasoning system."""
        # Initialize declared fields
        self.reasoning_ensemble = ReasoningEnsemble(num_units=num_reasoning_paths)
        self.verifier = ReasoningVerifier()
        self.synthesizer = ReasoningSynthesizer()

    def forward(self, *, inputs: ReasoningInput) -> ReasoningOutput:
        """Process the query through the complete reasoning pipeline."""
        # Step 1: Get multiple reasoning paths (executed in parallel)
        ensemble_result = self.reasoning_ensemble(inputs=inputs)

        # Step 2: Verify each reasoning path (also executed in parallel)
        verification_result = self.verifier(inputs=ensemble_result)

        # Step 3: Synthesize a final response from verified reasoning
        final_result = self.synthesizer(inputs=verification_result)

        return final_result

###############################################################################

# Example Usage

###############################################################################

def main() -> None:
"""Demonstrates the advanced reasoning system."""
print("\n=== Advanced Reasoning System Example ===\n")

    # Create the reasoning system
    reasoning_system = AdvancedReasoningSystem(num_reasoning_paths=3)

    # Sample query
    query = "What are the potential economic implications of large-scale quantum computing adoption?"
    context = """
    Quantum computing leverages quantum mechanical phenomena to perform computations
    that would be infeasible on classical computers. Current estimates suggest that
    practical quantum computers could break widely-used cryptographic systems and
    enable significant advances in materials science, drug discovery, and optimization
    problems.
    """

    print(f"Query: {query}\n")
    print("Executing reasoning pipeline with maximum parallelization...\n")

    # Execute with parallelization enabled
    with execution_options(max_workers=4):
        result = reasoning_system(inputs={"query": query, "context": context})

    # Display results
    print(f"Final Answer: {result.final_answer}\n")
    print(f"Confidence: {result.confidence:.2f}\n")

    print(f"Number of reasoning paths: {len(result.reasoning_paths)}")
    print(f"Number of verification results: {len(result.verification_results)}\n")

    print("System Architecture:")
    print("1. ReasoningEnsemble: Generates multiple reasoning approaches in parallel")
    print(
        "2. ReasoningVerifier: Checks each reasoning path for accuracy, coherence, completeness"
    )
    print("3. ReasoningSynthesizer: Creates final response from verified reasoning\n")

    print("Key Benefits:")
    print("- Automatic parallelization of independent operations")
    print("- Structured, type-safe inputs and outputs")
    print("- Verification to reduce errors and hallucinations")
    print("- Synthesis to combine multiple perspectives\n")

if **name** == "**main**":
main()

</code>

src\ember\examples\advanced\test_auto_discovery.py:
<code>
"""Test the automatic model discovery functionality.

This script tests the automatic model discovery functionality in the model registry.
It's a simple test script that directly uses the registry and discovery service.

To run:
uv run python src/ember/examples/advanced/test_auto_discovery.py
"""

import logging
import os

# Import correctly from ember packages

from ember.core.registry.model.initialization import initialize_registry

# Configure logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

def test_auto_discovery():
"""Test the automatic model discovery functionality.""" # First, initialize the registry with auto_discover=False
logger.info("Initializing registry with auto_discover=False")
registry_without_discovery = initialize_registry(auto_discover=False)

    # Check if models were discovered
    model_ids_1 = registry_without_discovery.list_models()
    if model_ids_1:
        logger.info(
            f"Registry without auto-discovery found {len(model_ids_1)} models: {model_ids_1}"
        )
    else:
        logger.info("Registry without auto-discovery has no models as expected")

    # Next, initialize with force_discovery=True
    logger.info("Initializing registry with force_discovery=True")
    registry_with_force = initialize_registry(auto_discover=False, force_discovery=True)

    # Check if models were discovered
    model_ids_2 = registry_with_force.list_models()
    if model_ids_2:
        logger.info(
            f"Registry with force_discovery found {len(model_ids_2)} models: {model_ids_2}"
        )
    else:
        logger.warning("Registry with force_discovery did not find any models")

    # Finally, initialize with auto_discover=True
    logger.info("Initializing registry with auto_discover=True")
    registry_with_auto = initialize_registry(auto_discover=True)

    # Check if models were discovered
    model_ids_3 = registry_with_auto.list_models()
    if model_ids_3:
        logger.info(
            f"Registry with auto_discover found {len(model_ids_3)} models: {model_ids_3}"
        )
    else:
        logger.warning("Registry with auto_discover did not find any models")

    # Select the registry with the most models for detailed display
    if len(model_ids_3) >= len(model_ids_2) and len(model_ids_3) >= len(model_ids_1):
        best_registry = registry_with_auto
    elif len(model_ids_2) >= len(model_ids_1):
        best_registry = registry_with_force
    else:
        best_registry = registry_without_discovery

    # Print all models in the best registry
    logger.info("All models in best registry:")
    for model_id in best_registry.list_models():
        model_info = best_registry.get_model_info(model_id)
        if model_info:
            provider_name = (
                model_info.provider.name
                if hasattr(model_info.provider, "name")
                else "Unknown"
            )
            context_window = (
                model_info.context_window
                if hasattr(model_info, "context_window")
                else "N/A"
            )
            cost = model_info.cost if hasattr(model_info, "cost") else None

            if cost:
                i_cost = (
                    f"${cost.input_cost_per_thousand:.5f}/1K"
                    if hasattr(cost, "input_cost_per_thousand")
                    else "N/A"
                )
                o_cost = (
                    f"${cost.output_cost_per_thousand:.5f}/1K"
                    if hasattr(cost, "output_cost_per_thousand")
                    else "N/A"
                )
                cost_str = f"(In: {i_cost}, Out: {o_cost})"
            else:
                cost_str = "(Cost: N/A)"

            logger.info(
                f"  - {model_id}: Provider={provider_name}, Context={context_window} {cost_str}"
            )

    return best_registry

if **name** == "**main**":
logger.info("Testing automatic model discovery...")

    # Check which API keys are available
    api_keys = {
        "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY"),
        "ANTHROPIC_API_KEY": os.environ.get("ANTHROPIC_API_KEY"),
        "GOOGLE_API_KEY": os.environ.get("GOOGLE_API_KEY"),
    }

    for key, value in api_keys.items():
        if value:
            logger.info(f"{key} is set - length: {len(value[:3])}...")
        else:
            logger.warning(f"{key} is NOT set")

    # Run the test
    test_auto_discovery()

</code>

src\ember\examples\advanced\_\_init\_\_.py:
<code>

</code>

src\ember\examples\basic\check_env.py:
<code>
"""Environment Variable Check

This script checks if API keys are properly set in the environment variables.

To run:
uv run python src/ember/examples/basic/check_env.py
"""

import os

def main():
"""Print environment variables related to API keys."""
print("\n=== Environment Variables Check ===\n")

    # Check for OpenAI API key
    openai_key = os.environ.get("OPENAI_API_KEY")
    if openai_key:
        print(f"OPENAI_API_KEY: {'*' * (len(openai_key) - 4) + openai_key[-4:]}")
    else:
        print("OPENAI_API_KEY: Not set")

    # Check for Anthropic API key
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    if anthropic_key:
        print(
            f"ANTHROPIC_API_KEY: {'*' * (len(anthropic_key) - 4) + anthropic_key[-4:]}"
        )
    else:
        print("ANTHROPIC_API_KEY: Not set")

    print("\nAll environment variables:")
    for key, value in os.environ.items():
        # Only print sensitive values with asterisks
        if "KEY" in key or "SECRET" in key or "TOKEN" in key or "PASSWORD" in key:
            masked_value = "****" if value else "Not set"
            print(f"  {key}: {masked_value}")
        elif (
            "PATH" not in key
            and "SHELL" not in key
            and "EDITOR" not in key
            and "TERM" not in key
        ):
            # Skip common environment variables like PATH, SHELL, etc.
            print(f"  {key}: {value}")

if **name** == "**main**":
main()

</code>

src\ember\examples\basic\compact_notation_example.py:
<code>
"""
Compact NON Graph Notation Example

This example demonstrates Ember's compact graph notation for creating
complex operator graphs with minimal syntax, following the same design
principles favored by Jeff Dean and Sanjay Ghemawat: clean, precise,
orthogonal abstractions with maximal composability.
"""

from ember.api import non

# Create custom operator registry for demonstrations

def create_custom_registry():
"""Create a custom operator registry with extended operator types.""" # Create a new registry with standard operators
registry = non.OpRegistry.create_standard_registry()

    # Register a custom operator type
    registry.register(
        "CE",  # Custom Ensemble
        lambda count, model, temp: non.Sequential(
            operators=[
                non.UniformEnsemble(
                    num_units=count, model_name=model, temperature=temp
                ),
                non.MostCommon(),  # Automatically add MostCommon to every ensemble
            ]
        ),
    )

    return registry

def main() -> None:
"""Demonstrates various ways to build NON pipelines using compact notation."""

    # Example 1: Basic ensemble with judge
    print("\n==== Example 1: Basic Ensemble + Judge ====")

    # Using compact notation
    compact_pipeline = non.build_graph(
        [
            "3:E:gpt-4o:0.7",  # Ensemble with 3 GPT-4o instances at temp=0.7
            "1:J:claude-3-5-sonnet:0.0",  # Judge using Claude with temp=0
        ]
    )

    # Equivalent pipeline using standard API
    standard_pipeline = non.Sequential(
        operators=[
            non.UniformEnsemble(num_units=3, model_name="gpt-4o", temperature=0.7),
            non.JudgeSynthesis(model_name="claude-3-5-sonnet", temperature=0.0),
        ]
    )

    print("Compact notation pipeline created.")
    print("Both pipelines are functionally equivalent.")

    # Example 2: Complex verification pipeline
    print("\n==== Example 2: Complex Verification Pipeline ====")

    verification_pipeline = non.build_graph(
        [
            "3:E:gpt-4o:0.7",  # Generate 3 candidate answers
            "1:J:claude-3-5-sonnet:0.0",  # Synthesize into one answer
            "1:V:gpt-4o:0.0",  # Verify the synthesized answer
        ]
    )

    print("Verification pipeline created.")

    # Example 3: Nested pipeline structure
    print("\n==== Example 3: Nested Architecture ====")

    # Build a nested architecture similar to the SubNetwork/NestedNetwork example
    nested_pipeline = non.build_graph(
        [
            # First branch - GPT ensemble + verification
            ["3:E:gpt-4o:0.7", "1:V:gpt-4o:0.0"],
            # Second branch - Claude ensemble + verification
            ["3:E:claude-3-5-haiku:0.7", "1:V:claude-3-5-haiku:0.0"],
            # Final synthesis judge
            "1:J:claude-3-5-sonnet:0.0",
        ]
    )

    print("Nested architecture pipeline created.")

    # Example 4: Recursive References
    print("\n==== Example 4: Recursive Component References ====")

    # Define component map with nested references
    component_map = {
        # Basic building blocks
        "gpt_ensemble": "3:E:gpt-4o:0.7",
        "claude_ensemble": "3:E:claude-3-5-haiku:0.7",
        # Reference other components
        "verification_pipeline": ["$gpt_ensemble", "1:V:gpt-4o:0.0"],
        # Complex compositions referencing other references
        "double_verification": [
            "$verification_pipeline",  # First verification
            ["$claude_ensemble", "1:V:claude-3-5-haiku:0.0"],  # Second verification
        ],
    }

    # Create a complex graph with multiple levels of references
    reference_pipeline = non.build_graph(
        [
            "$double_verification",  # Two parallel verification branches
            "1:J:claude-3-5-sonnet:0.0",  # Final synthesis
        ],
        components=component_map,
    )

    print("Recursive reference pipeline created.")

    # Example 5: Custom Operator Types
    print("\n==== Example 5: Custom Operator Types ====")

    # Create a custom registry with extended operator types
    custom_registry = create_custom_registry()

    # Use the custom operator type in a specification with the custom registry
    custom_pipeline = non.build_graph(
        [
            "5:CE:gpt-4o:0.7",  # Custom ensemble with built-in MostCommon
            "1:J:claude-3-5-sonnet:0.0",  # Judge to synthesize
        ],
        type_registry=custom_registry,
    )

    print("Custom operator type pipeline created using custom type registry.")

    # Example 6: Exact Structural Match to NestedNetwork in example_architectures.py
    print("\n==== Example 6: NestedNetwork Equivalent ====")

    # Define the SubNetwork component exactly as in example_architectures.py
    # SubNetwork: ensemble → verifier pipeline (forward flow)
    subnetwork = [
        "2:E:gpt-4o:0.0",  # Ensemble with 2 identical models at temp=0
        "1:V:gpt-4o:0.0",  # Verify first response from ensemble
    ]

    # Create components map with the SubNetwork component
    component_map = {
        "sub": subnetwork,  # Reusable SubNetwork definition
    }

    # Build the NestedNetwork equivalent using the exact structure from example_architectures.py:
    # - Two parallel SubNetwork instances (identical configuration)
    # - Judge to synthesize results from both branches
    nested_network = non.build_graph(
        [
            # Two parallel branches, both using the same SubNetwork structure
            "$sub",  # First branch: SubNetwork instance
            "$sub",  # Second branch: SubNetwork instance
            "1:J:gpt-4o:0.0",  # Final Judge synthesizes results from both branches
        ],
        components=component_map,
    )

    print(
        "NestedNetwork equivalent created with identical structure to example_architectures.py"
    )
    # Example query to demonstrate usage
    print("\n==== Using the Pipeline ====")
    print("Example query: 'What causes the northern lights?'")
    print("(Not executing to avoid actual API calls)")
    print("Usage would be:")
    print("  result = pipeline(query='What causes the northern lights?')")
    print("  print(f'Answer: {result.answer}')")

if **name** == "**main**":
main()

</code>

src\ember\examples\basic\context_example.py:
<code>
"""Example of using the new context system.

This example demonstrates how to use the new context system with its core
components and threading features.
"""

import threading
import time
from typing import Dict, Any

from ember.core.context import Registry
from ember.core.context.config import ConfigComponent
from ember.core.context.model import ModelComponent
from ember.core.context.data import DataComponent
from ember.core.context.metrics import MetricsComponent
from ember.core.context.management import scoped_registry, temp_component

def basic_usage() -> None:
"""Demonstrate basic usage of the context system."""
print("\n=== Basic Component Usage ===")

    # Create configuration
    config = ConfigComponent(
        config_data={"models": {"simple_model": {"type": "mock", "temperature": 0.7}}}
    )

    # Create model component
    model = ModelComponent()

    # Create simple mock model
    class MockModel:
        def __init__(self, name: str):
            self.name = name

        def generate(self, prompt: str) -> str:
            return f"[{self.name}] Response to: {prompt}"

    # Register a model
    model.register_model("example_model", MockModel("Example"))

    # Use the model
    example_model = model.get_model("example_model")
    response = example_model.generate("Hello, world!")
    print(f"Model response: {response}")

    # Check configuration
    temp = config.get_value("models", "simple_model", {}).get("temperature")
    print(f"Model temperature: {temp}")

    # Access through registry
    registry = Registry.current()
    config_from_registry = registry.get("config")
    model_from_registry = registry.get("model")

    print(
        f"Found in registry: config={config_from_registry is config}, "
        f"model={model_from_registry is model}"
    )

def thread_isolation() -> None:
"""Demonstrate thread isolation with the context system."""
print("\n=== Thread Isolation ===")

    # Create shared metric component for tracking
    metrics = MetricsComponent()

    def worker_thread(thread_id: int) -> None:
        """Thread worker function."""
        # Each thread gets its own registry
        registry = Registry.current()

        # Create thread-local components
        config = ConfigComponent(config_data={"thread_info": {"id": thread_id}})
        model = ModelComponent()

        # Create mock model for this thread
        class ThreadModel:
            def generate(self, prompt: str) -> str:
                return f"Thread {thread_id}: {prompt}"

        # Register thread-specific model
        model.register_model("thread_model", ThreadModel())

        # Use the model
        thread_model = model.get_model("thread_model")
        response = thread_model.generate("Hello from thread")

        # Record metrics (shared across threads)
        with metrics.timed(f"thread_{thread_id}_duration"):
            time.sleep(0.1 * thread_id)  # Simulate work

        metrics.counter("threads_completed")

        # Print result with thread ID from config
        thread_id_from_config = config.get_value("thread_info", "id", thread_id)
        print(f"Thread {thread_id_from_config} response: {response}")

    # Create and start threads
    threads = []
    for i in range(3):
        thread = threading.Thread(target=worker_thread, args=(i,))
        threads.append(thread)
        thread.start()

    # Wait for all threads to complete
    for thread in threads:
        thread.join()

    # Show metrics
    all_metrics = metrics.get_metrics()
    print(f"Threads completed: {all_metrics['counters'].get('threads_completed', 0)}")
    print("Thread timings:")
    for key, value in all_metrics.get("histograms", {}).items():
        if key.startswith("thread_"):
            print(f"  {key}: {value.get('sum', 0):.1f}ms")

def scoped_registry_example() -> None:
"""Demonstrate using scoped registry for isolation."""
print("\n=== Scoped Registry ===")

    # Create components in main registry
    main_config = ConfigComponent(config_data={"config_info": {"scope": "main"}})

    # Create a scoped registry
    with scoped_registry() as registry:
        # Create components in scoped registry
        scoped_config = ConfigComponent(registry, config_data={"config_info": {"scope": "scoped"}})

        # Check values
        main_scope = main_config.get_value("config_info", "scope", "unknown")
        scoped_scope = scoped_config.get_value("config_info", "scope", "unknown")

        print(f"Inside scope - Main: {main_scope}, Scoped: {scoped_scope}")

    # After scope, only main registry exists
    current_reg = Registry.current()
    config = current_reg.get("config")
    scope = config.get_value("config_info", "scope", "unknown")
    print(f"After scope - Current: {scope}")

def temp_component_example() -> None:
"""Demonstrate using temporary components."""
print("\n=== Temporary Component ===")

    # Create model component
    model = ModelComponent()

    # Register original model
    class OriginalModel:
        def generate(self, prompt: str) -> str:
            return f"Original: {prompt}"

    model.register_model("test_model", OriginalModel())

    # Create temporary model
    class TemporaryModel:
        def generate(self, prompt: str) -> str:
            return f"Temporary: {prompt}"

    # Use temporary model
    with temp_component("model", TemporaryModel()) as temp_model:
        response = temp_model.generate("Hello")
        print(f"Within temp scope: {response}")

    # After temp component, original is restored
    registry = Registry.current()
    model_component = registry.get("model")
    model_instance = model_component.get_model("test_model")
    response = model_instance.generate("Hello again")
    print(f"After temp scope: {response}")

if **name** == "**main**": # Clear any existing registries
Registry.clear()

    # Run examples
    basic_usage()
    thread_isolation()
    scoped_registry_example()
    temp_component_example()

    print("\nAll examples completed successfully!")

</code>

src\ember\examples\basic\minimal_example.py:
<code>
"""Minimal Example

This module demonstrates the simplest possible Ember operator.

This example illustrates how the Ember `Operator` construct provides a minimal
foundation that can be used for language-model transformations or to wrap
arbitrary function logic.

To run:
poetry run python src/ember/examples/basic/minimal_example.py
"""

from typing import Any, Dict, List, Optional, Type

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel, Field

class MinimalInput(EmberModel):
"""Input model for MinimalOperator.

    Attributes:
        value: The integer value to be processed
        options: Optional configuration parameters
    """

    value: int = Field(description="The integer value to be processed")
    options: Optional[Dict[str, Any]] = Field(
        default=None, description="Optional configuration parameters"
    )

class MinimalOutput(EmberModel):
"""Output model for MinimalOperator.

    Attributes:
        value: The processed value
        steps: List of processing steps applied
    """

    value: int = Field(description="The processed value")
    steps: List[str] = Field(
        default_factory=list, description="List of processing steps applied"
    )

    def summarize(self) -> str:
        """Get a summary of the processing result.

        Returns:
            A formatted summary string
        """
        step_text = "\n".join(f"- {step}" for step in self.steps)
        return f"Result: {self.value}\nSteps applied:\n{step_text}"

class MinimalSpecification(Specification):
"""Specification for MinimalOperator.

    Defines the input/output contract that the operator fulfills.
    """

    input_model: Type[EmberModel] = MinimalInput
    structured_output: Type[EmberModel] = MinimalOutput

class MinimalOperator(Operator[MinimalInput, MinimalOutput]):
"""A minimal operator that performs configurable numeric operations.

    This operator demonstrates the clean patterns for creating Ember operators
    with proper typing, immutability, and clean interfaces.

    Attributes:
        specification: The operator's input/output contract
        increment: The value to add to the input
        multiplier: The value to multiply by after incrementing
    """

    # Class-level specification (immutable)
    specification: Specification = MinimalSpecification()

    # Instance attributes with type hints
    increment: int
    multiplier: int

    def __init__(self, *, increment: int = 1, multiplier: int = 1) -> None:
        """Initialize with transformation parameters.

        Args:
            increment: The value to add to the input
            multiplier: The value to multiply by after incrementing
        """
        self.increment = increment
        self.multiplier = multiplier

    def forward(self, *, inputs: MinimalInput) -> MinimalOutput:
        """Process the input value with configured transformations.

        Args:
            inputs: Validated input data containing the value to process

        Returns:
            Structured output containing the processed value and steps
        """
        # Track processing steps
        steps = []

        # Apply increment
        result = inputs.value + self.increment
        steps.append(f"Added {self.increment} to {inputs.value} = {result}")

        # Apply multiplier
        if self.multiplier != 1:
            prev = result
            result *= self.multiplier
            steps.append(f"Multiplied {prev} by {self.multiplier} = {result}")

        # Apply any custom options if provided
        if inputs.options:
            if "square" in inputs.options and inputs.options["square"]:
                prev = result
                result = result**2
                steps.append(f"Squared {prev} = {result}")

            if "add" in inputs.options:
                value = inputs.options["add"]
                prev = result
                result += value
                steps.append(f"Added {value} to {prev} = {result}")

        # Return structured output
        return MinimalOutput(value=result, steps=steps)

def main() -> None:
"""Run a simple demonstration of the MinimalOperator."""
print("\n=== Minimal Operator Example ===\n")

    # Create the operator
    op = MinimalOperator(increment=5, multiplier=2)

    # Create input with basic value
    basic_input = MinimalInput(value=10)

    # Process with only the basic configuration
    basic_result = op(inputs=basic_input)

    print("Basic Example:")
    print(f"  Input: {basic_input.value}")
    print(f"  Result: {basic_result.value}")
    print("  Steps:")
    for step in basic_result.steps:
        print(f"    {step}")

    # Create input with advanced options
    advanced_input = MinimalInput(value=7, options={"square": True, "add": 3})

    # Process with advanced options
    advanced_result = op(inputs=advanced_input)

    print("\nAdvanced Example:")
    print(f"  Input: {advanced_input.value} with options {advanced_input.options}")
    print(f"  Result: {advanced_result.value}")
    print("  Steps:")
    for step in advanced_result.steps:
        print(f"    {step}")

    # Alternative invocation patterns
    print("\nAlternative Invocation Patterns:")

    # Using dict input
    dict_result = op(inputs={"value": 3})
    print(f"  Dict input result: {dict_result.value}")

    # Using keyword arguments
    kwargs_result = op(value=4)
    print(f"  Keyword args result: {kwargs_result.value}")

    print("\nSummary using model method:")
    print(advanced_result.summarize())
    print()

if **name** == "**main**":
main()

</code>

src\ember\examples\basic\minimal_operator_example.py:
<code>
"""Minimal Operator Example

This script demonstrates the recommended pattern for creating and using Ember operators.
It shows how to create, configure, and execute operators with proper typing and error handling.

To run:
uv run python src/ember/examples/basic/minimal_operator_example.py
"""

# Import the minimal example

from ember.examples.basic.minimal_example import MinimalInput, MinimalOperator, main

if **name** == "**main**": # Run the example from the minimal_example module
main()

    # Additional demonstration of composition
    print("\n=== Operator Composition Example ===\n")

    # Create operators with different configurations
    op1 = MinimalOperator(increment=2, multiplier=3)
    op2 = MinimalOperator(increment=10, multiplier=1)

    # Create an input
    input_data = MinimalInput(value=5)

    # Process through the first operator
    intermediate = op1(inputs=input_data)
    print(f"After first operator: {intermediate.value}")

    # Process through the second operator
    final = op2(inputs=MinimalInput(value=intermediate.value))
    print(f"After second operator: {final.value}")

    # Show all processing steps
    print("\nAll processing steps:")
    for step in intermediate.steps:
        print(f"  {step}")
    for step in final.steps:
        print(f"  {step}")
    print()

</code>

src\ember\examples\basic\README.md:
<code>

# Basic Ember Examples

This directory contains basic examples demonstrating core Ember concepts with minimal complexity. These examples are ideal for beginners getting started with the framework.

## Examples

- `minimal_example.py` - The simplest possible Ember operator with proper typing and structure
- `check_env.py` - Verify your environment is set up correctly for Ember
- `minimal_operator_example.py` - Basic example of creating and using an operator
- `simple_jit_demo.py` - Introduction to using the JIT system

## Running Examples

To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/basic/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/basic/example_name.py
```

Replace `example_name.py` with the desired example file.

## Next Steps

After familiarizing yourself with these basic examples, explore the following directories:

- `models/` - For working with the model registry
- `operators/` - For more complex operator examples
- `data/` - For data handling examples

</code>

src\ember\examples\basic\simple_jit_demo.py:
<code>
"""Just-In-Time optimization with parallel execution demonstration.

This module demonstrates JIT compilation and parallel execution capabilities
provided by the Ember XCS engine. It showcases how computation graphs
are automatically traced, compiled, and optimized for different execution
strategies, with a focus on concurrent execution benefits.

Key concepts demonstrated:

1. JIT tracing and compilation of operator execution graphs
2. Performance comparison between sequential and parallel execution
3. Practical patterns for building concurrent operator workflows
4. Latency improvements through execution strategy optimization

Example usage:
uv run python -m ember.examples.basic.simple_jit_demo
"""

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type, Union

from ember.core.registry.operator.base.operator_base import Operator, Specification
from ember.core.types.ember_model import EmberModel, Field
from ember.xcs import (
ExecutionOptions,
TracerContext,
XCSGraph,
execute_graph,
execution_options,
jit,
)
from ember.xcs.engine.execution_options import get_execution_options

# Configure logging

logging.basicConfig(
level=logging.INFO,
format="%(asctime)s - %(levelname)s - %(message)s",
datefmt="%H:%M:%S",
)
logger = logging.getLogger(**name**)

# Input/Output model definitions

class DelayInput(EmberModel):
"""Input model for delay-based operators.

    Provides task identification for tracing execution paths through the
    computation graph. Used to correlate results back to their originating
    requests throughout the demonstration.

    Attributes:
        task_id: Unique identifier for correlating the task through execution
    """

    task_id: str = Field(description="Unique identifier for the task execution")

class DelayOutput(EmberModel):
"""Output model from delay-based operations.

    Captures operation results along with the original task identifier for
    correlation and tracing. Preserves the execution context through the
    computation graph.

    Attributes:
        result: The textual representation of the operation result
        task_id: The original task identifier passed through from the input
    """

    result: str = Field(description="Result of the operation processing")
    task_id: str = Field(description="Original task identifier for correlation")

class EnsembleOutput(EmberModel):
"""Output model for ensemble operations combining multiple results.

    Aggregates results from multiple child operations into a single collection
    while preserving the original task context for traceability.

    Attributes:
        results: Collection of result strings from child operations
        task_id: The original task identifier passed through from the input
    """

    results: List[str] = Field(description="Aggregated results from child operations")
    task_id: str = Field(description="Original task identifier for correlation")

# Operator specifications

class DelaySpecification(Specification):
"""Specification for delay-based operators.

    Defines the input/output contract for operations that simulate
    computational latency with predictable performance characteristics.

    Attributes:
        input_model: The expected input model type
        structured_output: The produced output model type
    """

    input_model: Type[EmberModel] = DelayInput
    structured_output: Type[EmberModel] = DelayOutput

class EnsembleSpecification(Specification):
"""Specification for ensemble operators.

    Defines the input/output contract for operations that aggregate
    results from multiple child operations into a consolidated output.

    Attributes:
        input_model: The expected input model type
        structured_output: The produced output model type
    """

    input_model: Type[EmberModel] = DelayInput
    structured_output: Type[EmberModel] = EnsembleOutput

class DelayOperator(Operator[DelayInput, DelayOutput]):
"""Simulates computational latency with a configurable delay.

    Used as a building block for demonstrating execution strategies
    with predictable performance characteristics. Each operator introduces
    a fixed delay to simulate a CPU-bound workload.

    Attributes:
        specification: The operator's input/output contract
        delay_seconds: The duration to sleep during execution
        op_id: Unique identifier for this operator instance
    """

    specification: ClassVar[Specification] = DelaySpecification()

    # Field declarations
    delay_seconds: float
    op_id: str

    def __init__(self, *, delay_seconds: float, op_id: str) -> None:
        """Initializes the delay operator with specified parameters.

        Args:
            delay_seconds: Duration to sleep during operation execution
            op_id: Unique identifier for this operator instance
        """
        self.delay_seconds = delay_seconds
        self.op_id = op_id

    def forward(self, *, inputs: DelayInput) -> DelayOutput:
        """Executes the operation with the configured delay.

        Simulates a computation that takes a predictable amount of time,
        creating a controlled environment for measuring execution strategies.

        Args:
            inputs: Task context information containing task identifier

        Returns:
            Operation result with correlation information
        """
        time.sleep(self.delay_seconds)
        return DelayOutput(
            result=f"Operator {self.op_id} completed task {inputs.task_id}",
            task_id=inputs.task_id,
        )

@jit
class JITEnsembleOperator(Operator[DelayInput, EnsembleOutput]):
"""JIT-optimized operator composing multiple delay operators sequentially.

    When executed, this operator creates a trace that can be compiled
    into an execution graph and optimized for different execution strategies.
    This implementation runs child operators sequentially but creates a
    data flow graph where each operation can be identified as independent.

    Attributes:
        specification: The operator's input/output contract
        operators: Collection of child delay operators to execute
    """

    specification: ClassVar[Specification] = EnsembleSpecification()

    # Field declarations
    operators: List[DelayOperator]

    def __init__(self, *, num_ops: int = 3, delay: float = 0.1) -> None:
        """Initializes the ensemble with a collection of delay operators.

        Creates a sequence of delay operators that will be executed
        according to the current execution context and tracing state.

        Args:
            num_ops: Number of child operators to create
            delay: Delay duration for each child operator in seconds
        """
        self.operators = [
            DelayOperator(delay_seconds=delay, op_id=f"Op-{i+1}")
            for i in range(num_ops)
        ]

    def forward(self, *, inputs: DelayInput) -> EnsembleOutput:
        """Executes all child operators sequentially and collects results.

        This implementation creates a natural data flow graph where
        each operation is independent but executed in sequence,
        making it an ideal candidate for parallel optimization.

        Args:
            inputs: Task context information containing task identifier

        Returns:
            Aggregated results from all child operations
        """
        results = []
        for i, op in enumerate(self.operators):
            # Creating a unique task ID for each child operation
            task_input = DelayInput(task_id=f"{inputs.task_id}-{i+1}")
            output = op(inputs=task_input)
            results.append(output.result)

        return EnsembleOutput(results=results, task_id=inputs.task_id)

@jit
class ParallelEnsembleOperator(Operator[DelayInput, EnsembleOutput]):
"""JIT-optimized operator explicitly using parallel execution.

    Demonstrates how operators can internally leverage concurrency
    while still benefiting from JIT tracing and optimization.
    This implementation explicitly uses thread-based parallelism
    to execute child operators concurrently.

    Attributes:
        specification: The operator's input/output contract
        operators: Collection of child delay operators to execute
        num_ops: Maximum number of concurrent operations
    """

    specification: ClassVar[Specification] = EnsembleSpecification()

    # Field declarations
    operators: List[DelayOperator]
    num_ops: int

    def __init__(self, *, num_ops: int = 3, delay: float = 0.1) -> None:
        """Initializes the ensemble with a collection of delay operators.

        Args:
            num_ops: Number of child operators to create and maximum concurrency
            delay: Delay duration for each child operator in seconds
        """
        self.operators = [
            DelayOperator(delay_seconds=delay, op_id=f"Op-{i+1}")
            for i in range(num_ops)
        ]
        self.num_ops = num_ops

    def forward(self, *, inputs: DelayInput) -> EnsembleOutput:
        """Executes all child operators concurrently and collects results.

        Uses explicit threading to achieve parallelism, demonstrating
        how operators can internally optimize execution while still
        participating in the JIT tracing ecosystem.

        Args:
            inputs: Task context information containing task identifier

        Returns:
            Aggregated results from all child operations
        """
        results = []

        with ThreadPoolExecutor(max_workers=self.num_ops) as executor:
            # Submitting all tasks to the executor
            futures = []
            for i, op in enumerate(self.operators):
                task_input = DelayInput(task_id=f"{inputs.task_id}-{i+1}")
                futures.append(executor.submit(op, inputs=task_input))

            # Collecting results as they complete
            for future in as_completed(futures):
                output = future.result()
                results.append(output.result)

        return EnsembleOutput(results=results, task_id=inputs.task_id)

def run_benchmark(
\*,
name: str,
operator: Operator,
num_runs: int = 2,
task_prefix: str = "run",
) -> Tuple[List[float], List[str]]:
"""Executes a benchmark of the provided operator.

    Runs the given operator multiple times and collects performance metrics
    and results. The first run typically includes JIT tracing overhead,
    while subsequent runs benefit from the cached execution plan.

    Args:
        name: Descriptive name for the benchmark
        operator: Operator instance to benchmark
        num_runs: Number of consecutive executions to measure
        task_prefix: Prefix for task identifiers

    Returns:
        Tuple containing (list of execution times, list of result strings)
    """
    times = []
    all_results = []

    logger.info("\n%s benchmark:", name)
    for i in range(num_runs):
        run_label = f"Run {i+1}" if i > 0 else "First run (with tracing)"
        logger.info("  %s...", run_label)

        start = time.time()
        result = operator(inputs=DelayInput(task_id=f"{task_prefix}-{i+1}"))
        elapsed = time.time() - start

        times.append(elapsed)
        all_results.extend(result.results)

        logger.info(
            "    Completed %d operations in %.4fs", len(result.results), elapsed
        )

    return times, all_results

def analyze_performance(
\*,
name: str,
times: List[float],
baseline_time: Optional[float] = None,
theoretical_time: Optional[float] = None,
) -> None:
"""Analyzes and reports performance metrics.

    Calculates and logs various performance metrics comparing the actual
    execution times against theoretical optimums and baselines.

    Args:
        name: Name of the execution strategy being analyzed
        times: List of execution times to analyze (first time typically includes tracing)
        baseline_time: Optional baseline time for comparison
        theoretical_time: Optional theoretical optimal time
    """
    if not times:
        return

    first_time = times[0]

    if len(times) > 1:
        subsequent_avg = sum(times[1:]) / len(times[1:])
        speedup = first_time / subsequent_avg if subsequent_avg > 0 else 0

        logger.info("\n%s Analysis:", name)
        logger.info("  First run:            %.4fs", first_time)
        logger.info("  Subsequent runs avg:  %.4fs", subsequent_avg)

        if speedup > 1.05:  # Only reporting meaningful speedups
            improvement = (first_time - subsequent_avg) / first_time
            logger.info(
                "  JIT caching benefit:  %.1f%% faster after tracing", improvement * 100
            )

    if baseline_time is not None and baseline_time > 0:
        best_time = min(times)
        speedup = baseline_time / best_time if best_time > 0 else 0

        if speedup > 1.05:  # Only reporting meaningful speedups
            improvement = (baseline_time - best_time) / baseline_time
            logger.info(
                "  Speedup vs baseline: %.1f%% faster (%.1fx)",
                improvement * 100,
                speedup,
            )

    if theoretical_time is not None and theoretical_time > 0:
        best_time = min(times[1:]) if len(times) > 1 else first_time
        accuracy = best_time / theoretical_time

        logger.info("  Theoretical optimum: %.4fs", theoretical_time)
        logger.info(
            "  Achieved efficiency: %.1f%% of theoretical optimum", (1 / accuracy) * 100
        )

def demo_sequential_vs_jit(\*, num_ops: int, delay: float) -> None:
"""Demonstrates basic JIT functionality without parallelism.

    Compares sequential execution time to JIT execution,
    showing the caching and optimization benefits of JIT
    even without explicit parallelism. Shows how the first run
    with tracing differs from subsequent cached runs.

    Args:
        num_ops: Number of operators to use in the ensemble
        delay: Delay duration for each operator in seconds
    """
    logger.info("\n%s", "=" * 80)
    logger.info(
        "DEMO 1: Sequential vs JIT - %d operators, %.3fs delay each", num_ops, delay
    )
    logger.info("%s", "=" * 80)
    logger.info(
        "Demonstrating JIT caching and optimization benefits without parallelism."
    )

    # Creating the JIT operator
    jit_op = JITEnsembleOperator(num_ops=num_ops, delay=delay)

    # Running benchmark
    times, _ = run_benchmark(
        name="Sequential JIT Ensemble",
        operator=jit_op,
        num_runs=3,
        task_prefix="sequential",
    )

    # Calculating theoretical sequential time
    theoretical_time = num_ops * delay

    # Analyzing performance
    analyze_performance(
        name="Sequential JIT",
        times=times,
        theoretical_time=theoretical_time,
    )

def demo_explicit_parallel_execution(\*, num_ops: int, delay: float) -> None:
"""Demonstrates JIT with explicit graph-based parallel execution.

    Shows how the XCS engine can transform a sequential execution
    graph into a parallel execution plan automatically. Compares
    the performance of sequential and parallel schedulers on
    the same computation graph.

    Args:
        num_ops: Number of operators to use in the ensemble
        delay: Delay duration for each operator in seconds
    """
    logger.info("\n%s", "=" * 80)
    logger.info(
        "DEMO 2: Sequential vs Parallel Execution - %d operators, %.3fs delay each",
        num_ops,
        delay,
    )
    logger.info("%s", "=" * 80)
    logger.info(
        "Demonstrating how JIT + parallel scheduling transforms execution graphs."
    )

    # Creating the ensemble operator
    ensemble = JITEnsembleOperator(num_ops=num_ops, delay=delay)

    # First tracing the execution to build a graph
    with TracerContext() as tracer:
        # Executing the operator to capture the trace
        _ = ensemble(inputs=DelayInput(task_id="trace-run"))

    logger.info("\nCaptured %d trace records", len(tracer.records))

    # Building a graph directly from operators
    logger.info("\nBuilding execution graph...")
    graph = XCSGraph()

    # Adding each operator as a separate node
    for i, op in enumerate(ensemble.operators):
        node_id = f"node_{i}"
        graph.add_node(
            operator=lambda inputs, op=op: op(
                inputs=DelayInput(task_id=inputs["task_id"])
            ),
            node_id=node_id,
        )

    # No need to explicitly compile the graph with the unified engine
    # execute_graph handles graph compilation internally

    # Executing with different schedulers
    logger.info("\nExecuting with different schedulers:")

    # Sequential execution
    logger.info("  Sequential scheduler...")
    start = time.time()
    _ = execute_graph(
        graph=graph,
        inputs={"task_id": "sequential"},
        options=ExecutionOptions(scheduler="wave"),
    )
    seq_time = time.time() - start
    logger.info("    Completed in %.4fs", seq_time)

    # Parallel execution
    logger.info("  Parallel scheduler...")
    start = time.time()
    _ = execute_graph(
        graph=graph,
        inputs={"task_id": "parallel"},
        options=ExecutionOptions(scheduler="parallel", max_workers=num_ops),
    )
    par_time = time.time() - start
    logger.info("    Completed in %.4fs", par_time)

    # Analysis
    theoretical_sequential = num_ops * delay
    theoretical_parallel = delay  # Theoretical: all operations run in parallel

    logger.info("\nExecution strategy comparison:")
    logger.info("  Sequential execution:    %.4fs", seq_time)
    logger.info("  Parallel execution:      %.4fs", par_time)
    logger.info("  Theoretical sequential:  %.4fs", theoretical_sequential)
    logger.info("  Theoretical parallel:    %.4fs", theoretical_parallel)

    if par_time < seq_time:
        speedup = seq_time / par_time
        improvement = (seq_time - par_time) / seq_time
        logger.info(
            "\n✅ Parallel scheduler achieved %.1f%% improvement (%.1fx)",
            improvement * 100,
            speedup,
        )

    # Efficiency analysis
    seq_efficiency = theoretical_sequential / seq_time if seq_time > 0 else 0
    par_efficiency = theoretical_parallel / par_time if par_time > 0 else 0

    logger.info("\nEfficiency analysis:")
    logger.info(
        "  Sequential scheduler: %.1f%% of theoretical optimum", seq_efficiency * 100
    )
    logger.info(
        "  Parallel scheduler:   %.1f%% of theoretical optimum", par_efficiency * 100
    )

def demo_execution_strategies(\*, num_ops: int, delay: float) -> None:
"""Demonstrates different operator execution strategies.

    Compares sequential, JIT-optimized, and explicitly parallel
    implementations to highlight performance characteristics.
    Also showcases a context-aware operator that can adapt its
    execution strategy based on runtime configuration.

    Args:
        num_ops: Number of operators to use in the ensemble
        delay: Delay duration for each operator in seconds
    """
    logger.info("\n%s", "=" * 80)
    logger.info(
        "DEMO 3: Execution Strategy Comparison - %d operators, %.3fs delay each",
        num_ops,
        delay,
    )
    logger.info("%s", "=" * 80)
    logger.info(
        "Comparing different execution strategies with the same logical operations."
    )

    # Creating operators with different execution strategies
    sequential_op = JITEnsembleOperator(num_ops=num_ops, delay=delay)
    parallel_op = ParallelEnsembleOperator(num_ops=num_ops, delay=delay)

    # Creating a context-dependent operator that adapts based on execution options
    @jit
    class ContextAwareOperator(Operator[DelayInput, EnsembleOutput]):
        """Operator that adapts execution strategy based on context.

        Demonstrates how operators can dynamically select their execution
        strategy based on runtime context, allowing the same logical
        operation to be optimized for different execution environments.

        Attributes:
            specification: The operator's input/output contract
            operators: Collection of child delay operators to execute
            num_ops: Maximum number of concurrent operations
        """

        specification: ClassVar[Specification] = EnsembleSpecification()

        # Field declarations
        operators: List[DelayOperator]
        num_ops: int

        def __init__(self, *, num_ops: int = 3, delay: float = 0.1) -> None:
            """Initializes with configurable operators.

            Args:
                num_ops: Number of child operators to create and maximum concurrency
                delay: Delay duration for each child operator in seconds
            """
            self.operators = [
                DelayOperator(delay_seconds=delay, op_id=f"Op-{i+1}")
                for i in range(num_ops)
            ]
            self.num_ops = num_ops

        def forward(
            self, *, inputs: Union[DelayInput, Dict[str, Any]]
        ) -> EnsembleOutput:
            """Executes with strategy based on context.

            Checks the execution context for configuration parameters
            that indicate whether to use parallel execution, and
            dynamically selects the appropriate strategy.

            Args:
                inputs: Task context information containing task identifier,
                       either as DelayInput object or dictionary

            Returns:
                Aggregated results from all child operations
            """
            results = []

            # Extract task_id from inputs, handling both object and dict formats
            if isinstance(inputs, dict) and "task_id" in inputs:
                task_id = inputs["task_id"]
            elif hasattr(inputs, "task_id"):
                task_id = inputs.task_id
            else:
                task_id = "unknown-task"

            # Checking execution context for parallelism hints
            current_options = get_execution_options()
            use_parallel = current_options.use_parallel

            if use_parallel:
                # Parallel execution strategy using threading
                with ThreadPoolExecutor(max_workers=self.num_ops) as executor:
                    futures = []
                    for i, op in enumerate(self.operators):
                        task_input = DelayInput(task_id=f"{task_id}-{i+1}")
                        futures.append(executor.submit(op, inputs=task_input))

                    for future in as_completed(futures):
                        output = future.result()
                        results.append(output.result)
            else:
                # Sequential execution strategy
                for i, op in enumerate(self.operators):
                    task_input = DelayInput(task_id=f"{task_id}-{i+1}")
                    output = op(inputs=task_input)
                    results.append(output.result)

            return EnsembleOutput(results=results, task_id=task_id)

    # Creating the context-aware operator
    adaptive_op = ContextAwareOperator(num_ops=num_ops, delay=delay)

    # Benchmarking sequential operator
    seq_times, _ = run_benchmark(
        name="Sequential Operator",
        operator=sequential_op,
        num_runs=2,
        task_prefix="seq",
    )

    # Benchmarking parallel operator
    par_times, _ = run_benchmark(
        name="Parallel Operator",
        operator=parallel_op,
        num_runs=2,
        task_prefix="par",
    )

    # Benchmarking adaptive operator with sequential context
    logger.info("\nAdaptive Operator (Sequential Context):")
    start = time.time()
    # Use execution options context manager for this execution
    with execution_options(use_parallel=False):
        _ = adaptive_op(inputs=DelayInput(task_id="adaptive-seq"))
    adaptive_seq_time = time.time() - start
    logger.info("  Completed in %.4fs", adaptive_seq_time)

    # Benchmarking adaptive operator with parallel context
    logger.info("\nAdaptive Operator (Parallel Context):")
    start = time.time()
    # Use execution options context manager for this execution
    with execution_options(use_parallel=True):
        _ = adaptive_op(inputs=DelayInput(task_id="adaptive-par"))
    adaptive_par_time = time.time() - start
    logger.info("  Completed in %.4fs", adaptive_par_time)

    # Performance analysis
    seq_best = min(seq_times)
    par_best = min(par_times)
    theoretical = delay  # Optimal parallel execution time

    logger.info("\nExecution Strategy Comparison:")
    logger.info("  Sequential best:       %.4fs", seq_best)
    logger.info("  Parallel best:         %.4fs", par_best)
    logger.info("  Adaptive (sequential): %.4fs", adaptive_seq_time)
    logger.info("  Adaptive (parallel):   %.4fs", adaptive_par_time)
    logger.info("  Theoretical optimum:   %.4fs", theoretical)

    # Calculating and reporting speedups
    if seq_best > 0:
        best_overall = min(par_best, adaptive_par_time)
        overall_speedup = seq_best / best_overall if best_overall > 0 else 0

        if overall_speedup > 1.05:
            logger.info(
                "\n✅ Parallelism achieved %.1fx speedup vs sequential", overall_speedup
            )

    # Context adaptability analysis
    if adaptive_seq_time > 0 and adaptive_par_time > 0:
        context_adaptability = adaptive_seq_time / adaptive_par_time

        if context_adaptability > 1.05:
            logger.info(
                "\n✅ Context-aware execution demonstrated %.1fx speedup through adaptability",
                context_adaptability,
            )

def main() -> None:
"""Executes the JIT optimization demonstration suite.

    Runs a series of demonstrations showcasing different aspects
    of JIT optimization and parallel execution strategies. Each
    demonstration highlights specific capabilities of the XCS
    engine and JIT tracing system.
    """
    logger.info("Just-In-Time Optimization Demonstration")
    logger.info("=======================================")
    logger.info("\nThis demonstration shows how JIT tracing and compilation")
    logger.info("enables significant performance optimizations through:")
    logger.info(" - Execution graph analysis and caching")
    logger.info(" - Automatic parallelization of independent operations")
    logger.info(" - Context-aware execution strategy selection")

    # Configuration parameters
    num_ops = 10
    delay = 0.1

    # Demo 1: Basic JIT functionality
    demo_sequential_vs_jit(num_ops=num_ops, delay=delay)

    # Demo 2: Manual parallel execution
    demo_explicit_parallel_execution(num_ops=num_ops, delay=delay)

    # Demo 3: Different execution strategies
    demo_execution_strategies(num_ops=num_ops, delay=delay)

    logger.info("\n%s", "=" * 80)
    logger.info("KEY INSIGHTS:")
    logger.info(
        "1. JIT tracing converts imperative code into optimizable computation graphs"
    )
    logger.info(
        "2. Independent operations are automatically identified for parallelization"
    )
    logger.info("3. Execution strategies can be selected based on runtime context")
    logger.info(
        "4. First-run tracing overhead is amortized through cached execution plans"
    )
    logger.info(
        "5. Context-aware operators can dynamically adapt to execution environments"
    )

    logger.info("\nPRACTICAL APPLICATIONS:")
    logger.info(
        "- Complex ML inference pipelines with many independent processing steps"
    )
    logger.info(
        "- Data transformation workflows with embarrassingly parallel operations"
    )
    logger.info("- Ensemble methods combining multiple model predictions")
    logger.info(
        "- Systems that need to adapt to different hardware capabilities at runtime"
    )

if **name** == "**main**":
main()

</code>

src\ember\examples\basic\_\_init\_\_.py:
<code>

</code>

src\ember\examples\data\context_example.py:
<code>
"""Example demonstrating the use of DataContext.

This example shows how to create and use a DataContext with custom configuration
for efficient and thread-safe management of dataset operations.
"""

import logging
from typing import Any, Dict, List

from ember.api.data import DataAPI
from ember.core.utils.data.base.models import TaskType
from ember.core.utils.data.context.data_context import (
DataConfig,
DataContext,
get_default_context,
set_default_context,
)

def setup_logging():
"""Configure logging for the example."""
logging.basicConfig(
level=logging.INFO,
format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)

def create_custom_context() -> DataContext:
"""Create a custom DataContext with specific configuration.

    This demonstrates how to create a DataContext with custom settings
    for memory usage and caching behavior.

    Returns:
        Custom DataContext
    """
    # Create custom configuration
    config = DataConfig(
        cache_dir="/tmp/ember_cache",  # Use local cache directory
        batch_size=64,  # Process data in larger batches
        cache_ttl=7200,  # Cache data for 2 hours
    )

    # Create context with auto-discovery
    context = DataContext(config=config, auto_discover=True)

    return context

def register_custom_dataset(context: DataContext) -> None:
"""Register a custom dataset with the context.

    This demonstrates how to register a custom dataset with metadata
    without needing to modify global state.

    Args:
        context: DataContext to register with
    """
    # Register a simple custom dataset
    context.register_dataset(
        name="example_dataset",
        source="example/source",
        task_type=TaskType.MULTIPLE_CHOICE,
        description="Example dataset for DataContext demo",
    )

    # Log available datasets
    logging.info("Available datasets: %s", context.registry.list_datasets())

def process_data_with_context(context: DataContext) -> List[Dict[str, Any]]:
"""Process data using the DataContext.

    This demonstrates how to use the DataContext for data operations
    with explicit dependency management.

    Args:
        context: DataContext for dataset operations

    Returns:
        List of processed data items
    """
    # Create API with explicit context
    api = DataAPI(context=context)

    # Create a custom data processing pipeline
    pipeline = (
        api.builder()
        .from_registry("mmlu")  # Use a standard dataset
        .split("validation")  # Select validation split
        .sample(10)  # Get 10 samples
        .transform(
            lambda x: {  # Transform data format
                "question": f"Question: {x.get('question', '')}",
                "options": x.get("choices", {}),
                "answer": x.get("answer", None),
            }
        )
    )

    # Check if custom dataset is available
    available_datasets = api.list()
    if "example_dataset" in available_datasets:
        # Also process custom dataset
        custom_pipeline = api.builder().from_registry("example_dataset").limit(5)

        # Collect data
        custom_data = list(custom_pipeline.build())
        logging.info("Processed %d items from custom dataset", len(custom_data))

    # Collect data from main pipeline
    results = list(pipeline.build())
    logging.info("Processed %d items", len(results))

    # Return processed items
    return [
        {
            "id": i,
            "question": item.question if hasattr(item, "question") else "",
            "options": item.options if hasattr(item, "options") else {},
        }
        for i, item in enumerate(results)
    ]

def demonstrate_thread_safety():
"""Demonstrate that DataContext is thread-safe.

    This shows how the DataContext can be used safely in a
    multi-threaded environment without race conditions.
    """
    import threading

    # Create shared context
    shared_context = create_custom_context()

    # Set as default for all threads
    set_default_context(shared_context)

    # Thread function that uses the default context
    def thread_function(thread_id: int):
        # Get default context (should be our shared context)
        context = get_default_context()

        # Log available datasets
        datasets = context.registry.list_datasets()
        logging.info("Thread %d sees datasets: %s", thread_id, datasets)

        # Process some data
        results = process_data_with_context(context)
        logging.info("Thread %d processed %d items", thread_id, len(results))

    # Create and start threads
    threads = []
    for i in range(3):
        thread = threading.Thread(target=thread_function, args=(i,))
        threads.append(thread)
        thread.start()

    # Wait for all threads to finish
    for thread in threads:
        thread.join()

    logging.info("All threads completed successfully")

def run_example():
"""Run the DataContext example.""" # Set up logging
setup_logging()

    # Create custom context
    context = create_custom_context()

    # Register custom dataset
    register_custom_dataset(context)

    # Process data with context
    results = process_data_with_context(context)

    # Show results
    for item in results[:3]:  # Show first 3 items
        logging.info("Processed item: %s", item)

    # Demonstrate thread safety
    demonstrate_thread_safety()

if **name** == "**main**":
run_example()

</code>

src\ember\examples\data\data_api_example.py:
<code>
"""Data API Example.

A demonstration of Ember's data system using the MMLU dataset.
Shows how to load, transform, and analyze dataset entries
with proper configuration and error handling.
"""

import logging
from typing import Dict, Any, List, Optional

from ember.core.utils.data.context.data_context import DataContext
from ember.core.utils.data import load_dataset_entries
from ember.core.utils.data.base.models import DatasetEntry
from ember.core.utils.data.datasets_registry.mmlu import MMLUConfig

def load_mmlu_questions(subject: str = "high_school_mathematics",
split: str = "test",
num_samples: int = 5) -> List[DatasetEntry]:
"""Load questions from MMLU dataset with proper configuration.

    Args:
        subject: Subject area to load
        split: Dataset split to use (test, validation, etc.)
        num_samples: Maximum number of questions to load

    Returns:
        List of dataset entries

    Raises:
        RuntimeError: If dataset loading fails
    """
    # Create data context
    context = DataContext(auto_discover=True)

    try:
        # MMLU requires specific config_name and split parameters
        config = MMLUConfig(config_name=subject, split=split)

        entries = load_dataset_entries(
            dataset_name="mmlu",
            config=config,
            num_samples=num_samples,
            context=context
        )
        return entries
    except Exception as e:
        logging.error(f"Failed to load MMLU dataset: {e}")
        raise RuntimeError(f"Dataset loading failed: {e}") from e

def print_question_details(entries: List[DatasetEntry]) -> None:
"""Display formatted questions with answers and options.

    Args:
        entries: List of dataset entries to display
    """
    for i, entry in enumerate(entries, 1):
        print(f"\nQuestion {i}: {entry.query}")

        if hasattr(entry, "choices") and entry.choices:
            print("\nOptions:")
            for key, text in sorted(entry.choices.items()):
                print(f"  {key}) {text}")

        if hasattr(entry, "metadata") and entry.metadata.get("correct_answer"):
            print(f"\nAnswer: {entry.metadata['correct_answer']}")

        print("-" * 80)

def transform_to_prompt_format(entries: List[DatasetEntry]) -> List[Dict[str, Any]]:
"""Transform dataset entries to LLM-ready prompt format.

    Args:
        entries: Dataset entries to transform

    Returns:
        List of formatted prompt dictionaries
    """
    transformed = []

    for entry in entries:
        # Format choices as a string
        options_text = ""
        if hasattr(entry, "choices") and entry.choices:
            options = []
            for key, text in sorted(entry.choices.items()):
                options.append(f"{key}. {text}")
            options_text = "\n".join(options)

        # Build formatted prompt with metadata
        prompt = {
            "formatted_question": f"Question: {entry.query}\n\n{options_text}",
            "answer": entry.metadata.get("correct_answer", "") if hasattr(entry, "metadata") else "",
            "subject": entry.metadata.get("subject", "") if hasattr(entry, "metadata") else "",
        }

        transformed.append(prompt)

    return transformed

def analyze_dataset(entries: List[DatasetEntry]) -> Dict[str, Any]:
"""Analyze dataset composition and structure.

    Args:
        entries: Dataset entries to analyze

    Returns:
        Dictionary of dataset statistics
    """
    stats = {
        "total_entries": len(entries),
        "avg_choices": 0,
        "choices_distribution": {},
        "query_length": {"min": float('inf'), "max": 0, "avg": 0},
    }

    total_choices = 0
    total_query_length = 0

    for entry in entries:
        # Count choices
        if hasattr(entry, "choices"):
            num_choices = len(entry.choices)
            total_choices += num_choices

            if num_choices not in stats["choices_distribution"]:
                stats["choices_distribution"][num_choices] = 0
            stats["choices_distribution"][num_choices] += 1

        # Measure query length
        query_length = len(entry.query)
        total_query_length += query_length
        stats["query_length"]["min"] = min(stats["query_length"]["min"], query_length)
        stats["query_length"]["max"] = max(stats["query_length"]["max"], query_length)

    # Calculate averages
    if stats["total_entries"] > 0:
        stats["avg_choices"] = total_choices / stats["total_entries"]
        stats["query_length"]["avg"] = total_query_length / stats["total_entries"]

    return stats

def main() -> None:
"""Run the MMLU data example.""" # Configure structured logging
logging.basicConfig(
level=logging.INFO,
format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

    print("EMBER DATA API EXAMPLE")
    print("=====================")

    try:
        # Load MMLU datasets from different subjects
        math_entries = load_mmlu_questions(
            subject="high_school_mathematics",
            num_samples=3
        )
        print(f"Loaded {len(math_entries)} MMLU math questions")

        biology_entries = load_mmlu_questions(
            subject="high_school_biology",
            num_samples=2
        )
        print(f"Loaded {len(biology_entries)} MMLU biology questions")

        # Combine entries
        entries = math_entries + biology_entries
        print(f"Total: {len(entries)} questions from multiple subjects\n")

        # Display formatted questions
        print_question_details(entries)

        # Transform to prompt format
        formatted = transform_to_prompt_format(entries)

        # Display an example of transformed format
        print("\nTRANSFORMED PROMPT FORMAT (EXAMPLE)")
        print("=================================")
        if formatted:
            print(formatted[0]["formatted_question"])
            print(f"\nExpected answer: {formatted[0]['answer']}")

        # Analyze dataset composition
        stats = analyze_dataset(entries)
        print("\nDATASET ANALYSIS")
        print("===============")
        print(f"Total entries: {stats['total_entries']}")
        print(f"Average choices per question: {stats['avg_choices']:.1f}")
        print(f"Question length: min={stats['query_length']['min']}, " +
              f"max={stats['query_length']['max']}, " +
              f"avg={stats['query_length']['avg']:.1f} characters")

    except Exception as e:
        logging.error(f"Example failed: {e}")
        print(f"\nError: {e}")
        print("\nPlease ensure you have the necessary dataset access and environment setup.")

if **name** == "**main**":
main()
</code>

src\ember\examples\data\enhanced_builder_example.py:
<code>
"""Example demonstrating the enhanced DatasetBuilder pattern in Ember.

This example shows how to use the DatasetBuilder to load, transform, and work with
datasets using a fluent interface. It demonstrates the optimized performance features:

1. Thread-safe DataContext for concurrent access
2. Memory-efficient streaming datasets
3. Optimized lookup patterns for high-performance execution

To run:
uv run python src/ember/examples/data/enhanced_builder_example.py
"""

import time
from typing import Any, Dict

from ember.api.data import DatasetBuilder, data, list_available_datasets
from ember.core.utils.data.context.data_context import get_default_context

def uppercase_transformer(item: Dict[str, Any]) -> Dict[str, Any]:
"""Transform dataset items by uppercasing text fields.

    Args:
        item: Dataset item to transform

    Returns:
        Transformed item with uppercase text fields
    """
    result = item.copy()

    # Convert question to uppercase if present
    if "question" in result:
        result["question"] = result["question"].upper()

    # Convert choices to uppercase if present
    if "choices" in result:
        if isinstance(result["choices"], dict):
            # Handle dictionary-style choices
            result["choices"] = {
                k: v.upper() if isinstance(v, str) else v
                for k, v in result["choices"].items()
            }
        elif isinstance(result["choices"], list):
            # Handle list-style choices (MMLU format)
            result["choices"] = [
                c.upper() if isinstance(c, str) else c for c in result["choices"]
            ]

    return result

def prompt_formatter(item: Dict[str, Any]) -> Dict[str, Any]:
"""Format dataset items into LLM-ready prompts.

    Args:
        item: Dataset item to format

    Returns:
        Item with added formatted prompt
    """
    result = item.copy()

    if "question" in result and "choices" in result:
        choices_text = ""

        if isinstance(result["choices"], dict):
            # Handle dictionary-style choices
            for key, value in result["choices"].items():
                choices_text += f"{key}. {value}\n"
        elif isinstance(result["choices"], list):
            # Handle list-style choices (MMLU format)
            options = ["A", "B", "C", "D"]
            for i, choice in enumerate(result["choices"]):
                if i < len(options):
                    choices_text += f"{options[i]}. {choice}\n"

        result["formatted_prompt"] = (
            f"Question: {result['question']}\n\n"
            f"Answer choices:\n{choices_text}\n"
            f"Please select the correct answer choice."
        )

    return result

def demonstrate_performance() -> None:
"""Demonstrates the optimized performance features."""
print("\nDemonstrating performance optimizations:")
print("======================================")

    # Get or create the thread-local DataContext
    context = get_default_context()
    print(f"Using thread-local DataContext at {id(context)}")

    # Measure DataContext lookup performance
    iterations = 10000
    start = time.time()
    for _ in range(iterations):
        # This should be extremely fast due to our optimizations
        ctx = get_default_context()
    duration = time.time() - start

    print(
        f"Context lookup performance: {duration * 1e6 / iterations:.2f}ns per operation"
    )
    print(f"  - Zero-overhead access to context in {iterations} iterations")
    print("  - Thread-local access with no locking")

    # Demonstrate streaming dataset access
    print("\nDemonstrating streaming dataset access:")
    try:
        # Create a streaming dataset (memory-efficient, O(1) space)
        start = time.time()
        stream = (
            data.builder()
            .from_registry("mmlu")  # Use a registered dataset
            .streaming(True)  # Enable streaming mode
            .batch_size(32)  # Configure processing batch size
            .transform(prompt_formatter)  # Add transformations (applied on the fly)
            .limit(5)  # Limit items for example
            .build()
        )
        build_time = time.time() - start

        print(f"  - Streaming dataset built in {build_time:.4f}s")
        print("  - Memory usage: O(1) regardless of dataset size")

        # Process the stream with minimal memory
        count = 0
        start = time.time()
        for item in stream:
            count += 1
            # Stop after 3 items for the example
            if count >= 3:
                break

        process_time = time.time() - start
        print(f"  - Processed {count} items in {process_time:.4f}s")

    except Exception as e:
        print(f"Error demonstrating streaming: {e}")

def main() -> None:
"""Run the enhanced DatasetBuilder example."""
print("Ember Enhanced DatasetBuilder Example")
print("=====================================")

    # List available datasets
    print("\nAvailable datasets in registry:")
    datasets = list_available_datasets()
    for dataset in datasets:
        print(f"  • {dataset}")

    # Use the enhanced builder pattern
    print("\nLoading dataset with builder pattern...")

    try:
        # Time the operation to demonstrate performance
        start_time = time.time()

        # Chain multiple configuration and transformation steps
        dataset = (
            DatasetBuilder()
            .from_registry("mmlu")  # Use a registered dataset
            .subset("high_school_mathematics")  # Select a specific subset
            .split("test")  # Choose the test split
            .sample(3)  # Random sample of 3 items
            .transform(uppercase_transformer)  # Transform to uppercase
            .transform(prompt_formatter)  # Format as prompts
            .streaming(False)  # Use non-streaming for this example
            .build()
        )

        end_time = time.time()
        print(f"Dataset loaded in {end_time - start_time:.4f} seconds")

        # Display the loaded dataset
        print(f"\nLoaded dataset with {len(dataset)} entries")

        for i, entry in enumerate(dataset):
            print(f"\nEntry {i+1}:")

            # Display query from entry
            print(f"Query: {entry.query}")

            # Display formatted prompt if available
            if hasattr(entry, "metadata") and entry.metadata:
                if "formatted_prompt" in entry.metadata:
                    print(f"\n{'-'*40}")
                    print(f"{entry.metadata['formatted_prompt']}")
                    print(f"{'-'*40}")

                # Display other metadata
                print("\nMetadata:")
                for key, value in entry.metadata.items():
                    if key != "formatted_prompt":
                        print(f"  {key}: {value}")

            # Display choices if available
            if hasattr(entry, "choices") and entry.choices:
                print("\nChoices:")
                for key, value in entry.choices.items():
                    print(f"  {key}: {value}")

        # Demonstrate performance optimizations
        demonstrate_performance()

    except Exception as e:
        print(f"Error loading dataset: {e}")
        import traceback

        traceback.print_exc()

if **name** == "**main**":
main()

</code>

src\ember\examples\data\explore_datasets.py:
<code>
"""Tool for exploring Ember's specialized datasets (AIME, GPQA, Codeforces).

This utility helps users explore sample problems from each specialized dataset,
understand their structure, and test their integration with the Ember framework.

Usage:
python -m ember.examples.data.explore_datasets --dataset aime --count 3
"""

import argparse
import logging

from ember.api import DatasetBuilder, datasets

# Set up logging

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(**name**)

def explore_aime(count: int = 3) -> None:
"""Explore AIME dataset structure.

    Args:
        count: Number of examples to show
    """
    try:
        logger.info("=== AIME Dataset Exploration ===")

        # Load AIME dataset
        aime_data = datasets("aime")
        logger.info(f"Successfully loaded {len(aime_data)} AIME problems")

        if not aime_data:
            logger.error("Dataset loaded but contains no problems")
            return

        # Sample problems
        sample_size = min(count, len(aime_data))
        logger.info(f"\nShowing {sample_size} sample AIME problems:")

        for i, problem in enumerate(aime_data[:sample_size]):
            problem_id = problem.metadata.get("problem_id", f"Problem {i+1}")
            logger.info(f"\n--- Problem {i+1}: {problem_id} ---")

            # Print problem
            logger.info(f"Query: {problem.query}\n")

            # Print answer
            answer = problem.metadata.get("correct_answer", "Unknown")
            logger.info(f"Answer: {answer}")

            # Print metadata
            logger.info("Metadata:")
            year = problem.metadata.get("year", "Unknown")
            contest = problem.metadata.get("contest", "Unknown")
            logger.info(f"  Year: {year}")
            logger.info(f"  Contest: {contest}")

            if i < sample_size - 1:
                logger.info("\n" + "-" * 40)

        # Show filtering example
        logger.info("\n=== Filtering Example ===")

        # Filter to AIME I problems
        from ember.core.utils.data.base.config import BaseDatasetConfig

        class AIMEConfig(BaseDatasetConfig):
            contest: str = None

        # Load dataset first, then filter manually
        aime_data = datasets("aime")

        # Filter to AIME I problems using config method
        aime_i = DatasetBuilder().from_registry("aime").config(contest="I").build()
        logger.info(f"AIME I problems: {len(aime_i)}")

        # Filter to AIME II problems
        aime_ii = DatasetBuilder().from_registry("aime").config(contest="II").build()
        logger.info(f"AIME II problems: {len(aime_ii)}")

    except Exception as e:
        logger.error(f"Error exploring AIME dataset: {e}")

def explore_gpqa(count: int = 3) -> None:
"""Explore GPQA dataset structure.

    Args:
        count: Number of examples to show
    """
    try:
        logger.info("=== GPQA Dataset Exploration ===")

        # Load GPQA dataset
        gpqa_data = datasets("gpqa")
        logger.info(f"Successfully loaded {len(gpqa_data)} GPQA questions")

        if not gpqa_data:
            logger.error("Dataset loaded but contains no questions")
            return

        # Sample questions
        sample_size = min(count, len(gpqa_data))
        logger.info(f"\nShowing {sample_size} sample GPQA questions:")

        for i, question in enumerate(gpqa_data[:sample_size]):
            question_id = question.metadata.get("id", f"Question {i+1}")
            logger.info(f"\n--- Question {i+1}: {question_id} ---")

            # Print question
            logger.info(f"Query: {question.query[:500]}...\n")

            # Print choices
            logger.info("Choices:")
            for key, choice in question.choices.items():
                logger.info(
                    f"  {key}: {choice[:100]}..."
                    if len(choice) > 100
                    else f"  {key}: {choice}"
                )

            # Print answer
            answer = question.metadata.get("correct_answer", "Unknown")
            logger.info(f"\nCorrect Answer: {answer}")

            # Print metadata
            logger.info("Metadata:")
            subject = question.metadata.get("subject", "Unknown")
            difficulty = question.metadata.get("difficulty", "Unknown")
            logger.info(f"  Subject: {subject}")
            logger.info(f"  Difficulty: {difficulty}")

            if i < sample_size - 1:
                logger.info("\n" + "-" * 40)

        # Show filtering example
        logger.info("\n=== Filtering Example ===")

        # Filter by subject
        physics_questions = (
            DatasetBuilder()
            .from_registry("gpqa")
            .filter(lambda item: "physics" in item.metadata.get("subject", "").lower())
            .build()
        )
        logger.info(f"Physics questions: {len(physics_questions)}")

        chemistry_questions = (
            DatasetBuilder()
            .from_registry("gpqa")
            .filter(
                lambda item: "chemistry" in item.metadata.get("subject", "").lower()
            )
            .build()
        )
        logger.info(f"Chemistry questions: {len(chemistry_questions)}")

    except Exception as e:
        logger.error(f"Error exploring GPQA dataset: {e}")
        if "gated dataset" in str(e) or "authentication" in str(e):
            logger.error("\nAuthentication required for GPQA dataset:")
            logger.error("1. Run: huggingface-cli login")
            logger.error(
                "2. Request access at: https://huggingface.co/datasets/Idavidrein/gpqa"
            )

def explore_codeforces(count: int = 3) -> None:
"""Explore Codeforces dataset structure.

    Args:
        count: Number of examples to show
    """
    try:
        logger.info("=== Codeforces Dataset Exploration ===")

        # Load Codeforces dataset with filtering
        cf_data = (
            DatasetBuilder()
            .from_registry("codeforces")
            .configure(difficulty_range=(800, 1200))  # Beginner-friendly
            .build()
        )
        logger.info(f"Successfully loaded {len(cf_data)} Codeforces problems")

        if not cf_data:
            logger.error("Dataset loaded but contains no problems")
            return

        # Sample problems
        sample_size = min(count, len(cf_data))
        logger.info(f"\nShowing {sample_size} sample Codeforces problems:")

        for i, problem in enumerate(cf_data[:sample_size]):
            problem_id = problem.metadata.get("id", f"Problem {i+1}")
            name = problem.metadata.get("name", "Unnamed problem")
            logger.info(f"\n--- Problem {i+1}: {problem_id} - {name} ---")

            # Print problem (truncated)
            query_preview = (
                problem.query[:500] + "..."
                if len(problem.query) > 500
                else problem.query
            )
            logger.info(f"Query: {query_preview}\n")

            # Print metadata
            logger.info("Metadata:")
            difficulty = problem.metadata.get("difficulty", "Unknown")
            tags = problem.metadata.get("tags", [])
            logger.info(f"  Difficulty: {difficulty}")
            logger.info(f"  Tags: {', '.join(tags)}")

            # Print test cases
            test_cases = problem.metadata.get("test_cases", [])
            if test_cases:
                logger.info(f"\nTest Cases ({len(test_cases)}):")
                for j, test in enumerate(
                    test_cases[:2]
                ):  # Show only first 2 test cases
                    logger.info(f"  Test {j+1}:")
                    logger.info(f"    Input: {test.get('input', 'N/A')}")
                    logger.info(f"    Expected Output: {test.get('output', 'N/A')}")

                if len(test_cases) > 2:
                    logger.info(f"  ... and {len(test_cases) - 2} more test cases")

            if i < sample_size - 1:
                logger.info("\n" + "-" * 40)

        # Show filtering example
        logger.info("\n=== Filtering Examples ===")

        # Filter by difficulty using config
        easy_problems = (
            DatasetBuilder()
            .from_registry("codeforces")
            .config(difficulty_range=(800, 1200))
            .build()
        )
        logger.info(f"Easy problems (800-1200): {len(easy_problems)}")

        # Medium difficulty problems
        medium_problems = (
            DatasetBuilder()
            .from_registry("codeforces")
            .config(difficulty_range=(1300, 1700))
            .build()
        )
        logger.info(f"Medium problems (1300-1700): {len(medium_problems)}")

        # Filter by tag
        dp_problems = (
            DatasetBuilder().from_registry("codeforces").config(tags=["dp"]).build()
        )
        logger.info(f"Dynamic Programming problems: {len(dp_problems)}")

    except Exception as e:
        logger.error(f"Error exploring Codeforces dataset: {e}")

def main() -> None:
"""Explore specialized datasets in Ember."""
parser = argparse.ArgumentParser(description="Explore Ember's specialized datasets")
parser.add_argument(
"--dataset",
choices=["aime", "gpqa", "codeforces", "all"],
default="all",
help="Dataset to explore",
)
parser.add_argument(
"--count", type=int, default=3, help="Number of examples to show"
)
args = parser.parse_args()

    # Explore requested datasets
    if args.dataset == "all" or args.dataset == "aime":
        explore_aime(args.count)
        if args.dataset != "all":
            return

    if args.dataset == "all" or args.dataset == "gpqa":
        if args.dataset == "all":
            logger.info("\n\n" + "=" * 50 + "\n")
        explore_gpqa(args.count)
        if args.dataset != "all":
            return

    if args.dataset == "all" or args.dataset == "codeforces":
        if args.dataset == "all":
            logger.info("\n\n" + "=" * 50 + "\n")
        explore_codeforces(args.count)

if **name** == "**main**":
main()

</code>

src\ember\examples\data\mcq_experiment_example.py:
<code>
"""
MCQ Experiment Example: Multiple-Choice Question Evaluation Example

This simplified example demonstrates how to create and evaluate multiple-choice
questions using different answer strategies, without requiring external API calls.

To run:
uv run python src/ember/examples/data/mcq_experiment_example.py
"""

import argparse
import random
from typing import Dict, List, Tuple

class DatasetEntry:
"""A simple dataset entry with questions and choices."""

    def __init__(
        self, query: str, choices: Dict[str, str], metadata: Dict[str, str]
    ) -> None:
        """Initialize a dataset entry.

        Args:
            query: The question text
            choices: A dictionary of lettered choices
            metadata: Additional information
        """
        self.query = query
        self.choices = choices
        self.metadata = metadata

class EnsureValidChoiceOperator:
"""Validates and corrects answers to match the valid choices."""

    def __init__(self, name: str = "EnsureValidChoice") -> None:
        """Initialize the operator.

        Args:
            name: The name of this operator
        """
        self.name = name

    def __call__(
        self, query: str, partial_answer: str, choices: Dict[str, str]
    ) -> Dict[str, str]:
        """Process the input to ensure a valid choice is returned.

        Args:
            query: The question text
            partial_answer: The proposed answer that may need validation
            choices: The valid choices

        Returns:
            A dictionary with the final answer
        """
        # First check if the answer is already valid
        if partial_answer in choices:
            return {"final_answer": partial_answer}

        # If not valid, try to match to the most similar valid choice
        # In a real system, this would use an LLM, but here we'll do a simple mapping

        # First check if it's just a case mismatch
        for choice in choices:
            if choice.upper() == partial_answer.upper():
                return {"final_answer": choice}

        # Next, check if it's the text of an answer rather than the key
        for choice, text in choices.items():
            if partial_answer.lower() in text.lower():
                return {"final_answer": choice}

        # If all else fails, return the first choice as fallback
        if choices:
            return {"final_answer": next(iter(choices))}

        # If no choices at all, return empty
        return {"final_answer": ""}

class SingleModelBaseline:
"""A simple model that makes a choice based on fixed probabilities."""

    def __init__(self, name: str = "SingleModelBaseline") -> None:
        """Initialize the model.

        Args:
            name: The name of this model
        """
        self.name = name
        self.ensure_valid_choice = EnsureValidChoiceOperator()

    def __call__(self, query: str, choices: Dict[str, str]) -> Dict[str, str]:
        """Process a question with a single model approach.

        Args:
            query: The question text
            choices: The available choices

        Returns:
            A dictionary with the final answer
        """
        # In a real system, this would call a language model
        # Here we'll simulate it with a biased random choice

        # Generate a mock response based on keywords in the query
        if "capital" in query.lower():
            # For capital questions, prefer C as answer
            weights = {"A": 0.1, "B": 0.1, "C": 0.7, "D": 0.1}
        elif "mammal" in query.lower() or "animal" in query.lower():
            # For biology questions, prefer D
            weights = {"A": 0.1, "B": 0.1, "C": 0.1, "D": 0.7}
        elif any(keyword in query.lower() for keyword in ["math", "square", "number"]):
            # For math questions, prefer B
            weights = {"A": 0.1, "B": 0.7, "C": 0.1, "D": 0.1}
        else:
            # Otherwise equal chance
            weights = {k: 1.0 / len(choices) for k in choices}

        # Make a weighted random choice
        choices_list = list(weights.keys())
        weights_list = [weights[k] for k in choices_list]
        partial_answer = random.choices(choices_list, weights=weights_list, k=1)[0]

        # Ensure it's a valid choice
        result = self.ensure_valid_choice(
            query=query, partial_answer=partial_answer, choices=choices
        )
        return {"final_answer": result["final_answer"]}

class MultiModelEnsemble:
"""Simulates an ensemble of models making independent predictions."""

    def __init__(self, name: str = "MultiModelEnsemble", num_models: int = 3) -> None:
        """Initialize the ensemble.

        Args:
            name: The name of this ensemble
            num_models: The number of models in the ensemble
        """
        self.name = name
        self.num_models = num_models
        self.ensure_valid_choice = EnsureValidChoiceOperator()

    def __call__(self, query: str, choices: Dict[str, str]) -> Dict[str, str]:
        """Process a question with an ensemble approach.

        Args:
            query: The question text
            choices: The available choices

        Returns:
            A dictionary with the final answer
        """
        # Generate multiple responses simulating different models
        responses = []
        for _ in range(self.num_models):
            # Each "model" has slightly different preferences
            if "capital" in query.lower():
                weights = {
                    "A": 0.1,
                    "B": 0.1,
                    "C": 0.6 + random.uniform(-0.2, 0.2),
                    "D": 0.1,
                }
            elif "mammal" in query.lower() or "animal" in query.lower():
                weights = {
                    "A": 0.1,
                    "B": 0.1,
                    "C": 0.1,
                    "D": 0.6 + random.uniform(-0.2, 0.2),
                }
            elif any(
                keyword in query.lower() for keyword in ["math", "square", "number"]
            ):
                weights = {
                    "A": 0.1,
                    "B": 0.6 + random.uniform(-0.2, 0.2),
                    "C": 0.1,
                    "D": 0.1,
                }
            else:
                # Otherwise slightly different preferences per model
                weights = {
                    k: 1.0 / len(choices) + random.uniform(-0.1, 0.1) for k in choices
                }
                # Normalize weights
                total = sum(weights.values())
                weights = {k: v / total for k, v in weights.items()}

            # Make a weighted random choice
            choices_list = list(weights.keys())
            weights_list = [weights[k] for k in choices_list]
            choice = random.choices(choices_list, weights=weights_list, k=1)[0]
            responses.append(choice)

        # Synthesize responses (use majority vote in this simplified version)
        # Count occurrences of each answer
        vote_counts = {}
        for response in responses:
            vote_counts[response] = vote_counts.get(response, 0) + 1

        # Find the majority answer
        if vote_counts:
            majority_answer = max(vote_counts.items(), key=lambda x: x[1])[0]
        else:
            majority_answer = next(iter(choices)) if choices else ""

        # Ensure it's a valid choice
        result = self.ensure_valid_choice(
            query=query, partial_answer=majority_answer, choices=choices
        )
        return {"final_answer": result["final_answer"]}

class VariedModelEnsemble:
"""Simulates different types of models in an ensemble."""

    def __init__(self, name: str = "VariedModelEnsemble") -> None:
        """Initialize the varied ensemble.

        Args:
            name: The name of this ensemble
        """
        self.name = name
        self.ensure_valid_choice = EnsureValidChoiceOperator()

    def __call__(self, query: str, choices: Dict[str, str]) -> Dict[str, str]:
        """Process a question with a varied model ensemble approach.

        Args:
            query: The question text
            choices: The available choices

        Returns:
            A dictionary with the final answer
        """
        # Simulate different model types with distinct characteristics
        model_types = [
            {"name": "fact_model", "strengths": ["capital", "country", "geography"]},
            {"name": "science_model", "strengths": ["mammal", "animal", "biology"]},
            {"name": "math_model", "strengths": ["math", "square", "number"]},
        ]

        # Generate responses from each model
        responses = []
        for model in model_types:
            # Calculate the confidence based on model's strengths
            confidence = 0.5  # baseline
            for strength in model["strengths"]:
                if strength in query.lower():
                    confidence += 0.2  # boost confidence for relevant questions

            # Generate answer based on model confidence and domain
            if model["name"] == "fact_model" and "capital" in query.lower():
                weights = {"A": 0.1, "B": 0.1, "C": 0.7 * confidence, "D": 0.1}
            elif model["name"] == "science_model" and (
                "mammal" in query.lower() or "animal" in query.lower()
            ):
                weights = {"A": 0.1, "B": 0.1, "C": 0.1, "D": 0.7 * confidence}
            elif model["name"] == "math_model" and any(
                keyword in query.lower() for keyword in ["math", "square", "number"]
            ):
                weights = {"A": 0.1, "B": 0.7 * confidence, "C": 0.1, "D": 0.1}
            else:
                # Less confident outside its domain
                weights = {k: 1.0 / len(choices) for k in choices}

            # Normalize weights
            total = sum(weights.values())
            if total > 0:
                weights = {k: v / total for k, v in weights.items()}
            else:
                weights = {k: 1.0 / len(choices) for k in choices}

            # Make a weighted random choice
            choices_list = list(weights.keys())
            weights_list = [weights[k] for k in choices_list]
            choice = random.choices(choices_list, weights=weights_list, k=1)[0]
            responses.append(choice)

        # "Judge synthesis" - weight answers by confidence and domain relevance
        # This is simplified; in real systems this would be more complex
        if "capital" in query.lower() or "country" in query.lower():
            # Trust fact_model more for geography
            final_answer = responses[0]
        elif (
            "mammal" in query.lower()
            or "animal" in query.lower()
            or "biology" in query.lower()
        ):
            # Trust science_model more for biology
            final_answer = responses[1]
        elif any(keyword in query.lower() for keyword in ["math", "square", "number"]):
            # Trust math_model more for math
            final_answer = responses[2]
        else:
            # Otherwise take majority vote
            vote_counts = {}
            for response in responses:
                vote_counts[response] = vote_counts.get(response, 0) + 1

            if vote_counts:
                final_answer = max(vote_counts.items(), key=lambda x: x[1])[0]
            else:
                final_answer = next(iter(choices)) if choices else ""

        # Ensure it's a valid choice
        result = self.ensure_valid_choice(
            query=query, partial_answer=final_answer, choices=choices
        )
        return {"final_answer": result["final_answer"]}

def create_mock_dataset() -> List[DatasetEntry]:
"""Create a mock multiple-choice dataset in MMLU style."""
return [
DatasetEntry(
query="What is the capital of France?",
choices={"A": "Berlin", "B": "Madrid", "C": "Paris", "D": "Rome"},
metadata={"correct_answer": "C", "subject": "Geography"},
),
DatasetEntry(
query="Which of these is a mammal?",
choices={"A": "Shark", "B": "Snake", "C": "Eagle", "D": "Dolphin"},
metadata={"correct_answer": "D", "subject": "Biology"},
),
DatasetEntry(
query="What is the square root of 144?",
choices={"A": "10", "B": "12", "C": "14", "D": "16"},
metadata={"correct_answer": "B", "subject": "Mathematics"},
),
DatasetEntry(
query="Who wrote 'Pride and Prejudice'?",
choices={
"A": "Jane Austen",
"B": "Charles Dickens",
"C": "Emily Brontë",
"D": "Mark Twain",
},
metadata={"correct_answer": "A", "subject": "Literature"},
),
DatasetEntry(
query="Which planet is known as the Red Planet?",
choices={"A": "Jupiter", "B": "Venus", "C": "Mars", "D": "Saturn"},
metadata={"correct_answer": "C", "subject": "Astronomy"},
),
]

def score_entry(
entry: DatasetEntry, pipeline_name: str, prediction: str
) -> Tuple[str, bool]:
"""Score a prediction against the correct answer.

    Args:
        entry: The dataset entry being evaluated
        pipeline_name: Name of the pipeline making the prediction
        prediction: The predicted answer

    Returns:
        A tuple with the pipeline name and whether the prediction was correct
    """
    correct_answer = entry.metadata.get("correct_answer", "").upper()
    prediction = prediction.upper()
    is_correct = prediction == correct_answer
    return (pipeline_name, is_correct)

def parse_arguments() -> argparse.Namespace:
"""Parse command-line arguments."""
parser = argparse.ArgumentParser(
description="MCQ Experiment Example demonstrating different answer strategies"
)
parser.add_argument(
"--num_samples", type=int, default=5, help="Number of samples to process"
)
parser.add_argument(
"--seed", type=int, default=42, help="Random seed for reproducibility"
)
return parser.parse_args()

def main() -> None:
"""Run the MCQ experiment example.""" # Parse command line arguments
args = parse_arguments()

    # Set random seed for reproducibility
    random.seed(args.seed)

    print("Ember MCQ Experiment Example")
    print("===========================")

    # Create dataset
    dataset = create_mock_dataset()
    dataset_size = min(args.num_samples, len(dataset))
    sample = dataset[:dataset_size]  # Take the first N entries

    # Create the pipelines
    baseline = SingleModelBaseline()
    ensemble = MultiModelEnsemble(num_models=3)
    varied = VariedModelEnsemble()

    # Track results
    results = {
        "SingleModelBaseline": {"correct": 0, "total": 0},
        "MultiModelEnsemble": {"correct": 0, "total": 0},
        "VariedModelEnsemble": {"correct": 0, "total": 0},
    }

    # Process each entry
    for i, entry in enumerate(sample):
        print(f"\nQuestion {i+1}: {entry.query}")
        print(f"Subject: {entry.metadata.get('subject', 'Unknown')}")

        # Show choices
        print("Choices:")
        correct = entry.metadata.get("correct_answer", "")
        for letter, text in entry.choices.items():
            is_correct = "✓" if letter == correct else " "
            print(f"  {letter}. {text} {is_correct}")

        # Process with each pipeline
        baseline_result = baseline(query=entry.query, choices=entry.choices)
        ensemble_result = ensemble(query=entry.query, choices=entry.choices)
        varied_result = varied(query=entry.query, choices=entry.choices)

        # Get predictions
        baseline_pred = baseline_result["final_answer"]
        ensemble_pred = ensemble_result["final_answer"]
        varied_pred = varied_result["final_answer"]

        # Score and update results
        for name, pred in [
            ("SingleModelBaseline", baseline_pred),
            ("MultiModelEnsemble", ensemble_pred),
            ("VariedModelEnsemble", varied_pred),
        ]:
            pipeline_name, is_correct = score_entry(entry, name, pred)
            results[pipeline_name]["total"] += 1
            if is_correct:
                results[pipeline_name]["correct"] += 1

            # Display prediction
            correct_str = "✓" if is_correct else "✗"
            print(f"{name} prediction: {pred} {correct_str}")

    # Show final results
    print("\nFinal Results:")
    print("-------------")
    header = f"{'Pipeline':<25} {'Accuracy':<10} {'Correct/Total'}"
    print(header)
    print("-" * len(header))

    for name, result in results.items():
        accuracy = (
            (result["correct"] / result["total"]) * 100 if result["total"] > 0 else 0
        )
        print(f"{name:<25} {accuracy:.2f}%      {result['correct']}/{result['total']}")

    print("\nNote: This example uses simulated models and predetermined outcomes.")
    print("In a real Ember pipeline, these would use actual language models.")

if **name** == "**main**":
main()

</code>

src\ember\examples\data\new_datasets_example.py:
<code>
"""Example for using specialized datasets (AIME, GPQA, Codeforces).

Run with: uvx python -m ember.examples.data.new_datasets_example [--skip-model-calls]
"""

import argparse
import logging
import sys
from typing import Dict

from ember.api import DatasetBuilder, datasets, models
from ember.core.exceptions import GatedDatasetAuthenticationError
from ember.core.utils.eval.evaluators import MultipleChoiceEvaluator
from ember.core.utils.eval.numeric_answer import AIMEAnswerEvaluator

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

def test_aime(skip_model_calls: bool = False) -> bool:
"""Test AIME dataset loading and evaluation.

    Args:
        skip_model_calls: If True, skip LLM evaluation

    Returns:
        Success status
    """
    logger.info("Testing AIME dataset...")

    try:
        aime_data = datasets("aime")
        logger.info(f"✓ Loaded {len(aime_data)} AIME problems")

        if len(aime_data) == 0:
            logger.error("Dataset loaded but contains no problems")
            return False

        # Sample problem
        problem = aime_data[0]
        logger.info(f"Sample problem: {problem.query[:100]}...")
        logger.info(f"Expected answer: {problem.metadata['correct_answer']}")

        # Test filtering
        aime_i = DatasetBuilder().from_registry("aime").config(contest="I").build()
        logger.info(f"✓ Filtered to {len(aime_i)} AIME-I problems")

        if not skip_model_calls:
            # Test evaluation
            logger.info("Testing model evaluation...")
            model = models.openai.gpt4o()
            response = model(problem.query)

            evaluator = AIMEAnswerEvaluator()
            result = evaluator.evaluate(response, problem.metadata["correct_answer"])
            logger.info(
                f"Model evaluation: {'correct' if result.is_correct else 'incorrect'}"
            )

        return True

    except Exception as e:
        logger.error(f"AIME dataset error: {e}")
        return False

def test_gpqa(skip_model_calls: bool = False) -> bool:
"""Test GPQA dataset loading and evaluation.

    Args:
        skip_model_calls: If True, skip LLM evaluation

    Returns:
        Success status
    """
    logger.info("Testing GPQA dataset...")

    try:
        gpqa_data = datasets("gpqa")
        logger.info(f"✓ Loaded {len(gpqa_data)} GPQA problems")

        if len(gpqa_data) == 0:
            logger.error("Dataset loaded but contains no questions")
            return False

        # Sample problem
        problem = gpqa_data[0]
        logger.info(f"Sample question: {problem.query[:100]}...")
        logger.info(f"Choices: {list(problem.choices.keys())}")
        logger.info(f"Expected answer: {problem.metadata['correct_answer']}")

        # Format prompt for evaluation
        prompt = problem.query + "\n\n"
        for key, choice in problem.choices.items():
            prompt += f"{key}. {choice}\n"

        if not skip_model_calls:
            # Test evaluation
            logger.info("Testing model evaluation...")
            model = models.openai.gpt4o()
            response = model(prompt)

            evaluator = MultipleChoiceEvaluator()
            result = evaluator.evaluate(response, problem.metadata["correct_answer"])
            logger.info(
                f"Model evaluation: {'correct' if result.is_correct else 'incorrect'}"
            )

        return True

    except GatedDatasetAuthenticationError as e:
        logger.error(f"Authentication required: {e.recovery_hint}")
        logger.info(
            "Request access at: https://huggingface.co/datasets/Idavidrein/gpqa"
        )
        return False
    except Exception as e:
        logger.error(f"GPQA dataset error: {e}")
        return False

def test_codeforces(skip_model_calls: bool = False) -> bool:
"""Test Codeforces dataset loading and evaluation.

    Args:
        skip_model_calls: If True, skip LLM evaluation

    Returns:
        Success status
    """
    logger.info("Testing Codeforces dataset...")

    try:
        # Test with difficulty filtering
        cf_data = (
            DatasetBuilder()
            .from_registry("codeforces")
            .config(difficulty_range=(800, 1200))
            .sample(5)
            .build()
        )
        logger.info(f"✓ Loaded {len(cf_data)} Codeforces problems")

        if len(cf_data) == 0:
            logger.error("Dataset loaded but contains no problems")
            return False

        # Sample problem
        problem = cf_data[0]
        logger.info(f"Sample problem: {problem.query[:100]}...")
        logger.info(f"Difficulty: {problem.metadata.get('difficulty')}")
        logger.info(f"Tags: {problem.metadata.get('tags', [])}")

        # Skip model evaluation for code problems to avoid long execution
        if not skip_model_calls:
            logger.info("Skipping code evaluation to avoid long execution")

        return True

    except Exception as e:
        logger.error(f"Codeforces dataset error: {e}")
        return False

def summary(results: Dict[str, bool]) -> None:
"""Print summary of test results.

    Args:
        results: Dictionary of test results
    """
    logger.info("\n--- Dataset Test Summary ---")

    all_passed = True
    for dataset, success in results.items():
        status = "✓ PASS" if success else "✗ FAIL"
        logger.info(f"{dataset}: {status}")
        all_passed = all_passed and success

    if all_passed:
        logger.info("\nAll datasets loaded successfully!")
    else:
        logger.warning("\nSome datasets failed to load. See errors above.")

    # Show authentication hint if GPQA failed
    if not results.get("gpqa", True):
        logger.info("\nFor GPQA authentication:")
        logger.info("1. Run: huggingface-cli login")
        logger.info(
            "2. Request access: https://huggingface.co/datasets/Idavidrein/gpqa"
        )

def main() -> None:
"""Run the dataset test example."""
parser = argparse.ArgumentParser(description="Test specialized datasets")
parser.add_argument(
"--skip-model-calls",
action="store_true",
help="Skip any calls to language models",
)
args = parser.parse_args()

    results = {}

    results["aime"] = test_aime(args.skip_model_calls)
    results["gpqa"] = test_gpqa(args.skip_model_calls)
    results["codeforces"] = test_codeforces(args.skip_model_calls)

    summary(results)

    # Return non-zero exit code if any test failed
    if not all(results.values()):
        sys.exit(1)

if **name** == "**main**":
main()

</code>

src\ember\examples\data\README.md:
<code>

# Ember Data Examples

This directory contains examples demonstrating Ember's data handling capabilities, including dataset creation, manipulation, and evaluation.

## Examples

- `data_api_example.py` - A standalone example that demonstrates core dataset concepts
- `mcq_experiment_example.py` - Multiple-choice question evaluation with various answer strategies
- `transformation_example.py` - XCS transformations for efficient data processing

## Running Examples

The examples in this directory are designed to run without requiring external API keys or dependencies. To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/data/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/data/example_name.py
```

Replace `example_name.py` with the desired example file.

## Example Details

### data_api_example.py

A self-contained example that demonstrates core dataset concepts without requiring the full Ember infrastructure:

- Creating and defining dataset entries with queries, choices, and metadata
- Displaying formatted dataset contents
- Sampling random entries from a dataset
- Processing and extracting information from dataset entries

This example uses a simplified `DatasetEntry` class that mimics the structure of Ember's actual data API but doesn't require any external dependencies or model registration.

### mcq_experiment_example.py

Demonstrates multiple approach strategies for answering multiple-choice questions:

- `SingleModelBaseline` - Simulates using a single model for answering
- `MultiModelEnsemble` - Simulates using an ensemble of similar models with voting
- `VariedModelEnsemble` - Simulates using specialized models for different domains

This example includes a mock dataset of multiple-choice questions across different subjects and simulates how different ensemble strategies might perform. It uses deterministic randomness (with a configurable seed) to demonstrate both correct and incorrect answers.

Command-line options:

```
--num_samples N   Number of samples to process (default: 5)
--seed N          Random seed for reproducibility (default: 42)
```

### Connecting to the Full Ember Data API

While these examples are designed to run independently, the real Ember Data API provides additional capabilities:

- Access to standard benchmarks like MMLU, TruthfulQA, and HaluEval
- Integration with the Hugging Face datasets library
- Advanced sampling and filtering
- Dataset registration and discovery
- Integration with model execution pipelines

To use the actual Ember Data API once you have set up the required environment:

```python
from ember.api import datasets, DatasetBuilder

# Simple loading
mmlu_data = datasets("mmlu")

# Advanced configuration with builder pattern
custom_dataset = (
    DatasetBuilder()
    .split("test")
    .sample(100)
    .seed(42)
    .config(config_name="abstract_algebra")
    .build("mmlu")
)
```

## Next Steps

After learning about data handling, explore:

- `models/` - For using models with datasets
- `operators/` - For building evaluation pipelines
- `advanced/` - For complex workflows combining these components
  </code>

src\ember\examples\data\transformation_example.py:
<code>
"""
Example demonstrating the use of XCS transformations: vmap, pmap/pjit, and mesh.

This example shows how to use the various XCS transformation APIs to parallelize and
distribute computations across devices.

To run:
uv run python src/ember/examples/data/transformation_example.py
"""

import argparse
import multiprocessing
import sys
import threading
from time import perf_counter
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union

import numpy as np

# Progress tracking utilities

from tqdm import tqdm

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel # Import EmberModel

# Note: Unused import removed to adhere to clean code practices.

from ember.xcs.transforms import DeviceMesh, PartitionSpec, mesh_sharded, pmap, vmap

class ProgressTracker:
"""Tracks progress across multiple parallel processes.

    This class provides a reusable mechanism for tracking progress in parallel
    processing scenarios, supporting tqdm progress bars.
    """

    def __init__(self, total: int, description: str, shared: bool = False) -> None:
        """Initialize a progress tracker.

        Args:
            total: Total number of items to process
            description: Description for the progress bar
            shared: Whether this progress bar is shared across processes
        """
        self.total = total
        self.description = description
        self.shared = shared
        self._lock = threading.Lock()
        self._progress = 0
        self._active = False
        self._pbar = None

    def __enter__(self) -> "ProgressTracker":
        """Context manager entry point."""
        self._active = True
        self._pbar = tqdm(
            total=self.total,
            desc=self.description,
            leave=True,
            position=0,
            file=sys.stdout,
        )
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Context manager exit point."""
        self._active = False
        if self._pbar:
            self._pbar.close()

    def update(self, amount: int = 1) -> None:
        """Update progress by the specified amount.

        Args:
            amount: Amount to increment progress
        """
        if not self._active:
            return

        with self._lock:
            self._progress += amount
            if self._pbar:
                self._pbar.update(amount)

def \_time_function_call(
callable_obj: Callable[..., Any], \*\*kwargs: Any
) -> Tuple[float, Any]:
"""Execute a callable with keyword arguments and measure its execution time.

    Args:
        callable_obj: The function (or callable) to execute.
        **kwargs: Arbitrary keyword arguments to pass to the callable.

    Returns:
        A tuple containing:
            - The elapsed time in seconds.
            - The result returned by the callable.
    """
    start: float = perf_counter()
    result: Any = callable_obj(**kwargs)
    elapsed: float = perf_counter() - start
    return elapsed, result

class SimpleInput(EmberModel):
"""Input model for SimpleOperator."""
prompts: Any = [] # Accept any type for prompts, default to empty list for transformation sharding

class SimpleOutput(EmberModel):
"""Output model for SimpleOperator."""
results: List[str]

class SimpleSpecification(Specification):
"""Specification for SimpleOperator."""
input_model: Type[EmberModel] = SimpleInput
structured_output: Type[EmberModel] = SimpleOutput

class SimpleOperator(Operator[SimpleInput, SimpleOutput]):
"""A simple operator that processes input prompts with CPU-intensive operations.

    This operator performs computation-heavy work that benefits from parallelization,
    demonstrating the performance advantages of different transformation strategies.

    This operator properly uses EmberModels for input and output to ensure
    type compatibility throughout the system, including with transformations.
    """

    specification = SimpleSpecification()

    # Class-level configuration
    is_heavyweight: bool = False
    progress_tracker: Optional[ProgressTracker] = None

    def forward(self, *, inputs: Union[SimpleInput, Dict[str, Any]]) -> SimpleOutput:
        """Process the provided inputs with CPU-intensive operations.

        Args:
            inputs: A SimpleInput model or a dictionary. The prompts field should contain
                a single string or a list of strings to be processed.

        Returns:
            A SimpleOutput model with the results field containing processed prompts.
        """
        # Handle both EmberModel and dictionary inputs for transformation compatibility
        if isinstance(inputs, dict):
            # For empty dictionaries or dictionaries without prompts, provide a default
            if not inputs or "prompts" not in inputs:
                prompts = ["Default prompt for empty input"]
            else:
                prompts = inputs["prompts"]
        else:
            # Already a SimpleInput model
            prompts = inputs.prompts

        # Perform CPU-intensive computation
        def cpu_intensive_task(text: str) -> str:
            """Perform an EXTREMELY CPU-intensive calculation on the input text.

            This function is designed to create a massively compute-intensive workload
            that will clearly demonstrate parallelization benefits. It uses a combination
            of techniques that are guaranteed to be CPU-bound.
            """
            # Start with the input text
            result = text

            # Determine workload intensity based on mode
            if SimpleOperator.is_heavyweight:
                # MESH-OPTIMIZED computation that will show parallelization benefits
                # The key to mesh parallelization is having computation that:
                # 1. Requires minimal communication between workers
                # 2. Has high compute-to-communication ratio
                # 3. Can be broken into independent chunks

                # Use a deterministic seed based on input text for reproducibility
                # This also ensures different inputs get different workloads
                seed = abs(hash(text)) % 10000
                np.random.seed(seed)

                # Matrix operations optimized for mesh parallelization
                # Using smaller matrices with many iterations is better for mesh
                # as it minimizes memory overhead and communication costs
                matrix_size = 300  # Smaller matrices, more iterations
                iterations = 20  # More iterations of independent computations

                result_accumulator = 0.0

                # Multiple independent iterations that can be distributed across mesh
                for iter_idx in range(iterations):
                    if SimpleOperator.progress_tracker and iter_idx % 2 == 0:
                        SimpleOperator.progress_tracker.update()

                    # Create new matrices each iteration to ensure high computation
                    matrix_a = np.random.rand(matrix_size, matrix_size)
                    matrix_b = np.random.rand(matrix_size, matrix_size)

                    # Compute matrix multiplication (O(n³) complexity)
                    result_matrix = np.matmul(matrix_a, matrix_b)

                    # Apply additional computation to increase CPU intensity
                    # Element-wise operations are very efficient for parallelization
                    result_matrix = np.sin(result_matrix) + np.cos(result_matrix * 0.5)
                    result_matrix = np.power(result_matrix, 2)

                    # Aggregate result using reduction operation
                    result_accumulator += np.sum(result_matrix)

                # Include result in output
                result = f"{result}:{abs(int(result_accumulator)) % 10000}"

                # Perfect for mesh - compute-intensive with minimal data dependencies
                # Each chunk is completely independent and can run on separate mesh cells

                def compute_mandelbrot_segment(
                    x_min: float,
                    x_max: float,
                    y_min: float,
                    y_max: float,
                    width: int,
                    height: int,
                    max_iterations: int,
                ) -> int:
                    """Compute Mandelbrot set density in a region (highly parallelizable)."""
                    total = 0

                    # For each pixel in the segment
                    for ix in range(width):
                        x0 = x_min + (x_max - x_min) * ix / width

                        for iy in range(height):
                            y0 = y_min + (y_max - y_min) * iy / height
                            x, y = 0.0, 0.0
                            iteration = 0

                            # Mandelbrot iteration
                            while x * x + y * y < 4.0 and iteration < max_iterations:
                                x_new = x * x - y * y + x0
                                y = 2 * x * y + y0
                                x = x_new
                                iteration += 1

                            # Count points that escape slowly (in the set)
                            if iteration > max_iterations // 2:
                                total += 1

                    return total

                # Divide computation into independent chunks for mesh parallelization
                # Each chunk computes a different region of the Mandelbrot set
                regions = 8  # Number of independent regions to compute
                mandelbrot_count = 0

                # These chunks are perfect for mesh cells - completely independent work
                for i in range(regions):
                    # Generate different regions based on input seed
                    x_shift = (seed % 100) / 100.0
                    y_shift = ((seed // 100) % 100) / 100.0

                    # Different region for each chunk
                    x_min = -2.0 + 0.5 * (i % 4) + x_shift
                    y_min = -1.5 + 0.5 * (i // 4) + y_shift

                    # Compute the mandelbrot set for this region
                    segment_result = compute_mandelbrot_segment(
                        x_min, x_min + 0.5, y_min, y_min + 0.5, 80, 80, 1000
                    )

                    mandelbrot_count += segment_result

                    if SimpleOperator.progress_tracker and i % 2 == 0:
                        SimpleOperator.progress_tracker.update()

                # Add the mandelbrot computation result
                result = f"{result}:{mandelbrot_count % 10000}"

            else:
                # Standard lightweight computation for quick demo
                def is_prime(n: int) -> bool:
                    """Check if a number is prime."""
                    if n <= 1:
                        return False
                    if n <= 3:
                        return True
                    if n % 2 == 0 or n % 3 == 0:
                        return False
                    i = 5
                    while i * i <= n:
                        if n % i == 0 or n % (i + 2) == 0:
                            return False
                        i += 6
                    return True

                # Find a small number of primes only
                seed = abs(hash(text)) % 10000 + 10000
                count = 0
                num = seed
                prime_count_target = 200

                while count < prime_count_target:
                    if is_prime(num):
                        result = f"{result}:{num % 100}"
                        count += 1
                    num += 1

            # Return a consistent result format
            return f"{text} -> processed:{hash(result) % 10000}"

        # Process each prompt with our CPU-intensive task
        if isinstance(prompts, list):
            processed_results: List[str] = [
                cpu_intensive_task(prompt) for prompt in prompts
            ]
        else:
            processed_results = [cpu_intensive_task(prompts)]

        # Return a SimpleOutput object with the processed results
        return SimpleOutput(results=processed_results)

def demonstrate_vmap() -> None:
"""Demonstrate the vmap (vectorized mapping) transformation.

    This function creates a vectorized version of a simple operator and compares
    its performance against sequential processing by timing both approaches.
    VMAP is ideal for batch processing without communication between items.
    """
    print("\n=== VMAP Demonstration (Vectorized Mapping) ===")
    print("VMAP transforms a function that operates on single elements into")
    print("one that efficiently processes multiple inputs in parallel.")
    print("It's ideal for batch processing with minimal overhead.\n")

    # In heavyweight mode, use a smaller batch size to make each item extremely intensive
    # Rather than many small items, we'll use fewer extremely heavy items
    cpu_count = multiprocessing.cpu_count()
    if SimpleOperator.is_heavyweight:
        # Use a batch size that's enough to demonstrate vectorization benefits
        # but not so large that each item takes too long
        batch_size = max(4, cpu_count)  # Ensure at least 4 items for good demonstration
    else:
        # Use more items for lightweight demo
        batch_size = cpu_count * 2

    print(
        f"Processing batch of {batch_size} items with{'out' if not SimpleOperator.is_heavyweight else ''} heavy computation..."
    )

    simple_operator: SimpleOperator = SimpleOperator()
    vectorized_operator: Callable[..., Any] = vmap(simple_operator)

    # Create batch inputs with appropriate size
    prompts_list = [f"VMap item {i:03d} batch" for i in range(batch_size)]
    batch_inputs = SimpleInput(prompts=prompts_list)

    print("\nRunning sequential processing (one item at a time)...")
    # Time sequential processing: apply the operator separately for each prompt.
    start_seq: float = perf_counter()
    sequential_results: List[SimpleOutput] = [
        simple_operator(inputs=SimpleInput(prompts=prompt))
        for prompt in prompts_list
    ]
    sequential_time: float = perf_counter() - start_seq
    print(f"Sequential processing time: {sequential_time:.4f}s")

    print("\nRunning vectorized processing (all items at once)...")
    # Time vectorized processing: apply the operator once across all inputs.
    start_vec: float = perf_counter()
    vectorized_results: SimpleOutput = vectorized_operator(inputs=batch_inputs)
    vectorized_time: float = perf_counter() - start_vec
    print(f"Vectorized processing time: {vectorized_time:.4f}s")

    if vectorized_time > 0 and sequential_time > 0:
        speedup: float = sequential_time / vectorized_time
        print(f"Speedup: {speedup:.2f}x")
        # Highlight significant speedups
        if speedup > 1.5:
            print(f"🚀 SIGNIFICANT SPEEDUP ACHIEVED: {speedup:.2f}x faster!")
            print("Vectorized processing is efficiently handling the batch!")
        elif speedup > 1.0:
            print(f"✓ Speedup achieved: {speedup:.2f}x faster")
            print("VMAP is showing benefits for batch processing.")
        else:
            print("⚠️ No speedup detected. This can happen when:")
            print("  1. The operation has overhead that negates vectorization benefits")
            print("  2. The batch size is too small to amortize setup costs")
            print("  3. The sequential implementation is already optimized")
            print("Try with the --heavy flag or larger batch sizes.")

    # Display sample results from the vectorized operator.
    print("\nResults from vectorized operator (sample):")
    results = vectorized_results.results if hasattr(vectorized_results, 'results') else []
    sample_size = min(3, len(results))
    for result in results[:sample_size]:
        print(f"  {result}")
    if len(results) > sample_size:
        print(f"  ... and {len(results) - sample_size} more results")

def demonstrate_pmap() -> None:
"""Demonstrate the pmap (parallel mapping) transformation.

    This function creates a parallelized operator and compares its performance on a batch
    of inputs against the sequential execution of the operator. PMAP distributes
    computation across available devices with minimal code changes.
    """
    print("\n=== PMAP Demonstration (Parallel Mapping) ===")
    print("PMAP automatically distributes computation across available CPU cores.")
    print("It's a simple yet powerful way to parallelize computation with minimal")
    print("code changes, offering good performance for many workloads.\n")

    # In heavyweight mode, use a batch size optimized for parallelism
    cpu_count = multiprocessing.cpu_count()
    if SimpleOperator.is_heavyweight:
        # For heavy computation, use exactly one task per core
        # The optimal batch size for PMAP is typically one item per available device
        batch_size = cpu_count
    else:
        # Use more items for lightweight demo
        batch_size = cpu_count * 2

    print(
        f"Processing batch of {batch_size} items with{'out' if not SimpleOperator.is_heavyweight else ''} parallel-optimized computation..."
    )
    print(f"System has {cpu_count} CPU cores available for parallelization")

    simple_operator: SimpleOperator = SimpleOperator()
    parallel_operator: Callable[..., Any] = pmap(simple_operator)

    # Create batch inputs with appropriate size
    prompts_list = [f"PMap item {i:03d} core{i%cpu_count}" for i in range(batch_size)]
    batch_inputs = SimpleInput(prompts=prompts_list)

    print("\nRunning sequential processing (single-threaded)...")
    # Time sequential processing on the batch.
    sequential_time, sequential_results = _time_function_call(
        simple_operator, inputs=batch_inputs
    )
    print(f"Sequential processing time: {sequential_time:.4f}s")

    print("\nRunning parallel processing (multi-threaded with pmap)...")
    # Time parallelized processing on the batch.
    parallel_time, parallel_results = _time_function_call(
        parallel_operator, inputs=batch_inputs
    )
    print(f"Parallel processing time: {parallel_time:.4f}s")

    if parallel_time > 0 and sequential_time > 0:
        speedup: float = sequential_time / parallel_time
        print(f"Speedup: {speedup:.2f}x")
        # Highlight significant speedups
        if speedup > 1.5:
            print(f"🚀 SIGNIFICANT SPEEDUP ACHIEVED: {speedup:.2f}x faster!")
            print("Parallel processing is effectively using multiple CPU cores!")

            # Calculate efficiency relative to theoretical maximum
            theoretical_max = min(cpu_count, batch_size)
            efficiency = (speedup / theoretical_max) * 100
            print(
                f"Parallelization efficiency: {efficiency:.1f}% of theoretical maximum"
            )

            if efficiency > 75:
                print(
                    "Excellent efficiency! The computation is well-suited for parallelization."
                )
            elif efficiency > 50:
                print(
                    "Good efficiency. Some overhead, but still effective parallelization."
                )
            else:
                print(
                    "Moderate efficiency. Consider optimizing for better parallel scaling."
                )
        elif speedup > 1.0:
            print(f"✓ Speedup achieved: {speedup:.2f}x faster")
            print("PMAP is utilizing multiple cores, showing performance benefits.")
        else:
            print("⚠️ No speedup detected. This can happen when:")
            print("  1. The operation has high thread coordination overhead")
            print("  2. The computation is too light to benefit from parallelization")
            print("  3. System resources are already constrained")
            print("Try with the --heavy flag for more intensive computation.")
    else:
        print("Parallel processing time too small to calculate speedup")

    # Display sample results from the parallel operator.
    print("\nResults from parallel operator (sample):")
    results = parallel_results.results if hasattr(parallel_results, 'results') else []
    sample_size = min(3, len(results))
    for result in results[:sample_size]:
        print(f"  {result}")
    if len(results) > sample_size:
        print(f"  ... and {len(results) - sample_size} more results")

def create_adaptive_mesh() -> Tuple[DeviceMesh, Dict[str, PartitionSpec]]:
"""Create an optimal device mesh based on available system resources.

    This function detects system capabilities and constructs a mesh configuration
    that will work efficiently on the current hardware. It handles various CPU
    counts gracefully and creates an appropriate partition specification.

    Returns:
        A tuple containing:
            - A configured DeviceMesh instance
            - An appropriate partition specification for the mesh

    Raises:
        ValueError: If unable to create a valid mesh configuration.
    """
    # Get available CPU cores, reserving at least one for system processes
    available_cores: int = max(2, multiprocessing.cpu_count() - 1)

    # Create devices list explicitly to avoid automatic device detection
    devices: List[str] = [f"cpu:{i}" for i in range(available_cores)]

    # Determine optimal mesh shape based on available cores
    if available_cores >= 4:
        # For 4+ cores, create a 2D mesh with shape that divides evenly
        # Calculate factors to find a balanced 2D shape
        for i in range(int(available_cores**0.5), 0, -1):
            if available_cores % i == 0:
                rows: int = i
                cols: int = available_cores // i
                break
        else:
            # Fallback: use a subset of cores for a clean 2x2 configuration
            rows, cols = 2, 2
            devices = devices[:4]  # Use only the first 4 devices

        mesh_shape: Tuple[int, ...] = (rows, cols)

        # Use explicit sharding along both dimensions for better efficiency
        # This optimizes the distribution of work across all mesh cells
        partition_spec: Dict[str, PartitionSpec] = {
            "prompts": PartitionSpec(0, 1)  # Shard along both dimensions
        }
    else:
        # For fewer cores, create a simple 1D mesh
        mesh_shape = (len(devices),)
        partition_spec = {"prompts": PartitionSpec(0)}  # Shard along the only dimension

    # Create the mesh with explicit devices and shape
    device_mesh: DeviceMesh = DeviceMesh(devices=devices, shape=mesh_shape)
    return device_mesh, partition_spec

def demonstrate_mesh() -> None:
"""Demonstrate the device mesh transformation.

    This function creates an adaptive device mesh based on available system resources
    and demonstrates distributed computation through mesh sharding. The implementation
    is resilient to varying hardware environments and will adapt accordingly.
    """
    print("\n=== Device Mesh Demonstration ===")

    simple_operator: SimpleOperator = SimpleOperator()

    try:
        # Create an adaptive mesh configuration
        device_mesh, partition_spec = create_adaptive_mesh()
        print(f"Created mesh: {device_mesh}")
        print(f"Partition spec: {partition_spec}")

        # Create the sharded operator
        sharded_operator: Callable[..., Any] = mesh_sharded(
            simple_operator, device_mesh, in_partition=partition_spec
        )

        # Customize batch size for mesh processing
        cpu_count = multiprocessing.cpu_count()

        # For best mesh performance, each batch item needs to be substantial
        # but we need enough items to fully utilize the mesh structure
        if SimpleOperator.is_heavyweight:
            # Create enough items to fully utilize all mesh cells with a 2x multiplier
            # to ensure each mesh cell gets multiple items for better load balancing
            mesh_size = np.prod(device_mesh.shape)
            batch_size = mesh_size * 2
        else:
            # Use more items for lightweight demo
            batch_size = cpu_count * 2

        print(
            f"Processing batch of {batch_size} items with{'out' if not SimpleOperator.is_heavyweight else ''} mesh-optimized computation..."
        )

        # Create input data with IDs that ensure even distribution
        prompts_list = [
            f"Mesh task {i:03d} region {i % np.prod(device_mesh.shape)}"
            for i in range(batch_size)
        ]
        batch_inputs = SimpleInput(prompts=prompts_list)

        print("\nMesh parallelization benefits explanation:")
        print("- Distributes work across a structured grid of devices")
        print("- Shards data along multiple dimensions (unlike simple pmap)")
        print("- Allows fine-grained control over work distribution")
        print("- Minimizes communication overhead between workers")
        print("- Especially beneficial for computation that can be chunked")
        print("  into independent parts with minimal coordination")
        print("")

        print("Running sequential processing...")
        # Time sequential processing on the batch
        sequential_time, sequential_results = _time_function_call(
            simple_operator, inputs=batch_inputs
        )
        print(f"Sequential processing time: {sequential_time:.4f}s")

        print("\nRunning mesh-sharded processing...")
        # Time mesh-sharded processing on the batch
        sharded_time, sharded_results = _time_function_call(
            sharded_operator, inputs=batch_inputs
        )
        print(f"Mesh-sharded processing time: {sharded_time:.4f}s")

        if sharded_time > 0 and sequential_time > 0:
            speedup: float = sequential_time / sharded_time
            print(f"Speedup: {speedup:.2f}x")
            # Highlight significant speedups
            if speedup > 1.5:
                print(f"🚀 SIGNIFICANT SPEEDUP ACHIEVED: {speedup:.2f}x faster!")
                print("The mesh parallelization is successfully distributing work!")
            elif speedup > 1.0:
                print(f"✓ Speedup achieved: {speedup:.2f}x faster")
                print("Consider increasing computation intensity for greater benefits.")
            else:
                print("⚠️ No speedup detected. This can happen when:")
                print("  1. Overhead of distribution exceeds computation benefits")
                print("  2. The computation is not well-suited for mesh parallelism")
                print("  3. System resources are already saturated")
                print("Try with the --heavy flag for more intensive computation.")
        else:
            print("Mesh-sharded processing time too small to calculate speedup")

        # Display a sample of results (limit output for large result sets)
        print("\nResults from mesh-sharded operator (sample):")
        results = sharded_results.results if hasattr(sharded_results, 'results') else []
        sample_size: int = min(5, len(results))
        for result in results[:sample_size]:
            print(f"  {result}")
        if len(results) > sample_size:
            print(f"  ... and {len(results) - sample_size} more results")

    except Exception as e:
        print(f"Error in mesh demonstration: {e}")
        print(
            "Mesh demonstration could not be completed due to resource constraints or configuration issues"
        )
        print("Other transformations (vmap and pmap) should still work correctly")

def parse_args() -> argparse.Namespace:
"""Parse command-line arguments.

    Returns:
        Parsed arguments namespace
    """
    parser = argparse.ArgumentParser(
        description="Demonstrate XCS transformations with parallelizable workloads"
    )
    parser.add_argument(
        "--heavy",
        action="store_true",
        help="Enable heavyweight computation to demonstrate significant parallelization benefits",
    )
    return parser.parse_args()

def main() -> None:
"""Run all transformation demonstrations with CPU-intensive workloads.

    This example demonstrates three different parallelization strategies:
    1. vmap - Vectorized mapping for batch processing of inputs
    2. pmap - Simple parallel execution across available devices
    3. mesh - Sophisticated distribution across a logical grid of devices

    Each strategy is benchmarked against sequential processing to show
    performance benefits for CPU-intensive workloads.
    """
    # Parse command line arguments
    args = parse_args()

    # Configure heavyweight computation if requested
    SimpleOperator.is_heavyweight = args.heavy

    # Welcome message
    print("XCS Transformation Examples")
    print("==========================")
    print("This example demonstrates three parallelization strategies:")
    print("  1. vmap - Vectorized mapping across batch dimensions")
    print("  2. pmap - Simple parallel execution across devices")
    print("  3. mesh - Advanced sharding across a structured grid of devices")
    print("")

    # Report computation mode
    if args.heavy:
        print("HEAVYWEIGHT COMPUTATION MODE ENABLED")
        print("This mode performs significantly more intensive calculations")
        print("to clearly demonstrate parallelization benefits.")
        print("Expected runtime: ~1-2 minutes per transformation.")
        print("")
    else:
        print("Using lightweight computation mode (for quick demonstration)")
        print("To see significant performance gains, run with --heavy flag")
        print(
            "Example: uv run python src/ember/examples/data/transformation_example.py --heavy"
        )
        print("")

    # Configure progress tracking if in heavyweight mode
    if args.heavy:
        print("Setting up progress tracking for intensive operations...")
        SimpleOperator.progress_tracker = ProgressTracker(
            total=100, description="Processing", shared=True  # Arbitrary progress units
        )

    # Run demonstrations
    try:
        if args.heavy and SimpleOperator.progress_tracker:
            with SimpleOperator.progress_tracker:
                demonstrate_vmap()
                demonstrate_pmap()
                demonstrate_mesh()
        else:
            demonstrate_vmap()
            demonstrate_pmap()
            demonstrate_mesh()

        print("\nAll transformations demonstrated successfully!")
    except KeyboardInterrupt:
        print("\nDemonstration interrupted by user.")
    finally:
        # Detailed summary with transformation comparison
        print("\n============== TRANSFORMATION COMPARISON ==============")
        print("\n1. VMAP (Vectorized Mapping)")
        print("   Best for: Batch processing without communication between items")
        print("   Advantages:")
        print("   - Simplest transformation to implement and understand")
        print("   - No communication overhead between batch items")
        print("   - Perfect for identical operations on many inputs")
        print("   - Minimal framework overhead")
        print("   Ideal use cases:")
        print("   - Preprocessing many input examples")
        print("   - Applying the same function to each item in a batch")
        print("   - Independent data transformations")

        print("\n2. PMAP (Parallel Mapping)")
        print("   Best for: Simple parallelism across available devices")
        print("   Advantages:")
        print("   - Straightforward parallelization with minimal code changes")
        print("   - Automatic utilization of available compute resources")
        print("   - Good for medium-complexity workloads")
        print("   - Balanced workload distribution")
        print("   Ideal use cases:")
        print("   - CPU-bound operations that easily divide across cores")
        print("   - When computation greatly exceeds communication costs")
        print("   - Parallel inference across multiple devices")

        print("\n3. MESH (Device Mesh Sharding)")
        print(
            "   Best for: Complex distributed computation across heterogeneous hardware"
        )
        print("   Advantages:")
        print("   - Most sophisticated and flexible parallelization")
        print("   - Multi-dimensional sharding capabilities")
        print("   - Fine-grained control over work distribution")
        print("   - Scales to extremely large computations")
        print("   - Can leverage specialized hardware (GPUs/TPUs) efficiently")
        print("   Ideal use cases:")
        print("   - Training extremely large models")
        print("   - Data-parallel and model-parallel workloads")
        print("   - Sharding computation and model parameters together")
        print("   - High-performance distributed computing")

        print("\n================= SCALING INSIGHTS =================")
        print("For enterprise-scale machine learning workloads, mesh parallelism")
        print("is the transformative technology that has enabled training models")
        print("with trillions of parameters by precisely controlling how")
        print("computation and data are distributed across hardware accelerators.")
        print("")
        print("The dramatic performance differences between these techniques become")
        print("most apparent at scale, especially when handling operations with the")
        print("following characteristics:")
        print("1. High computational intensity per operation")
        print("2. Minimal inter-operation dependencies")
        print("3. Ability to chunk work into independent segments")
        print("4. Operations that match the underlying hardware capabilities")
        print("")
        print("To further explore these capabilities, run additional experiments with")
        print("the --heavy flag and varying input sizes to see how each transformation")
        print("behaves across different workload characteristics.")

if **name** == "**main**":
main()

</code>

src\ember\examples\data\_\_init\_\_.py:
<code>

</code>

src\ember\examples\integration\api_operators_example.py:
<code>
"""Example demonstrating the improved Ember Operators API with graceful fallbacks.

This example shows how to create and compose operators using the simplified API,
demonstrating basic operators, ensemble patterns, and advanced composition techniques.
It follows the latest API patterns and includes proper error handling for when
API keys are not available.

Run this example with:

```bash
uv run python -m src.ember.examples.integration.api_operators_example
```

"""

import logging
from typing import ClassVar, List

from ember.api.models import ModelEnum, get_model_service, get_registry
from ember.api.operators import (
EnsembleOperator,
Field,
JudgeSynthesisOperator,
MostCommonAnswerSelector,
Operator,
Specification,
)
from ember.core.registry.model.model_module.lm import LMModule
from ember.core.types.ember_model import EmberModel

# Define input/output models

class QuestionInput(EmberModel):
"""Input model for question answering."""

    question: str = Field(..., description="The question to be answered")

class AnswerOutput(EmberModel):
"""Output model for question answering."""

    answer: str = Field(..., description="The answer to the question")
    confidence: float = Field(
        default=1.0, description="Confidence score for the answer"
    )

class MultipleAnswersOutput(EmberModel):
"""Output model with multiple candidate answers."""

    answers: List[str] = Field(..., description="Multiple candidate answers")

# Helper class for simulating models when API keys aren't available

class MockLMModule:
"""Simulates a language model without requiring API access."""

    def __init__(self, model_name: str, temperature: float = 0.7):
        """Initialize mock LM with model name and temperature.

        Args:
            model_name: Name of the model to simulate
            temperature: Simulated temperature parameter
        """
        self.model_name = model_name
        self.temperature = temperature
        # Dictionary of canned responses
        self.canned_responses = {
            "What is the meaning of life?": f"This is a simulated response from {model_name}: The answer is 42."
        }

    def __call__(self, *, prompt: str) -> str:
        """Generate a simulated response based on the prompt.

        Args:
            prompt: The input prompt text

        Returns:
            A simulated model response
        """
        # Check for synthesis prompts
        if "synthesize" in prompt.lower() or "multiple advisors" in prompt.lower():
            return (
                "Reasoning: All three perspectives agree on the value '42' as the answer to the meaning of life, "
                "though they approach it from different angles. The scientific perspective presents it as a universal constant, "
                "the philosophical perspective frames it as an existential truth, and the humorous perspective references "
                "Douglas Adams' 'Hitchhiker's Guide to the Galaxy'. This convergence across different domains suggests '42' "
                "is indeed the most comprehensive answer.\n\n"
                "Final Answer: The meaning of life is 42."
            )

        # Check for prompt prefixes for diverse answers
        for prefix in [
            "Scientific perspective:",
            "Philosophical perspective:",
            "Humorous perspective:",
        ]:
            if prefix in prompt:
                if "Scientific" in prefix:
                    return f"From a {prefix} The mathematical constant 42 appears in various equations describing the universe."
                elif "Philosophical" in prefix:
                    return f"From a {prefix} The meaning of life is to find your own purpose and create meaning through authentic choices."
                elif "Humorous" in prefix:
                    return f"From a {prefix} According to the Hitchhiker's Guide to the Galaxy, the answer is definitively 42."

        # Check for known questions
        for question, answer in self.canned_responses.items():
            if question.lower() in prompt.lower():
                return answer

        # Generic response
        return f"Simulated response from {self.model_name}: I don't have a specific answer for this question."

class ModelProvider:
"""Helper class to provide real or mock models based on availability."""

    @staticmethod
    def get_lm_module(model_enum: ModelEnum, temperature: float = 0.7) -> LMModule:
        """Get a real or mocked LM module based on model availability.

        Args:
            model_enum: The model enum to get
            temperature: Temperature setting for the model

        Returns:
            Either a real LM module or a mock LM module
        """
        registry = get_registry()

        # Try to get the real model
        try:
            if hasattr(model_enum, "value") and registry.is_registered(
                model_enum.value
            ):
                model_service = get_model_service()
                model = model_service.get_model(model_enum.value)
                model.temperature = temperature
                return model
        except Exception as e:
            logging.warning(f"Could not load model {model_enum}: {str(e)}")

        # Fall back to mock implementation
        model_name = model_enum.name if hasattr(model_enum, "name") else str(model_enum)
        logging.info(f"Using mock implementation for {model_name}")
        return MockLMModule(model_name=model_name, temperature=temperature)

# Basic operator example

class SimpleQuestionAnswerer(Operator[QuestionInput, AnswerOutput]):
"""A simple operator that answers questions using a language model."""

    specification: ClassVar[Specification[QuestionInput, AnswerOutput]] = Specification(
        input_model=QuestionInput, structured_output=AnswerOutput
    )

    # Field declarations
    lm_module: LMModule

    def __init__(self, *, model_enum: ModelEnum, temperature: float = 0.7):
        """Initialize the operator with model configuration.

        Args:
            model_enum: The model enum to use
            temperature: Sampling temperature for generation
        """
        self.lm_module = ModelProvider.get_lm_module(model_enum, temperature)

    def forward(self, *, inputs: QuestionInput) -> AnswerOutput:
        """Generate an answer to the input question.

        Args:
            inputs: The question input model

        Returns:
            Structured answer output
        """
        # Call the language model with the question
        response = self.lm_module(prompt=inputs.question)

        # Extract text content from response if it's not already a string
        if hasattr(response, "data"):
            response_text = response.data
        else:
            response_text = str(response)

        # Return structured output
        return AnswerOutput(answer=response_text, confidence=0.95)

# Diversification operator

class DiverseAnswerGenerator(Operator[QuestionInput, MultipleAnswersOutput]):
"""Generates multiple diverse answers to a question."""

    specification: ClassVar[
        Specification[QuestionInput, MultipleAnswersOutput]
    ] = Specification(
        input_model=QuestionInput, structured_output=MultipleAnswersOutput
    )

    # Field declarations
    prefixes: List[str]
    lm_module: LMModule

    def __init__(self, *, prefixes: List[str], model_enum: ModelEnum = ModelEnum.gpt_4):
        """Initialize with different prefixes to guide diverse responses.

        Args:
            prefixes: Different framing instructions to get diverse answers
            model_enum: The model enum to use
        """
        self.prefixes = prefixes
        self.lm_module = ModelProvider.get_lm_module(model_enum)

    def forward(self, *, inputs: QuestionInput) -> MultipleAnswersOutput:
        """Generate multiple diverse answers using different prefixes.

        Args:
            inputs: The question input

        Returns:
            Multiple answers output
        """
        answers = []

        for prefix in self.prefixes:
            # Prepend the prefix to the question
            prompt = f"{prefix} {inputs.question}"
            response = self.lm_module(prompt=prompt)

            # Extract text content from response if it's not already a string
            if hasattr(response, "data"):
                response_text = response.data
            else:
                response_text = str(response)

            answers.append(response_text)

        return MultipleAnswersOutput(answers=answers)

def main():
"""Run the example pipeline to demonstrate operator composition.""" # Configure logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    # Check if API keys are available
    registry = get_registry()
    if not registry.list_models():
        logging.warning(
            "No models were discovered. This example will use mock models instead of real ones."
        )
        logging.warning(
            "To use real models, set one of these environment variables: "
            "OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY"
        )

    # Create a question input
    question = QuestionInput(question="What is the meaning of life?")

    print(f"Question: {question.question}\n")

    # 1. Simple operator with GPT-4
    simple_answerer = SimpleQuestionAnswerer(model_enum=ModelEnum.gpt_4)
    result1 = simple_answerer(inputs=question)
    print("1. Simple Operator:")
    print(f"   Answer: {result1.answer}")
    print(f"   Confidence: {result1.confidence}\n")

    # Create a custom ensemble operator that handles response objects
    class CustomEnsembleOperator(EnsembleOperator):
        def forward(self, *, inputs: dict) -> dict:
            # Call the underlying models with the query
            raw_responses = [lm(prompt=inputs["query"]) for lm in self.lm_modules]

            # Process responses to extract text
            processed_responses = []
            for response in raw_responses:
                if hasattr(response, "data"):
                    processed_responses.append(response.data)
                else:
                    processed_responses.append(str(response))

            # Return the processed responses
            return {"responses": processed_responses}

    # Using fully typed model enums for clarity
    ensemble = CustomEnsembleOperator(
        lm_modules=[
            ModelProvider.get_lm_module(ModelEnum.gpt_4),
            ModelProvider.get_lm_module(ModelEnum.claude_3_5_sonnet),
            ModelProvider.get_lm_module(ModelEnum.gemini_1_5_pro),
        ]
    )

    result2 = ensemble(inputs={"query": question.question})

    print("2. Ensemble Operator:")
    for i, response in enumerate(result2["responses"], 1):
        print(f"   Model {i}: {response}")
    print()

    # 3. Ensemble with answer selection
    # Demonstrating direct invocation of the MostCommonAnswerSelector
    result3 = MostCommonAnswerSelector()(inputs={"responses": result2["responses"]})
    print("3. Ensemble with Most Common Answer Selector:")
    print(f"   Selected Answer: {result3['final_answer']}\n")

    # 4. Diverse answers with synthesis
    diverse_generator = DiverseAnswerGenerator(
        prefixes=[
            "Scientific perspective:",
            "Philosophical perspective:",
            "Humorous perspective:",
        ],
        model_enum=ModelEnum.gpt_4,
    )

    # Generate diverse answers
    diverse_results = diverse_generator(inputs=question)

    # Create a custom JudgeSynthesisOperator that handles response objects
    class CustomJudgeSynthesisOperator(JudgeSynthesisOperator):
        def forward(self, *, inputs: dict) -> dict:
            # Prepare synthesizer prompt
            question = inputs["query"]
            responses = inputs["responses"]

            # Build the prompt
            prompt = (
                f"I need you to synthesize the following perspectives on this question: '{question}'\n\n"
                "Multiple advisors have provided their views:\n\n"
            )

            for i, response in enumerate(responses, 1):
                prompt += f"Advisor {i}: {response}\n\n"

            prompt += (
                "Based on these perspectives, please provide:\n"
                "1. Your reasoning process, synthesizing the different viewpoints\n"
                "2. A final, balanced answer that represents the best synthesis\n\n"
                "Format your response like this:\n"
                "Reasoning: [your reasoning here]\n\n"
                "Final Answer: [your final answer here]"
            )

            # Get the response
            response = self.lm_module(prompt=prompt)

            # Extract the text from the response if it's not already a string
            if hasattr(response, "data"):
                response_text = response.data
            else:
                response_text = str(response)

            # Extract reasoning and final answer
            reasoning = ""
            final_answer = ""

            if "Reasoning:" in response_text and "Final Answer:" in response_text:
                parts = response_text.split("Final Answer:")
                if len(parts) >= 2:
                    reasoning_part = parts[0]
                    if "Reasoning:" in reasoning_part:
                        reasoning = reasoning_part.split("Reasoning:", 1)[1].strip()
                    final_answer = parts[1].strip()
            else:
                # Fallback if the expected format isn't found
                reasoning = "Could not extract reasoning"
                final_answer = response_text

            return {"reasoning": reasoning, "final_answer": final_answer}

    # Use the judge synthesis operator to synthesize the answers
    judge_lm = ModelProvider.get_lm_module(ModelEnum.claude_3_5_sonnet)
    synthesizer = CustomJudgeSynthesisOperator(lm_module=judge_lm)

    # Prepare the input for the synthesizer
    synthesis_input = {"query": question.question, "responses": diverse_results.answers}

    # Run the synthesizer
    result4 = synthesizer(inputs=synthesis_input)

    print("4. Diverse Answers with Synthesis:")
    print(f"   Synthesized Answer: {result4['final_answer']}")
    print(f"   Reasoning: {result4['reasoning']}\n")

if **name** == "**main**":
main()

</code>

src\ember\examples\integration\README.md:
<code>

# Ember Integration Examples

This directory contains examples demonstrating how to integrate Ember with other systems, libraries, and frameworks.

## Examples

- `api_operators_example.py` - Using Ember's API with custom operators

## Running Examples

To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/integration/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/integration/example_name.py
```

Replace `example_name.py` with the desired example file.

## Integration Concepts

These examples show how to:

- Integrate Ember with external APIs and services
- Use Ember as part of a larger system
- Connect Ember to data processing pipelines
- Extend Ember with custom functionality

## Next Steps

After exploring these integration examples, you may want to check out:

- `advanced/` - For advanced Ember usage patterns
- Contributing to Ember with your own extensions

</code>

src\ember\examples\integration\_\_init\_\_.py:
<code>

</code>

src\ember\examples\models\dependency_injection.py:
<code>
"""
Example: Dependency Injection with ModelContext

This example demonstrates how to use the dependency injection capabilities
of the new model API to isolate different execution contexts.

Key concepts:

1. Creating multiple contexts with different configurations
2. Using models with explicit context dependencies
3. Provider namespaces with contexts
4. Temporary configuration overrides
   """

import os
from typing import Dict, List

from ember.api.models import (
ContextConfig,
configure,
create_context,
create_provider_namespace,
model,
)

def create_isolated_contexts():
"""Create multiple isolated contexts with different configurations."""
print("\n=== Creating Isolated Contexts ===\n")

    # Create a context for production use
    prod_config = ContextConfig(
        auto_discover=True,
        api_keys={
            "openai": os.environ.get("OPENAI_API_KEY", "sk-prod-key"),
            "anthropic": os.environ.get("ANTHROPIC_API_KEY", "sk-prod-key"),
        },
        default_timeout=30,
    )
    prod_context = create_context(config=prod_config)

    # Create a context for testing
    test_config = ContextConfig(
        auto_discover=False,
        api_keys={
            "openai": "sk-test-key",
            "anthropic": "sk-test-key",
        },
        default_timeout=5,
    )
    test_context = create_context(config=test_config)

    # Use both contexts
    try:
        # This will use the production context (real API keys)
        prod_model = model("gpt-4o", context=prod_context)

        # This will use the test context (test API keys)
        test_model = model("gpt-4o", context=test_context)

        # Demonstrate the isolation - these will have different URLs and keys
        print(f"Production model: {prod_model.model_id}")
        print(f"Test model: {test_model.model_id}")

        # NOTE: Since we're using fake test keys, this would fail in a real setting
        # response = test_model("This is a test prompt")

    except Exception as e:
        print(f"Exception: {e}")

    print("\nContexts remain isolated - changing one doesn't affect the other")

def provider_namespaces_with_contexts():
"""Create provider namespaces with different contexts."""
print("\n=== Provider Namespaces with Contexts ===\n")

    # Create two contexts with different configurations
    context1 = create_context(
        config=ContextConfig(api_keys={"openai": "key1", "anthropic": "key1"})
    )

    context2 = create_context(
        config=ContextConfig(api_keys={"openai": "key2", "anthropic": "key2"})
    )

    # Create provider namespaces with the contexts
    openai1 = create_provider_namespace("openai", context=context1)
    openai2 = create_provider_namespace("openai", context=context2)

    # The models from these namespaces will use different contexts
    model1 = openai1.gpt4o
    model2 = openai2.gpt4o

    print("Model 1 using context 1")
    print("Model 2 using context 2")

    print("\nEach model will use its own isolated key and configuration")

def configuration_contexts():
"""Demonstrate the use of configuration contexts."""
print("\n=== Configuration Contexts ===\n")

    # Create a model with default config
    gpt4 = model("gpt-4")

    print("Default configuration:")
    print(f"Temperature: {gpt4.config.get('temperature', 0.7)}")

    # Temporarily override configuration
    with configure(temperature=0.2, max_tokens=100):
        print("\nWith configure() context manager:")
        print(f"Temperature: {gpt4.config.get('temperature', 0.7)}")

        # The override is only for this context
        with configure(temperature=0.9):
            print("\nNested configure() context:")
            print(f"Temperature: {gpt4.config.get('temperature', 0.7)}")

        print("\nBack to first configure() context:")
        print(f"Temperature: {gpt4.config.get('temperature', 0.7)}")

    print("\nBack to default configuration:")
    print(f"Temperature: {gpt4.config.get('temperature', 0.7)}")

def simulate_ab_testing():
"""Simulate A/B testing with different model configurations."""
print("\n=== Simulating A/B Testing ===\n")

    # Create contexts for A/B testing
    context_a = create_context(
        config=ContextConfig(
            api_keys={"openai": "key-a", "anthropic": "key-a"}, auto_discover=True
        )
    )

    context_b = create_context(
        config=ContextConfig(
            api_keys={"openai": "key-b", "anthropic": "key-b"}, auto_discover=True
        )
    )

    # Create models with different contexts and configurations
    model_a = model("gpt-4o", context=context_a, temperature=0.5)
    model_b = model("claude-3-5-sonnet", context=context_b, temperature=0.7)

    # Function to simulate running experiments
    def run_experiment(prompt: str, models: Dict[str, callable], n_trials: int = 3):
        """Run an experiment with multiple models."""
        results: Dict[str, List[str]] = {name: [] for name in models}

        for trial in range(n_trials):
            for name, model_fn in models.items():
                print(f"Trial {trial+1}: Running experiment with {name}")
                # In a real setting, we would call the model
                # response = model_fn(prompt)
                # results[name].append(str(response))

        return results

    # Run the experiment
    experiment = run_experiment(
        prompt="Explain the benefits of quantum computing in three sentences.",
        models={"GPT-4o": model_a, "Claude-3.5": model_b},
    )

    print("\nExperiment completed, contexts remained isolated")

def main():
"""Run all examples."""
print("=== Model Context and Dependency Injection Examples ===")

    create_isolated_contexts()
    provider_namespaces_with_contexts()
    configuration_contexts()
    simulate_ab_testing()

    print("\n=== End of Examples ===")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\function_style_api.py:
<code>
"""
Example: Function-Style Model API

This example demonstrates the new function-style API for interacting with models.
This API is designed to be more intuitive, easier to use, and more aligned with
Python's functional programming style.

Key concepts:

1. Function-style model creation and invocation
2. Provider namespaces for direct model access
3. Configuration management
4. Response handling
   """

from ember.api.models import Response, model

def basic_usage():
"""Demonstrate basic usage of the function-style API."""
print("\n=== Basic Usage ===\n")

    # The most direct way to use a model - creation and invocation in one line
    try:
        # Note: This will fail without actual API keys
        # response = model("gpt-4o")("What is the capital of France?")
        # print(f"Response: {response}")

        # Simulation for demonstration purposes
        print('model("gpt-4o")("What is the capital of France?")')
        print("Response: Paris is the capital of France.")
    except Exception as e:
        print(f"Note: This would fail without actual API keys: {e}")

    # Create a reusable model instance
    gpt4 = model("gpt-4o")

    # Use it multiple times
    print("\nReusable model instance:")
    print('gpt4 = model("gpt-4o")')
    print('gpt4("Tell me a joke")')
    print('gpt4("Explain quantum computing")')

def provider_namespaces():
"""Demonstrate the use of provider namespaces."""
print("\n=== Provider Namespaces ===\n")

    # Provider namespaces provide direct access to models
    try:
        # Note: This will fail without actual API keys
        # response = openai.gpt4o("What is the capital of France?")
        # print(f"Response: {response}")

        # Simulation for demonstration purposes
        print('openai.gpt4o("What is the capital of France?")')
        print("Response: Paris is the capital of France.")

        print("\nUsing other providers:")
        print('anthropic.claude("Tell me about the Roman Empire")')
        print('deepmind.gemini("Explain quantum mechanics")')
    except Exception as e:
        print(f"Note: This would fail without actual API keys: {e}")

    # The provider namespaces handle model name normalization
    print("\nModel name normalization:")
    print("openai.gpt4_o == openai.gpt4o  # Underscores converted to hyphens")

def configuration_management():
"""Demonstrate configuration management."""
print("\n=== Configuration Management ===\n")

    # Set configuration during model creation
    print("Model with configuration:")
    print('gpt4 = model("gpt-4o", temperature=0.7, max_tokens=100)')

    # Global configuration (for all models)

    print("\nGlobal configuration:")
    print("config.temperature = 0.5")
    print("config.update(max_tokens=200, top_p=0.8)")

    # Temporary configuration using context manager
    print("\nTemporary configuration:")
    print("with configure(temperature=0.2, max_tokens=50):")
    print('    response = model("gpt-4o")("Write a short poem")')

    # Configuration hierarchy
    print("\nConfiguration hierarchy (order of precedence):")
    print("1. Call-specific arguments: model(...)(prompt, temperature=0.1)")
    print("2. Model instance config: model(..., temperature=0.2)(prompt)")
    print("3. Temporary config: with configure(temperature=0.3): ...")
    print("4. Global config: config.temperature = 0.4")

def response_handling():
"""Demonstrate response handling."""
print("\n=== Response Handling ===\n")

    # Creating a simulated response for demonstration
    class SimulatedResponse:
        def __init__(self):
            self.data = "This is a simulated response."
            self.usage = type(
                "Usage",
                (),
                {
                    "prompt_tokens": 10,
                    "completion_tokens": 20,
                    "total_tokens": 30,
                    "cost": 0.0012,
                },
            )

    # Wrap the simulated response
    response = Response(SimulatedResponse())

    # String conversion
    print(f"String conversion: {response}")

    # Accessing metadata
    print("\nAccessing metadata:")
    print(f"Model ID: {response.model_id}")
    print(f"Total tokens: {response.usage.total_tokens}")
    print(f"Prompt tokens: {response.usage.prompt_tokens}")
    print(f"Completion tokens: {response.usage.completion_tokens}")
    print(f"Cost: ${response.usage.cost:.6f}")

    # Visualization (simplified)
    print("\nVisualization:")
    print("response.visualize()  # Displays a rich representation in notebooks")

def complete_function():
"""Demonstrate the complete() function."""
print("\n=== Complete Function ===\n")

    # The complete() function is a convenience function for one-off completions
    print("Using complete() function:")
    print('complete("What is 2+2?", model="gpt-4o", temperature=0.5)')
    print("Response: 4")

    # It's equivalent to:
    print("\nEquivalent to:")
    print('model("gpt-4o", temperature=0.5)("What is 2+2?")')

    # It's useful when you want to prioritize the prompt in the code:
    print("\nUseful when prioritizing the prompt:")
    print("result = complete(")
    print('    "Explain the significance of the year 1969 in space exploration.",')
    print('    model="gpt-4o",')
    print("    temperature=0.7,")
    print("    max_tokens=200")
    print(")")

def comparing_api_styles():
"""Compare the function-style API with the object-oriented API."""
print("\n=== Comparing API Styles ===\n")

    # Function-style API (recommended)
    print("Function-style API (recommended):")
    print("from ember.api.models import model")
    print('response = model("gpt-4o")("What is the capital of France?")')

    # Object-oriented API (for backward compatibility)
    print("\nObject-oriented API (backward compatibility):")
    print("from ember.api.models import ModelAPI")
    print('api = ModelAPI("gpt-4o")')
    print('response = api.generate("What is the capital of France?")')

    # Builder pattern (for backward compatibility)
    print("\nBuilder pattern (backward compatibility):")
    print("from ember.api.models import ModelBuilder")
    print('model = ModelBuilder().temperature(0.7).max_tokens(100).build("gpt-4o")')
    print('response = model.generate("What is the capital of France?")')

    # Function-style API benefits
    print("\nFunction-style API benefits:")
    print("1. More intuitive and natural")
    print("2. More consistent with Python's functional programming style")
    print("3. Cleaner and more concise code")
    print("4. More flexible configuration management")
    print("5. Better IDE assistance with type hints")

def main():
"""Run all examples."""
print("=== Function-Style Model API Examples ===")

    basic_usage()
    provider_namespaces()
    configuration_management()
    response_handling()
    complete_function()
    comparing_api_styles()

    print("\n=== End of Examples ===")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\list_models.py:
<code>
"""Ember Model Discovery Example

This script demonstrates how to list available models in the Ember registry
using the simplified API. The script shows how to check for model availability
and retrieve model information.

IMPORTANT: Model pricing and context window information must be manually configured!
When models are discovered via API, they DO NOT include pricing or context window
information automatically. You must:

1. Add this information in your config.yaml file in the project root:

   ```yaml
   model_registry:
     providers:
       openai:
         models:
           - id: "gpt-4o"
             name: "GPT-4o"
             context_window: 128000 # <-- Add this for context window
             cost:
               input_cost_per_thousand: 5.0
               output_cost_per_thousand: 15.0
   ```

2. Or register models with complete information in code using the ModelInfo class
   as shown in the register_openai_models() and register_anthropic_models() functions below.

See model pricing and specifications:
https://docs.anthropic.com/en/docs/about-claude/models/all-models
https://openai.com/api/pricing/

To run:
uv run python src/ember/examples/models/list_models.py
"""

import logging
import os
from typing import List

from prettytable import PrettyTable

from ember.api import models

# Setup logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

# Initialize the registry with auto_discover=True

# Using the get_registry() function will initialize the registry if needed

logger.info("Initializing registry with auto_discover=True...")
registry = models.get_registry()

# Check if models were discovered during initialization

model_ids = registry.list_models()
if model_ids:
logger.info(
f"Successfully discovered {len(model_ids)} models during initialization: {model_ids}"
)
else:
logger.info(
"No models discovered during initialization, attempting manual discovery..."
)

    # Try explicit discovery
    discovered_models = registry.discover_models()

    if discovered_models:
        logger.info(
            f"Manual discovery found {len(discovered_models)} models: {discovered_models}"
        )
    else:
        logger.info("No models discovered, falling back to manual registration")

def register_openai_models():
"""Register OpenAI models with the registry."""
openai_key = os.environ.get("OPENAI_API_KEY")
if not openai_key:
logger.warning("OPENAI_API_KEY not set, skipping OpenAI model registration")
return []

    # Create the models
    model_infos = [
        models.ModelInfo(
            id="openai:gpt-4o",
            name="GPT-4o",
            context_window=128000,
            cost=models.ModelCost(
                input_cost_per_thousand=0.005, output_cost_per_thousand=0.015
            ),
            rate_limit=models.RateLimit(
                tokens_per_minute=10000000, requests_per_minute=1500
            ),
            provider={
                "name": "OpenAI",
                "default_api_key": openai_key,
                "base_url": "https://api.openai.com/v1",
            },
        ),
        models.ModelInfo(
            id="openai:gpt-4o-mini",
            name="GPT-4o Mini",
            context_window=128000,
            cost=models.ModelCost(
                input_cost_per_thousand=0.00015, output_cost_per_thousand=0.0006
            ),
            rate_limit=models.RateLimit(
                tokens_per_minute=10000000, requests_per_minute=1500
            ),
            provider={
                "name": "OpenAI",
                "default_api_key": openai_key,
                "base_url": "https://api.openai.com/v1",
            },
        ),
    ]

    # Register the models, skipping any that are already registered
    registered_models = []
    for model_info in model_infos:
        if not registry.is_registered(model_info.id):
            try:
                registry.register_model(model_info=model_info)
                logger.info(f"Registered model: {model_info.id}")
                registered_models.append(model_info.id)
            except ValueError:
                logger.info(f"Model {model_info.id} already registered, skipping")
        else:
            logger.info(f"Model {model_info.id} already registered, skipping")

    return registered_models

def register_anthropic_models():
"""Register Anthropic models with the registry."""
anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
if not anthropic_key:
logger.warning(
"ANTHROPIC_API_KEY not set, skipping Anthropic model registration"
)
return []

    # Create the models
    model_infos = [
        models.ModelInfo(
            id="anthropic:claude-3-5-sonnet",
            name="Claude 3.5 Sonnet",
            context_window=200000,
            cost=models.ModelCost(
                input_cost_per_thousand=0.003, output_cost_per_thousand=0.015
            ),
            rate_limit=models.RateLimit(
                tokens_per_minute=5000000, requests_per_minute=1000
            ),
            provider={
                "name": "Anthropic",
                "default_api_key": anthropic_key,
                "base_url": "https://api.anthropic.com/v1",
            },
        ),
        models.ModelInfo(
            id="anthropic:claude-3-opus",
            name="Claude 3 Opus",
            context_window=200000,
            cost=models.ModelCost(
                input_cost_per_thousand=0.015, output_cost_per_thousand=0.075
            ),
            rate_limit=models.RateLimit(
                tokens_per_minute=5000000, requests_per_minute=1000
            ),
            provider={
                "name": "Anthropic",
                "default_api_key": anthropic_key,
                "base_url": "https://api.anthropic.com/v1",
            },
        ),
    ]

    # Register the models, skipping any that are already registered
    registered_models = []
    for model_info in model_infos:
        if not registry.is_registered(model_info.id):
            try:
                registry.register_model(model_info=model_info)
                logger.info(f"Registered model: {model_info.id}")
                registered_models.append(model_info.id)
            except ValueError:
                logger.info(f"Model {model_info.id} already registered, skipping")
        else:
            logger.info(f"Model {model_info.id} already registered, skipping")

    return registered_models

# Register models manually as a fallback

register_openai_models()
register_anthropic_models()

def check_api_keys():
"""Check if API keys are set in environment variables."""
openai_key = os.environ.get("OPENAI_API_KEY")
anthropic_key = os.environ.get("ANTHROPIC_API_KEY")

    if openai_key:
        logger.info("OPENAI_API_KEY is set")
    else:
        logger.warning("OPENAI_API_KEY is not set")

    if anthropic_key:
        logger.info("ANTHROPIC_API_KEY is set")
    else:
        logger.warning("ANTHROPIC_API_KEY is not set")

def list_available_models():
"""List available models in the registry using the new API.

    With the new API, model discovery happens automatically.
    """
    logger.info("Listing available models...")

    # Get all available models
    model_ids = registry.list_models()

    # Create a table for display
    table = PrettyTable()
    table.field_names = [
        "Provider",
        "Model ID",
        "Context Window",
        "Input Cost",
        "Output Cost",
    ]
    table.align = "l"

    # Group by provider
    providers = {}
    for model_id in model_ids:
        if ":" in model_id:
            provider, name = model_id.split(":", 1)
            if provider not in providers:
                providers[provider] = []
            providers[provider].append(model_id)
        else:
            # Handle models without provider prefix
            if "other" not in providers:
                providers["other"] = []
            providers["other"].append(model_id)

    # Print models by provider
    logger.info(f"Found {len(model_ids)} models across {len(providers)} providers")

    # Add models to table
    for provider, ids in sorted(providers.items()):
        for model_id in sorted(ids):
            try:
                info = registry.get_model_info(model_id)
                table.add_row(
                    [
                        provider,
                        model_id,
                        f"{info.context_window if hasattr(info, 'context_window') else 'N/A'}",
                        (
                            f"${info.cost.input_cost_per_thousand:.4f}"
                            if hasattr(info, "cost") and info.cost
                            else "N/A"
                        ),
                        (
                            f"${info.cost.output_cost_per_thousand:.4f}"
                            if hasattr(info, "cost") and info.cost
                            else "N/A"
                        ),
                    ]
                )
            except Exception:
                table.add_row([provider, model_id, "Error", "Error", "Error"])

    logger.info(f"Model table:\n{table}")

def check_specific_models(model_ids: List[str]):
"""Check if specific models are available in the registry.

    Args:
        model_ids: List of model IDs to check
    """
    logger.info("Checking specific models:")
    for model_id in model_ids:
        exists = registry.is_registered(model_id)
        if exists:
            info = registry.get_model_info(model_id)
            logger.info(f"✅ Model '{model_id}' is available")
            logger.info(
                f"   - Provider: {info.provider.name if hasattr(info.provider, 'name') else 'Unknown'}"
            )
            if hasattr(info, "cost") and info.cost:
                logger.info(
                    f"   - Input cost: ${info.cost.input_cost_per_thousand:.4f} per 1K tokens"
                )
                logger.info(
                    f"   - Output cost: ${info.cost.output_cost_per_thousand:.4f} per 1K tokens"
                )
        else:
            logger.warning(f"❌ Model '{model_id}' is not available")

def main():
"""Run the model discovery example."""
logger.info("=== Ember Model Discovery Example ===")

    # Check if API keys are set
    check_api_keys()

    # List all available models
    list_available_models()

    # Check specific models
    check_specific_models(
        [
            "openai:gpt-4o",
            "openai:gpt-4o-mini",
            "anthropic:claude-3-5-sonnet",
            "anthropic:claude-3-opus",
        ]
    )

    # Example of the simpler usage pattern
    logger.info("Using simpler direct model identification:")
    logger.info("To check if a model exists: registry.is_registered('openai:gpt-4o')")
    logger.info("To get model info: registry.get_model_info('openai:gpt-4o')")
    logger.info("To use a model: model_service = models.create_model_service(registry)")
    logger.info(
        "               model_service.invoke_model('openai:gpt-4o', 'What is the capital of France?')"
    )

    logger.info("Example completed!")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\manual_model_registration.py:
<code>
"""Manual Model Registration Example

This example demonstrates how to manually register models with the ModelRegistry
using the new simplified API.

IMPORTANT: Model pricing and context window information must be manually configured!
When models are discovered via API, they DO NOT include pricing or context window
information automatically. You must:

1. Add this information in your config.yaml file in the project root:

   ```yaml
   model_registry:
     providers:
       openai:
         models:
           - id: "gpt-4o"
             name: "GPT-4o"
             context_window: 128000 # <-- Add this for context window
             cost:
               input_cost_per_thousand: 5.0
               output_cost_per_thousand: 15.0
   ```

2. Or register models with complete information in code using the ModelInfo class
   as shown in the register_openai_models() and register_anthropic_models() functions below.

See model pricing and specifications:
https://docs.anthropic.com/en/docs/about-claude/models/all-models
https://openai.com/api/pricing/

To run:
uv run python src/ember/examples/models/manual_model_registration.py

    # Or if in an activated virtual environment
    python src/ember/examples/models/manual_model_registration.py

Required environment variables:
OPENAI_API_KEY (optional): Your OpenAI API key for registering OpenAI models
ANTHROPIC_API_KEY (optional): Your Anthropic API key for registering Anthropic models
"""

import logging
import os
from typing import List

from prettytable import PrettyTable

from ember.api import models
from ember.api.models import ModelCost, ModelInfo, ProviderInfo, RateLimit

# Set up logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

def register_models() -> List[str]:
"""Register models manually with the registry.

    Returns:
        List of registered model IDs
    """
    # Get API keys from environment
    openai_api_key = os.environ.get("OPENAI_API_KEY")
    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY")

    # Get the registry
    registry = models.get_registry()

    registered_models = []

    if openai_api_key:
        # Register OpenAI models
        # Register GPT-4o
        gpt4o_info = ModelInfo(
            id="openai:gpt-4o",
            name="GPT-4o",
            context_window=128000,
            cost=ModelCost(
                input_cost_per_thousand=0.005,  # $0.005 per 1K input tokens
                output_cost_per_thousand=0.015,  # $0.015 per 1K output tokens
            ),
            rate_limit=RateLimit(
                tokens_per_minute=10000000,  # Up to 10 million tokens per minute
                requests_per_minute=1500,  # Tier 5 rate limit
            ),
            provider=ProviderInfo(
                name="OpenAI",
                default_api_key=openai_api_key,
                base_url="https://api.openai.com/v1",
            ),
        )
        if not registry.is_registered("openai:gpt-4o"):
            try:
                registry.register_model(model_info=gpt4o_info)
                logger.info("Registered model: openai:gpt-4o")
            except ValueError as e:
                logger.info(f"Model registration error: {e}")
        else:
            logger.info("Model openai:gpt-4o already registered, skipping")

        registered_models.append("openai:gpt-4o")

        # Register GPT-4o-mini
        gpt4o_mini_info = ModelInfo(
            id="openai:gpt-4o-mini",
            name="GPT-4o Mini",
            context_window=128000,
            cost=ModelCost(
                input_cost_per_thousand=0.00015,  # $0.00015 per 1K input tokens
                output_cost_per_thousand=0.0006,  # $0.0006 per 1K output tokens
            ),
            rate_limit=RateLimit(tokens_per_minute=10000000, requests_per_minute=1500),
            provider=ProviderInfo(
                name="OpenAI",
                default_api_key=openai_api_key,
                base_url="https://api.openai.com/v1",
            ),
        )
        if not registry.is_registered("openai:gpt-4o-mini"):
            try:
                registry.register_model(model_info=gpt4o_mini_info)
                logger.info("Registered model: openai:gpt-4o-mini")
            except ValueError as e:
                logger.info(f"Model registration error: {e}")
        else:
            logger.info("Model openai:gpt-4o-mini already registered, skipping")

        registered_models.append("openai:gpt-4o-mini")
    else:
        logger.warning("OPENAI_API_KEY not found in environment variables")

    if anthropic_api_key:
        # Register Anthropic models
        # Register Claude 3.5 Sonnet
        claude_sonnet_info = ModelInfo(
            id="anthropic:claude-3-5-sonnet",
            name="Claude 3.5 Sonnet",
            context_window=200000,
            cost=ModelCost(
                input_cost_per_thousand=0.003,  # $0.003 per 1K input tokens
                output_cost_per_thousand=0.015,  # $0.015 per 1K output tokens
            ),
            rate_limit=RateLimit(tokens_per_minute=5000000, requests_per_minute=1000),
            provider=ProviderInfo(
                name="Anthropic",
                default_api_key=anthropic_api_key,
                base_url="https://api.anthropic.com/v1",
            ),
        )
        if not registry.is_registered("anthropic:claude-3-5-sonnet"):
            try:
                registry.register_model(model_info=claude_sonnet_info)
                logger.info("Registered model: anthropic:claude-3-5-sonnet")
            except ValueError as e:
                logger.info(f"Model registration error: {e}")
        else:
            logger.info(
                "Model anthropic:claude-3-5-sonnet already registered, skipping"
            )

        registered_models.append("anthropic:claude-3-5-sonnet")
    else:
        logger.warning("ANTHROPIC_API_KEY not found in environment variables")

    # Log all registered models
    logger.info(f"Registered models: {registered_models}")

    return registered_models

def display_models():
"""Display information about the registered models in a formatted table."""
registry = models.get_registry()
model_ids = registry.list_models()

    if not model_ids:
        print("No models found in the registry.")
        return

    # Create a table for displaying model information
    table = PrettyTable()
    table.field_names = [
        "Model ID",
        "Provider",
        "Context Window",
        "Input Cost",
        "Output Cost",
    ]
    table.align = "l"

    # Group by provider
    providers = {}
    for model_id in model_ids:
        if ":" in model_id:
            provider, name = model_id.split(":", 1)
            if provider not in providers:
                providers[provider] = []
            providers[provider].append(model_id)
        else:
            # Handle models without provider prefix
            if "other" not in providers:
                providers["other"] = []
            providers["other"].append(model_id)

    # Add models to table
    for provider, ids in sorted(providers.items()):
        for model_id in sorted(ids):
            try:
                info = registry.get_model_info(model_id)
                provider_name = provider
                if hasattr(info, "provider"):
                    if isinstance(info.provider, dict):
                        provider_name = info.provider.get("name", provider)
                    elif hasattr(info.provider, "name"):
                        provider_name = info.provider.name
                context_window = (
                    info.context_window if hasattr(info, "context_window") else "N/A"
                )

                input_cost = (
                    f"${info.cost.input_cost_per_thousand:.4f}/1K"
                    if hasattr(info, "cost") and info.cost
                    else "N/A"
                )
                output_cost = (
                    f"${info.cost.output_cost_per_thousand:.4f}/1K"
                    if hasattr(info, "cost") and info.cost
                    else "N/A"
                )

                table.add_row(
                    [model_id, provider_name, context_window, input_cost, output_cost]
                )
            except Exception as e:
                table.add_row([model_id, f"Error: {str(e)[:20]}...", "", "", ""])

    print("\nRegistered Models:")
    print(table)

def main():
"""Run the manual model registration example."""
print("\n=== Manual Model Registration Example ===\n")

    # Display models before registration
    print("Models before registration:")
    display_models()

    # Register models
    registered_model_ids = register_models()

    # Display updated models
    print("\nModels after registration:")
    display_models()

    # Try to use one of the registered models
    if "openai:gpt-4o" in registered_model_ids:
        try:
            print("\n=== Using a registered model ===")
            model_service = models.get_model_service()
            response = model_service.invoke_model(
                "openai:gpt-4o", "What is the capital of France?"
            )
            print("Query: What is the capital of France?")
            print(f"Response: {response.data[:150]}...")
            print(f"Usage stats: {response.usage}")
        except Exception as e:
            logger.error(f"Error using model: {e}")

    # Example code for using a model
    print("\nTo use a registered model with the API:")
    print("from ember.api import models")
    print("# Using the model service")
    print("model_service = models.get_model_service()")
    print(
        'response = model_service.invoke_model("openai:gpt-4o", "What is the capital of France?")'
    )
    print("print(response.data)")
    print("\n# Or using the provider namespace (preferred)")
    print('response = models.openai.gpt4o("What is the capital of France?")')
    print("print(response.data)")

    print("\nExample completed!")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\model_api_example.py:
<code>
"""
Example demonstrating the Ember Models API.

This file shows how to use the models API to initialize and interact
with language models from different providers.

To run:
uv run python src/ember/examples/models/model_api_example.py

    # Or if in an activated virtual environment
    python src/ember/examples/models/model_api_example.py

Required environment variables:
OPENAI_API_KEY (optional): Your OpenAI API key for OpenAI model examples
ANTHROPIC_API_KEY (optional): Your Anthropic API key for Anthropic model examples
"""

import logging
import os

# Configure logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(**name**)

# Import from the simplified API

from ember.api.models import (
ModelCost,
ModelInfo,
ModelService,
RateLimit,
UsageService,
initialize_registry,
)

def registry_setup_example():
"""Demonstrate how to initialize the model registry."""
print("\n=== Registry Initialization Example ===")

    # Initialize the registry with auto-discovery
    registry = initialize_registry(auto_discover=True)

    # List all discovered models
    model_ids = registry.list_models()
    print(f"Discovered {len(model_ids)} models:")

    # Show first 5 models (if available)
    for model_id in model_ids[:5]:
        print(f"  - {model_id}")

    if len(model_ids) > 5:
        print(f"  ... and {len(model_ids) - 5} more")

    return registry

def model_service_example(registry):
"""Demonstrate using ModelService to invoke models."""
print("\n=== Model Service Example ===")

    # Create a usage service for tracking
    usage_service = UsageService()

    # Create a model service with the registry and usage tracking
    model_service = ModelService(registry=registry, usage_service=usage_service)

    # Using OpenAI's model
    try:
        if registry.is_registered("openai:gpt-4"):
            info = registry.get_model_info("openai:gpt-4")
            print(f"Model info found for openai:gpt-4: {info.name}")

            # Get model directly using fixed parse_model_str
            model = registry.get_model("openai:gpt-4")
            if model:
                print(f"Successfully retrieved model: {model.model_info.id}")

                # We would invoke it like this in a real application
                print("In a real application, you would invoke the model with:")
                print(
                    "response = model_service.invoke_model('openai:gpt-4', 'What is the capital of France?')"
                )

                # Avoiding actual API call to save credits
                print("(API call skipped in this example to avoid using API credits)")
            else:
                print("Model retrieval failed.")
        else:
            print("Model not found in registry.")

    except Exception as e:
        print(f"Error using model service: {e}")

    return model_service, usage_service

def direct_model_example(registry):
"""Demonstrate getting and using a model directly."""
print("\n=== Direct Model Example ===")

    try:
        # Check if model is registered
        if registry.is_registered("anthropic:claude-3-sonnet"):
            info = registry.get_model_info("anthropic:claude-3-sonnet")
            print(f"Model info found for anthropic:claude-3-sonnet: {info.name}")

            # Get model directly
            model = registry.get_model("anthropic:claude-3-sonnet")
            if model:
                print(f"Successfully retrieved model: {model.model_info.id}")

                # We could call the model directly like this
                print("In a real application, you would call the model directly with:")
                print("response = model('Explain quantum computing in simple terms.')")

                # Avoiding actual API call to save credits
                print("(API call skipped in this example to avoid using API credits)")
            else:
                print("Model retrieval failed.")
        else:
            print("Model not found in registry.")
    except Exception as e:
        print(f"Error using model directly: {e}")

def model_metadata_example(registry):
"""Demonstrate accessing model metadata."""
print("\n=== Model Metadata Example ===")

    try:
        # Get metadata for specific models
        for model_id in ["openai:gpt-4", "anthropic:claude-3-sonnet"]:
            if registry.is_registered(model_id):
                info = registry.get_model_info(model_id)
                if info:
                    print(f"\nModel: {model_id}")
                    print(f"  Name: {info.name}")
                    print(f"  Provider: {info.provider.name}")
                    print(
                        f"  Input cost: ${info.cost.input_cost_per_thousand:.4f} per 1K tokens"
                    )
                    print(
                        f"  Output cost: ${info.cost.output_cost_per_thousand:.4f} per 1K tokens"
                    )
            else:
                print(f"Model {model_id} not found in registry")
    except Exception as e:
        print(f"Error accessing model metadata: {e}")

def usage_tracking_example(model_service, usage_service):
"""Demonstrate usage tracking capabilities."""
print("\n=== Usage Tracking Example ===")

    try:
        # Demonstration of usage tracking
        print("Usage tracking works with model invocations:")
        print("1. Make model calls with model_service.invoke_model()")
        print("2. UsageService automatically records token usage")
        print("3. Get usage statistics with usage_service.get_usage_summary()")

        # In a real application with actual API calls, you would see non-zero usage
        print(
            "\nIn this example, we skip actual API calls but demonstrate the tracking API"
        )

        # Retrieve usage summary for demonstration
        model_id = "openai:gpt-4"
        usage_summary = usage_service.get_usage_summary(model_id=model_id)

        print("\nUsage Summary:")
        print(f"  Model: {usage_summary.model_name}")
        print(f"  Total tokens: {usage_summary.total_tokens_used}")
        print(f"  Prompt tokens: {usage_summary.total_usage.prompt_tokens}")
        print(f"  Completion tokens: {usage_summary.total_usage.completion_tokens}")
        print(f"  Estimated cost: ${usage_summary.total_usage.cost_usd:.4f}")

        # Get summaries for all registered models
        print("\nAll Models Usage:")
        # Get the model registry's registered models
        registry = model_service._registry
        for model_id in registry.list_models():
            try:
                summary = usage_service.get_usage_summary(model_id=model_id)
                if summary.total_tokens_used > 0:
                    print(
                        f"  {model_id}: {summary.total_tokens_used} tokens, ${summary.total_usage.cost_usd:.4f}"
                    )
            except Exception as e:
                print(f"  Error getting usage for {model_id}: {e}")

    except Exception as e:
        print(f"Error tracking usage: {e}")

def custom_model_example(registry):
"""Demonstrate registering a custom model."""
print("\n=== Custom Model Registration Example ===")

    try:
        # Create custom model info
        custom_model = ModelInfo(
            id="custom:my-custom-model",
            name="My Custom LLM",
            cost=ModelCost(
                input_cost_per_thousand=0.0005, output_cost_per_thousand=0.0015
            ),
            rate_limit=RateLimit(tokens_per_minute=100000, requests_per_minute=3000),
            provider={
                "name": "CustomAI",
                "base_url": "https://api.custom-ai.example.com/v1",
                "default_api_key": "${CUSTOM_API_KEY}",
            },
        )

        # Register the model
        registry.register_model(model_info=custom_model)
        print(f"Model {custom_model.id} registered successfully")

        # Verify it's in the registry
        if registry.is_registered(custom_model.id):
            info = registry.get_model_info(custom_model.id)
            print(f"Confirmed model in registry: {info.id} ({info.name})")
            print(f"Provider: {info.provider.name}")
        else:
            print("Custom model registration failed")
    except Exception as e:
        print(f"Error registering custom model: {e}")

def register_models(registry):
"""Register test models with the registry."""
registered_count = 0

    # Register OpenAI models
    openai_key = os.environ.get("OPENAI_API_KEY")
    if openai_key:
        # Create the models
        model_infos = [
            ModelInfo(
                id="openai:gpt-4",
                name="GPT-4",
                context_window=128000,
                cost=ModelCost(
                    input_cost_per_thousand=0.01, output_cost_per_thousand=0.03
                ),
                rate_limit=RateLimit(
                    tokens_per_minute=10000000, requests_per_minute=1000
                ),
                provider={
                    "name": "OpenAI",
                    "default_api_key": openai_key,
                    "base_url": "https://api.openai.com/v1",
                },
            )
        ]

        # Register the models, but check if they're already registered first
        for model_info in model_infos:
            if not registry.is_registered(model_info.id):
                registry.register_model(model_info=model_info)
                print(f"Registered model: {model_info.id}")
                registered_count += 1
            else:
                print(
                    f"Model {model_info.id} already registered ✅ - using existing registration"
                )

    # Register Anthropic models
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    if anthropic_key:
        model_infos = [
            ModelInfo(
                id="anthropic:claude-3-sonnet",
                name="Claude 3 Sonnet",
                context_window=200000,
                cost=ModelCost(
                    input_cost_per_thousand=0.003, output_cost_per_thousand=0.015
                ),
                rate_limit=RateLimit(
                    tokens_per_minute=5000000, requests_per_minute=1000
                ),
                provider={
                    "name": "Anthropic",
                    "default_api_key": anthropic_key,
                    "base_url": "https://api.anthropic.com/v1",
                },
            )
        ]

        # Register the models, but check if they're already registered first
        for model_info in model_infos:
            if not registry.is_registered(model_info.id):
                registry.register_model(model_info=model_info)
                print(f"Registered model: {model_info.id}")
                registered_count += 1
            else:
                print(
                    f"Model {model_info.id} already registered ✅ - using existing registration"
                )

    # Return the number of newly registered models
    return registered_count

def main():
"""Run all examples in sequence.""" # Check for API keys
if not os.environ.get("OPENAI_API_KEY"):
print("Warning: OPENAI_API_KEY not set. OpenAI examples will fail.")

    if not os.environ.get("ANTHROPIC_API_KEY"):
        print("Warning: ANTHROPIC_API_KEY not set. Anthropic examples will fail.")

    print("Running Models API examples...")

    # Example 1: Initialize registry
    registry = registry_setup_example()

    # Register models manually
    print("\nRegistering models manually:")
    register_models(registry)

    # Example 2: Model service
    model_service, usage_service = model_service_example(registry)

    # Example 3: Direct model access
    direct_model_example(registry)

    # Example 4: Model metadata
    model_metadata_example(registry)

    # Example 5: Usage tracking
    usage_tracking_example(model_service, usage_service)

    # Example 6: Custom model
    custom_model_example(registry)

    print("\nAll examples completed.")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\model_registry_direct.py:
<code>
"""
Direct Model Registry Example with environment variable API keys.

This example demonstrates how to directly use the model registry with API keys
from environment variables.

To run:
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
uv run python src/ember/examples/models/model_registry_direct.py

    # Or if in an activated virtual environment
    python src/ember/examples/models/model_registry_direct.py

"""

import logging
import os

# Import the registry components

from ember import initialize_ember
from ember.core.registry.model.base.schemas.model_info import ModelInfo
from ember.core.registry.model.base.schemas.provider_info import ProviderInfo
from ember.core.registry.model.base.services.model_service import ModelService
from ember.core.registry.model.base.services.usage_service import UsageService

# Configure logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

def main():
"""Run a direct model registry example."""
print("\n=== Direct Model Registry Example ===\n")

    # Initialize the registry with no auto-discovery
    registry = initialize_ember(auto_discover=False, initialize_context=False)

    # Get API keys from environment variables
    openai_key = os.environ.get("OPENAI_API_KEY")
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")

    if not openai_key:
        logger.warning(
            "OPENAI_API_KEY environment variable not set. OpenAI example will fail."
        )

    if not anthropic_key:
        logger.warning(
            "ANTHROPIC_API_KEY environment variable not set. Anthropic example will fail."
        )

    # Create provider info objects
    openai_provider = ProviderInfo(
        name="OpenAI", default_api_key=openai_key, base_url="https://api.openai.com/v1"
    )

    anthropic_provider = ProviderInfo(
        name="Anthropic",
        default_api_key=anthropic_key,
        base_url="https://api.anthropic.com/v1",
    )

    # Create and register model info
    gpt4o_model = ModelInfo(id="openai:gpt-4o", name="gpt-4o", provider=openai_provider)

    claude_model = ModelInfo(
        id="anthropic:claude-3-opus",
        name="claude-3-opus",
        provider=anthropic_provider,
    )

    # Register the models
    registry.register_model(model_info=gpt4o_model)
    registry.register_model(model_info=claude_model)

    # Create a model service
    usage_service = UsageService()
    model_service = ModelService(registry=registry, usage_service=usage_service)

    # Try to use the models
    if openai_key:
        try:
            print("Trying OpenAI GPT-4o:")
            openai_response = model_service(
                "openai:gpt-4o", "What is the capital of France?"
            )
            print(f"Response: {openai_response.data}")
        except Exception as e:
            print(f"Error with OpenAI: {e}")
    else:
        print("Skipping OpenAI example because OPENAI_API_KEY is not set.")

    if anthropic_key:
        try:
            print("\nTrying Anthropic Claude:")
            anthropic_response = model_service(
                "anthropic:claude-3-opus", "What is the capital of Italy?"
            )
            print(f"Response: {anthropic_response.data}")
        except Exception as e:
            print(f"Error with Anthropic: {e}")
    else:
        print("\nSkipping Anthropic example because ANTHROPIC_API_KEY is not set.")

    print("\nExample completed!")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\model_registry_example.py:
<code>
"""Model Registry Usage Example

Demonstrates patterns for integrating LLMs using the Ember model registry.

This example shows:

1. The one-line initialization pattern
2. The standard initialization pattern
3. Direct model access (PyTorch-like pattern)
4. Usage tracking and cost estimation
5. Batch processing with multiple models
6. Working with model enums for type safety
7. Adding custom models to the registry

For comprehensive documentation, see:
docs/quickstart/model_registry.md

To run:
uv run python src/ember/examples/models/model_registry_example.py

    # Or if in the virtual env
    python src/ember/examples/models/model_registry_example.py

Required environment variables:
OPENAI_API_KEY (optional): Your OpenAI API key for OpenAI model examples
ANTHROPIC_API_KEY (optional): Your Anthropic API key for Anthropic model examples
"""

import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor

from ember.api import models
from ember.api.models import ModelCost, ModelEnum, ModelInfo, ProviderInfo, RateLimit

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(**name**)

def one_line_pattern():
"""Demonstrates the simplest one-line initialization pattern.

    Returns:
        The models API object for reuse, or None if an error occurs
    """
    # With the new API, we don't need to initialize anything
    # Just use the models API directly

    try:
        # Direct model invocation
        response = models.openai.gpt_4o("What is the capital of France?")
        print("\n=== One-line pattern result ===")
        print(response.data)
        return models  # Return for reuse in other examples
    except Exception as e:
        logger.exception("Error in one-line pattern: %s", str(e))
        return None

def standard_pattern():
"""Demonstrates the standard initialization pattern with more control.

    Returns:
        The configured model instance, or None if an error occurs
    """
    try:
        # With the new API, we can create a model instance with more control
        from ember.api.models import ModelBuilder

        # Create a model with specific parameters
        model = (
            ModelBuilder()
            .temperature(0.7)
            .max_tokens(100)
            .build("anthropic:claude-3-5-sonnet")
        )

        # Use the model
        response = model.generate(prompt="Explain quantum computing in one sentence.")

        print("\n=== Standard pattern result ===")
        print(f"Response: {response.data}")

        # Check usage statistics
        if response.usage:
            print(f"Total tokens used: {response.usage.total_tokens}")
            print("Estimated cost: available through the models.usage API")

        return model
    except Exception as e:
        logger.exception("Error in standard pattern: %s", str(e))
        return None

def direct_model_pattern():
"""Demonstrates direct model access (PyTorch-like pattern).

    Returns:
        The models API object for reuse, or None if an error occurs
    """
    try:
        # With the new API, we can use models directly
        from ember.api.models import ModelAPI

        # Use the direct model ID pattern
        model = ModelAPI(model_id="openai:gpt-4o")
        response = model.generate(prompt="What is the tallest mountain in the world?")

        print("\n=== Direct model pattern result ===")
        print(response.data)

        return models
    except Exception as e:
        logger.exception("Error in direct model pattern: %s", str(e))
        return None

def type_safe_enum_pattern() -> None:
"""Demonstrates using ModelEnum for type-safe model references."""
try: # With the new API, we can use enums directly with models
from ember.api.models import ModelAPI, get_registry

        # Use enum instead of string literals
        model = ModelAPI.from_enum(ModelEnum.gpt_4o)
        response = model.generate(
            prompt="What's your favorite programming language and why?"
        )

        print("\n=== Type-safe enum pattern result ===")
        # Safely truncate long text
        truncated_text = (
            response.data[:150] + "..." if len(response.data) > 150 else response.data
        )
        print(f"Response: {truncated_text}")

        # Access model metadata
        registry = get_registry()
        model_info = registry.get_model_info(model_id="openai:gpt-4o")
        print("\nModel metadata:")
        print(f"Name: {model_info.name}")
        print(f"Provider: {model_info.provider.name}")
        print(
            f"Input cost per 1K tokens: ${model_info.cost.input_cost_per_thousand:.4f}"
        )
        print(
            f"Output cost per 1K tokens: ${model_info.cost.output_cost_per_thousand:.4f}"
        )
        print(f"Context window: {model_info.context_window} tokens")

        # Get version safely using get() instead of hasattr
        version = getattr(model_info, "version", "N/A")
        print(f"Version: {version}")

    except Exception as e:
        logger.exception("Error in type-safe enum pattern: %s", str(e))

def batch_processing_pattern() -> None:
"""Demonstrates batch processing with multiple models."""
try:
from ember.api.models import ModelAPI

        # Define prompts and models
        prompts = [
            "What is machine learning?",
            "Explain the concept of a neural network.",
            "What is transfer learning?",
            "Describe reinforcement learning.",
        ]

        model_ids = [
            "openai:gpt-4o",
            "openai:gpt-4o-mini",
            "anthropic:claude-3-sonnet",
            "anthropic:claude-3-haiku",
        ]

        # Process in parallel with proper typing
        def process_prompt(args: tuple[str, str]) -> tuple[str, str, str, float]:
            """Process a single prompt with the specified model.

            Args:
                args: Tuple of (model_id, prompt)

            Returns:
                Tuple of (model_id, prompt, result_text, duration)
            """
            model_id, prompt = args
            try:
                model = ModelAPI(model_id=model_id)
                start_time = time.time()
                response = model.generate(prompt=prompt)
                duration = time.time() - start_time
                return model_id, prompt, response.data, duration
            except Exception as e:
                logger.warning(
                    "Error processing prompt with model %s: %s", model_id, str(e)
                )
                return model_id, prompt, f"Error: {str(e)}", 0.0

        print("\n=== Batch processing results ===")
        tasks = list(zip(model_ids, prompts))
        results = []

        # Use context manager for ThreadPoolExecutor for proper resource cleanup
        with ThreadPoolExecutor(max_workers=4) as executor:
            # Collect results as they complete
            results = list(executor.map(process_prompt, tasks))

        # Print results
        for i, (model, prompt, result, duration) in enumerate(results):
            print(f"\nTask {i+1}:")
            print(f"Model: {model}")
            print(f"Prompt: {prompt}")
            print(f"Duration: {duration:.2f} seconds")
            # Safely truncate long results
            truncated_result = (
                result[:100] + "..." if result and len(result) > 100 else result
            )
            print(f"Result: {truncated_result}")

        # Calculate and show aggregate stats
        total_duration = sum(duration for _, _, _, duration in results)
        avg_duration = total_duration / len(results) if results else 0
        # Check for error state in a more robust way
        completed_tasks = sum(
            1
            for _, _, result, _ in results
            if result and not result.startswith("Error:")
        )

        print("\n=== Batch Processing Statistics ===")
        print(f"Total tasks: {len(results)}")
        print(f"Successfully completed: {completed_tasks}")
        print(f"Failed: {len(results) - completed_tasks}")
        print(f"Total processing time: {total_duration:.2f} seconds")
        print(f"Average response time: {avg_duration:.2f} seconds per task")

        if total_duration > 0:
            print(
                f"Effective throughput: {len(results) / total_duration:.2f} tasks per second"
            )
        else:
            print("Effective throughput: N/A (no time elapsed)")

        # Usage tracking with new API
        print(
            "\nTotal usage across all batch operations is available through models.usage API"
        )
    except Exception as e:
        logger.exception("Error in batch processing pattern: %s", str(e))

def custom_model_pattern() -> None:
"""Demonstrates adding custom models to the registry."""
try: # With the new API, we get the registry and register models directly
from ember.api.models import get_registry

        # Get the registry
        registry = get_registry()

        # Register a custom model with realistic values
        custom_model = ModelInfo(
            id="custom:my-advanced-llm",
            name="MyOrg Advanced LLM",
            cost=ModelCost(
                input_cost_per_thousand=0.0015,  # $0.0015 per 1K input tokens
                output_cost_per_thousand=0.002,  # $0.002 per 1K output tokens
            ),
            rate_limit=RateLimit(
                tokens_per_minute=100000,  # 100K tokens per minute
                requests_per_minute=3000,  # 3K requests per minute
            ),
            context_window=128000,  # 128K context window
            provider=ProviderInfo(
                name="MyOrg AI",
                default_api_key="${MYORG_API_KEY}",
                api_base="https://api.myorg-ai.example.com/v1",
            ),
        )

        # Check if model is already registered to avoid errors
        if not registry.is_registered(custom_model.id):
            # Register the model
            registry.register_model(model_info=custom_model)
            print(f"Registered custom model: {custom_model.id}")
        else:
            print(
                f"Model {custom_model.id} already registered ✅ - using existing registration"
            )

        # List all models
        model_ids = registry.list_models()

        print("\n=== Custom model registration ===")
        print(f"Registered models: {len(model_ids)} models found")
        print(f"Sample models: {model_ids[:5]} ... and more")

        # Check model exists and get info
        model_id = "custom:my-advanced-llm"
        exists = registry.is_registered(model_id)
        if exists:
            info = registry.get_model_info(model_id)
            print("\nCustom model details:")
            print(f"ID: {info.id}")
            print(f"Name: {info.name}")
            print(f"Provider: {info.provider.name}")
            print(f"API Base URL: {getattr(info.provider, 'api_base', 'N/A')}")
            print(f"Context window: {info.context_window} tokens")
            print(f"Input cost: ${info.cost.input_cost_per_thousand:.4f} per 1K tokens")
            print(
                f"Output cost: ${info.cost.output_cost_per_thousand:.4f} per 1K tokens"
            )
            print(
                f"Rate limits: {info.rate_limit.tokens_per_minute} tokens/min, {info.rate_limit.requests_per_minute} req/min"
            )
        else:
            print("Custom model registration failed!")
    except Exception as e:
        logger.exception("Error in custom model pattern: %s", str(e))

def main() -> None:
"""Run all example patterns."""
print("Running Model Registry examples with the new API...\n")
print("Make sure you have set up your API keys in environment variables:")
print(" - OPENAI_API_KEY")
print(" - ANTHROPIC_API_KEY")
print(" - GOOGLE_API_KEY (if using Gemini models)")

    # Run each pattern
    one_line_pattern()
    standard_pattern()
    direct_model_pattern()
    type_safe_enum_pattern()
    custom_model_pattern()

    # Only run batch processing if we have API keys configured
    keys_needed = ["OPENAI_API_KEY", "ANTHROPIC_API_KEY"]
    missing_keys = [key for key in keys_needed if os.environ.get(key) is None]
    if not missing_keys:
        batch_processing_pattern()
    else:
        print("\nSkipping batch processing example due to missing API keys.")

    print("\nAll examples completed!")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\README.md:
<code>

# Ember Model Examples

This directory contains examples demonstrating how to use Ember's model registry system to work with various LLM providers.

## Examples

- `model_registry_example.py` - General usage patterns for the model registry
- `list_models.py` - How to list available models and their capabilities
- `model_registry_direct.py` - Direct usage of the model registry API
- `model_api_example.py` - Using the model API for inference
- `manual_model_registration.py` - Manually registering custom models
- `register_models_directly.py` - Direct model registration example

## Running Examples

To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/models/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/models/example_name.py
```

Replace `example_name.py` with the desired example file, such as:

```bash
# Example: Run the model registry example
uv run python src/ember/examples/models/model_registry_example.py

# Example: List available models
uv run python src/ember/examples/models/list_models.py
```

## Required Environment Variables

Most examples require API keys for LLM providers to be set in your environment:

```bash
# Set your API keys before running examples
export OPENAI_API_KEY="sk-xxxxxxxxxxxxx"
export ANTHROPIC_API_KEY="sk_ant_xxxxxxxxxxxxx"
```

Some examples (like `register_models_directly.py` and `model_registry_direct.py`) allow you to set API keys directly in the code as an alternative to environment variables.

## Best Practices

### 1. Accessing the Registry

```python
from ember.api import models

# Get the registry
registry = models.get_registry()
```

### 2. Using Provider Namespaces

```python
from ember.api import models

# Direct invocation with namespace
response = models.openai.gpt4o("What is the capital of France?")
response = models.anthropic.claude("Tell me about quantum physics")
```

### 3. Getting and Using Model Instances

```python
from ember.api import models

# Get model service
model_service = models.get_model_service()

# Get a model instance
model = model_service.get_model("openai:gpt-4o")

# Use the model instance
response = model(prompt="What is the capital of France?")
print(response.data)

# Alternative: direct invocation
response = model_service.invoke_model("openai:gpt-4o", "What is the capital of France?")
```

### 4. Model Registration with ProviderInfo

```python
from ember.api.models import ModelInfo, ProviderInfo, ModelCost, RateLimit

model_info = ModelInfo(
    id="provider:model-name",
    name="Human-Readable Name",
    context_window=32000,
    cost=ModelCost(
        input_cost_per_thousand=0.001,
        output_cost_per_thousand=0.002,
    ),
    provider=ProviderInfo(
        name="Provider Name",
        default_api_key="${PROVIDER_API_KEY}",  # Reference environment variable
        base_url="https://api.provider.com",
    ),
)

# Register the model
registry = models.get_registry()
registry.register_model(model_info=model_info)
```

## Example-specific Instructions

- **list_models.py**: Lists all available models in the registry and shows their details.
- **model_registry_example.py**: Demonstrates multiple patterns for working with the model registry.
- **model_api_example.py**: Shows how to initialize and interact with models from different providers.
- **manual_model_registration.py**: Demonstrates how to manually register models with the registry.
- **model_registry_direct.py**: Example of using the registry with manually specified API keys.
- **register_models_directly.py**: Registers models directly without environment variables.

## Next Steps

After understanding model usage, explore:

- `operators/` - For examples of building computation with models
- `advanced/` - For complex model usage patterns

</code>

src\ember\examples\models\register_models_directly.py:
<code>
"""Register Models Directly

This script registers models directly to the registry without using environment variables.
It demonstrates the new simplified API for model registration.

To run:
uv run python src/ember/examples/models/register_models_directly.py

    # Or if in the virtual env
    python src/ember/examples/models/register_models_directly.py

Required environment variables:
OPENAI_API_KEY (optional): Your OpenAI API key for registering OpenAI models
ANTHROPIC_API_KEY (optional): Your Anthropic API key for registering Anthropic models

Note: If your env variables are not set, you can also edit this file to add your API keys directly.
"""

import logging
from typing import List

from prettytable import PrettyTable

from ember.api import models
from ember.api.models import ModelCost, ModelInfo, ModelRegistry, RateLimit

# Set up logging

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(**name**)

# Use these functions to register models with your API keys

def register_openai_models(api_key: str, registry: ModelRegistry) -> List[str]:
"""Register OpenAI models with the registry.

    Args:
        api_key: OpenAI API key
        registry: ModelRegistry instance

    Returns:
        List of registered model IDs
    """
    # Create the models
    model_infos = [
        ModelInfo(
            id="openai:gpt-4o",
            name="GPT-4o",
            context_window=128000,
            cost=ModelCost(
                input_cost_per_thousand=0.005,  # $0.005 per 1K input tokens
                output_cost_per_thousand=0.015,  # $0.015 per 1K output tokens
            ),
            rate_limit=RateLimit(tokens_per_minute=10000000, requests_per_minute=1500),
            provider={
                "name": "OpenAI",
                "default_api_key": api_key,
                "base_url": "https://api.openai.com/v1",
            },
        ),
        ModelInfo(
            id="openai:gpt-4o-mini",
            name="GPT-4o Mini",
            context_window=128000,
            cost=ModelCost(
                input_cost_per_thousand=0.00015,  # $0.00015 per 1K input tokens
                output_cost_per_thousand=0.0006,  # $0.0006 per 1K output tokens
            ),
            rate_limit=RateLimit(tokens_per_minute=10000000, requests_per_minute=1500),
            provider={
                "name": "OpenAI",
                "default_api_key": api_key,
                "base_url": "https://api.openai.com/v1",
            },
        ),
    ]

    # Register the models
    registered_ids = []
    for model_info in model_infos:
        if not registry.is_registered(model_info.id):
            registry.register_model(model_info=model_info)
            logger.info(f"Registered model: {model_info.id}")
        else:
            logger.info(
                f"Model {model_info.id} already registered ✅ - using existing registration"
            )
        registered_ids.append(model_info.id)

    return registered_ids

def register_anthropic_models(api_key: str, registry: ModelRegistry) -> List[str]:
"""Register Anthropic models with the registry.

    Args:
        api_key: Anthropic API key
        registry: ModelRegistry instance

    Returns:
        List of registered model IDs
    """
    # Create the models
    model_infos = [
        ModelInfo(
            id="anthropic:claude-3-5-sonnet",
            name="Claude 3.5 Sonnet",
            context_window=200000,
            cost=ModelCost(
                input_cost_per_thousand=0.003,  # $0.003 per 1K input tokens
                output_cost_per_thousand=0.015,  # $0.015 per 1K output tokens
            ),
            rate_limit=RateLimit(tokens_per_minute=5000000, requests_per_minute=1000),
            provider={
                "name": "Anthropic",
                "default_api_key": api_key,
                "base_url": "https://api.anthropic.com/v1",
            },
        ),
        ModelInfo(
            id="anthropic:claude-3-opus",
            name="Claude 3 Opus",
            context_window=200000,
            cost=ModelCost(
                input_cost_per_thousand=0.015,  # $0.015 per 1K input tokens
                output_cost_per_thousand=0.075,  # $0.075 per 1K output tokens
            ),
            rate_limit=RateLimit(tokens_per_minute=5000000, requests_per_minute=1000),
            provider={
                "name": "Anthropic",
                "default_api_key": api_key,
                "base_url": "https://api.anthropic.com/v1",
            },
        ),
    ]

    # Register the models
    registered_ids = []
    for model_info in model_infos:
        if not registry.is_registered(model_info.id):
            registry.register_model(model_info=model_info)
            logger.info(f"Registered model: {model_info.id}")
        else:
            logger.info(
                f"Model {model_info.id} already registered ✅ - using existing registration"
            )
        registered_ids.append(model_info.id)

    return registered_ids

def check_models(model_ids: List[str], registry: ModelRegistry) -> None:
"""Check if specific models are available in the registry.

    Args:
        model_ids: List of model IDs to check
        registry: ModelRegistry instance
    """
    # Create a table for displaying model information
    table = PrettyTable()
    table.field_names = [
        "Model ID",
        "Status",
        "Provider",
        "Context Window",
        "Input Cost",
        "Output Cost",
    ]
    table.align = "l"

    for model_id in model_ids:
        exists = registry.is_registered(model_id)
        if exists:
            try:
                info = registry.get_model_info(model_id)
                provider_name = (
                    info.provider.name if hasattr(info, "provider") else "Unknown"
                )
                context_window = getattr(info, "context_window", "N/A")

                input_cost = (
                    f"${info.cost.input_cost_per_thousand:.4f}/1K"
                    if hasattr(info, "cost") and info.cost
                    else "N/A"
                )
                output_cost = (
                    f"${info.cost.output_cost_per_thousand:.4f}/1K"
                    if hasattr(info, "cost") and info.cost
                    else "N/A"
                )

                table.add_row(
                    [
                        model_id,
                        "✅ Available",
                        provider_name,
                        context_window,
                        input_cost,
                        output_cost,
                    ]
                )

                logger.info(f"Model '{model_id}' is available and initialized")
            except Exception as e:
                table.add_row(
                    [model_id, "⚠️ Error", "Error", "Error", "Error", "Error"]
                )
                logger.warning(f"Error getting model info for '{model_id}': {e}")
        else:
            table.add_row([model_id, "❌ Not Found", "N/A", "N/A", "N/A", "N/A"])
            logger.warning(f"Model '{model_id}' is not available")

    print("\nModel Availability:")
    print(table)

def main():
"""Run the direct model registration example."""
print("\n=== Direct Model Registration Example ===\n")

    # Initialize the model registry
    registry = models.initialize_registry(auto_discover=True)

    # Enter your API keys here to register the models
    # If you don't want to hardcode them, you would usually get them from environment variables
    # Get keys from environment variables if available
    import os

    openai_api_key = os.environ.get("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY")
    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY", "YOUR_ANTHROPIC_API_KEY")

    # Get list of models before registration
    print("Models before registration:")
    all_models = registry.list_models()
    print(f"Found {len(all_models)} models\n")

    # Register the models (uncomment these lines and add your API keys)
    registered_models = []

    if openai_api_key != "YOUR_OPENAI_API_KEY":
        openai_models = register_openai_models(openai_api_key, registry)
        registered_models.extend(openai_models)
    else:
        print("Skipping OpenAI model registration (no API key provided)")

    if anthropic_api_key != "YOUR_ANTHROPIC_API_KEY":
        anthropic_models = register_anthropic_models(anthropic_api_key, registry)
        registered_models.extend(anthropic_models)
    else:
        print("Skipping Anthropic model registration (no API key provided)")

    # Check which models are available
    check_models(
        [
            "openai:gpt-4o",
            "openai:gpt-4o-mini",
            "anthropic:claude-3-5-sonnet",
            "anthropic:claude-3-opus",
        ],
        registry,
    )

    # Example of using a registered model
    print("\nTo use a registered model:")
    print('response = models("openai:gpt-4o", "What is the capital of France?")')
    print("print(response.text)")

    print("\nExample completed! To use models, add your API keys to the script.")

if **name** == "**main**":
main()

</code>

src\ember\examples\models\_\_init\_\_.py:
<code>
"""Model API examples for Ember."""

</code>

src\ember\examples\operators\composition_example.py:
<code>
"""Operator Composition with Enhanced JIT API.

This example demonstrates how to create complex pipelines by composing operators
with the enhanced JIT API. It shows three patterns:

1. Functional composition with the `compose` utility
2. Sequential operator chaining with explicit dependencies
3. Nested operators within a container class

All approaches benefit from automatic graph building and execution.

To run:
uv run python src/ember/examples/composition_example.py
"""

import logging
import time
from typing import Any, Callable, ClassVar, Dict, List, Type, TypeVar

from prettytable import PrettyTable

# ember API imports

from ember.core import non
from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel
from ember.xcs.engine.execution_options import execution_options
from ember.xcs.tracer.tracer_decorator import jit

T = TypeVar("T")
U = TypeVar("U")
V = TypeVar("V")

###############################################################################

# Composition Utilities

###############################################################################
def compose(f: Callable[[U], V], g: Callable[[T], U]) -> Callable[[T], V]:
"""Compose two functions: f ∘ g.

    Args:
        f: Function that takes output of g
        g: Function that takes initial input

    Returns:
        Composed function (f ∘ g)(x) = f(g(x))
    """

    def composed(x: T) -> V:
        return f(g(x))

    return composed

###############################################################################

# Custom Operators

###############################################################################
class QuestionRefinementInputs(EmberModel):
"""Input model for QuestionRefinement operator."""

    query: str

class QuestionRefinementOutputs(EmberModel):
"""Output model for QuestionRefinement operator."""

    refined_query: str

class QuestionRefinementSpecification(Specification):
"""Specification for QuestionRefinement operator."""

    input_model: Type[EmberModel] = QuestionRefinementInputs
    structured_output: Type[EmberModel] = QuestionRefinementOutputs
    prompt_template: str = (
        "You are an expert at refining questions to make them clearer and more precise.\n"
        "Please refine the following question:\n\n"
        "{query}\n\n"
        "Provide a refined version that is more specific and answerable."
    )

@jit()
class QuestionRefinement(Operator[QuestionRefinementInputs, QuestionRefinementOutputs]):
"""Operator that refines a user question to make it more precise."""

    specification: ClassVar[Specification] = QuestionRefinementSpecification()
    model_name: str
    temperature: float
    lm_module: LMModule

    def __init__(self, *, model_name: str, temperature: float = 0.3) -> None:
        self.model_name = model_name
        self.temperature = temperature

        # Configure internal LM module
        self.lm_module = LMModule(
            config=LMModuleConfig(
                id=model_name,  # Fixed: using "id" instead of "model_name"
                temperature=temperature,
            )
        )

    def forward(self, *, inputs: QuestionRefinementInputs) -> QuestionRefinementOutputs:
        prompt = self.specification.render_prompt(inputs=inputs)

        try:
            response = self.lm_module(prompt=prompt)

            # Get text from response
            refined_query = (
                response.strip() if isinstance(response, str) else str(response).strip()
            )

            return QuestionRefinementOutputs(refined_query=refined_query)
        except Exception as e:
            # Graceful error handling for model failures
            logging.warning(f"Error invoking model {self.model_name}: {str(e)}")
            # Return a fallback refinement that doesn't fail the pipeline
            fallback_query = f"Refined: {inputs.query}"
            return QuestionRefinementOutputs(refined_query=fallback_query)

###############################################################################

# Pipeline Pattern 1: Functional Composition

###############################################################################
def create_functional_pipeline(\*, model_name: str) -> Callable[[Dict[str, Any]], Any]:
"""Create a pipeline using functional composition.

    Args:
        model_name: Name of the LLM to use

    Returns:
        A callable pipeline function
    """
    # Create individual operators
    refiner = QuestionRefinement(model_name=model_name)
    ensemble = non.UniformEnsemble(num_units=3, model_name=model_name, temperature=0.7)
    aggregator = non.MostCommon()

    # Use partial application to adapt the interfaces
    def adapt_refiner_output(inputs: Dict[str, Any]) -> Dict[str, Any]:
        result = refiner(inputs=QuestionRefinementInputs(**inputs))
        return {"query": result.refined_query}

    def adapt_ensemble_output(inputs: Dict[str, Any]) -> Dict[str, Any]:
        result = ensemble(inputs=inputs)
        return {"query": inputs["query"], "responses": result["responses"]}

    # Compose the pipeline
    pipeline = compose(aggregator, compose(adapt_ensemble_output, adapt_refiner_output))

    return pipeline

###############################################################################

# Pipeline Pattern 2: Container Class with Nested Operators

###############################################################################
class PipelineInput(EmberModel):
"""Input for NestedPipeline."""

    query: str

class PipelineOutput(EmberModel):
"""Output for NestedPipeline."""

    final_answer: str

class PipelineSpecification(Specification):
"""Specification for NestedPipeline."""

    input_model: Type[EmberModel] = PipelineInput
    structured_output: Type[EmberModel] = PipelineOutput

@jit(sample_input={"query": "What is the speed of light?"})
class NestedPipeline(Operator[PipelineInput, PipelineOutput]):
"""Pipeline implemented as a container class with nested operators."""

    specification: ClassVar[Specification] = PipelineSpecification()
    refiner: QuestionRefinement
    ensemble: non.UniformEnsemble
    aggregator: non.MostCommon

    def __init__(self, *, model_name: str) -> None:
        self.refiner = QuestionRefinement(model_name=model_name)
        self.ensemble = non.UniformEnsemble(
            num_units=3, model_name=model_name, temperature=0.7
        )
        self.aggregator = non.MostCommon()

    def forward(self, *, inputs: PipelineInput) -> PipelineOutput:
        # Step 1: Refine the question
        refined = self.refiner(inputs=QuestionRefinementInputs(query=inputs.query))

        # Step 2: Generate ensemble of answers
        ensemble_result = self.ensemble(inputs={"query": refined.refined_query})

        # Step 3: Aggregate results
        final_result = self.aggregator(
            inputs={
                "query": refined.refined_query,
                "responses": ensemble_result["responses"],
            }
        )

        return PipelineOutput(final_answer=final_result["final_answer"])

###############################################################################

# Pipeline Pattern 3: Sequential Chaining

###############################################################################
def create_sequential_pipeline(\*, model_name: str) -> Callable[[Dict[str, Any]], Any]:
"""Create a pipeline by explicitly chaining operators.

    Args:
        model_name: Name of the LLM to use

    Returns:
        A callable pipeline function
    """
    # Create individual operators
    refiner = QuestionRefinement(model_name=model_name)
    ensemble = non.UniformEnsemble(num_units=3, model_name=model_name, temperature=0.7)
    aggregator = non.MostCommon()

    # Create the chained function
    def pipeline(inputs: Dict[str, Any]) -> Any:
        refined = refiner(inputs=QuestionRefinementInputs(**inputs))
        ensemble_result = ensemble(inputs={"query": refined.refined_query})
        final_result = aggregator(
            inputs={
                "query": refined.refined_query,
                "responses": ensemble_result["responses"],
            }
        )
        return final_result

    return pipeline

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run demonstration of different composition patterns."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    # Define configuration parameters
    model_name: str = "openai:gpt-3.5-turbo"

    # Create pipelines using different patterns
    functional_pipeline = create_functional_pipeline(model_name=model_name)
    nested_pipeline = NestedPipeline(model_name=model_name)
    sequential_pipeline = create_sequential_pipeline(model_name=model_name)

    # List of questions to process
    questions: List[str] = [
        "How does gravity work?",
        "Tell me about the history of Rome",
        "What's the difference between DNA and RNA?",
    ]

    # Prepare table for results comparison
    table = PrettyTable()
    table.field_names = ["Pipeline", "Time (s)", "Result"]
    table.align = "l"

    # Process questions with each pipeline
    print("\n=== Functional Composition Pipeline ===")
    for question in questions[:1]:  # Use first question only for brevity
        print(f"\nProcessing: {question}")
        start_time = time.perf_counter()
        result = functional_pipeline({"query": question})
        elapsed = time.perf_counter() - start_time

        # Show details of pipeline execution
        print(f'Original query: "{question}"')
        final_answer = result.get("final_answer", "")
        if isinstance(result, dict) and "refined_query" in result:
            print(f"Refined query: \"{result['refined_query']}\"")
        print(
            f'Final answer: "{final_answer[:150]}..."'
            if len(final_answer) > 150
            else f'Final answer: "{final_answer}"'
        )
        print(f"Time: {elapsed:.4f}s")

        # Store in table
        table.add_row(
            [
                "Functional",
                f"{elapsed:.4f}",
                (final_answer[:50] + "..." if len(final_answer) > 50 else final_answer),
            ]
        )

    print("\n=== Nested Pipeline ===")
    for question in questions[:1]:
        print(f"\nProcessing: {question}")
        start_time = time.perf_counter()
        result = nested_pipeline(inputs={"query": question})
        elapsed = time.perf_counter() - start_time

        # Show details of pipeline execution
        final_answer = (
            result.final_answer if hasattr(result, "final_answer") else str(result)
        )
        print(
            f'Final answer: "{final_answer[:150]}..."'
            if len(final_answer) > 150
            else f'Final answer: "{final_answer}"'
        )
        print(f"Time: {elapsed:.4f}s")

        # Store in table
        table.add_row(
            [
                "Nested",
                f"{elapsed:.4f}",
                (final_answer[:50] + "..." if len(final_answer) > 50 else final_answer),
            ]
        )

    print("\n=== Sequential Pipeline ===")
    for question in questions[:1]:
        print(f"\nProcessing: {question}")
        start_time = time.perf_counter()
        result = sequential_pipeline({"query": question})
        elapsed = time.perf_counter() - start_time

        # Show details of pipeline execution
        final_answer = (
            result.get("final_answer", "") if isinstance(result, dict) else str(result)
        )
        print(
            f'Final answer: "{final_answer[:150]}..."'
            if len(final_answer) > 150
            else f'Final answer: "{final_answer}"'
        )
        print(f"Time: {elapsed:.4f}s")

        # Store in table
        table.add_row(
            [
                "Sequential",
                f"{elapsed:.4f}",
                (final_answer[:50] + "..." if len(final_answer) > 50 else final_answer),
            ]
        )

    # Demonstrate execution options with the nested pipeline
    print("\n=== Nested Pipeline with Sequential Execution ===")
    with execution_options(scheduler="sequential"):
        for question in questions[:1]:
            print(f"\nProcessing: {question}")
            start_time = time.perf_counter()
            result = nested_pipeline(inputs={"query": question})
            elapsed = time.perf_counter() - start_time

            # Show details of pipeline execution
            final_answer = (
                result.final_answer if hasattr(result, "final_answer") else str(result)
            )
            print(
                f'Final answer: "{final_answer[:150]}..."'
                if len(final_answer) > 150
                else f'Final answer: "{final_answer}"'
            )
            print(f"Time: {elapsed:.4f}s")
            print("Execution mode: Sequential scheduler")

            # Store in table
            table.add_row(
                [
                    "Nested (Sequential)",
                    f"{elapsed:.4f}",
                    (
                        final_answer[:50] + "..."
                        if len(final_answer) > 50
                        else final_answer
                    ),
                ]
            )

    # Display performance comparison
    print("\n=== Performance Comparison ===")
    print(table)

    print("\n=== Pipeline Pattern Comparison ===")
    print("1. Functional Composition:")
    print("   • Advantages: Clean separation of concerns, explicit data flow")
    print("   • Use cases: When components need to be reused separately")

    print("\n2. Nested Pipeline:")
    print("   • Advantages: Benefits from JIT optimization, cleaner code structure")
    print("   • Use cases: Complex pipelines where performance matters")

    print("\n3. Sequential Pipeline:")
    print("   • Advantages: Simplicity, flexibility for custom logic")
    print("   • Use cases: Prototyping or simpler workflows")

if **name** == "**main**":
main()

</code>

src\ember\examples\operators\container_operator_example.py:
<code>
"""Container Operator Example with JIT.

This example demonstrates how to create a container operator that encapsulates
a complex pipeline, using JIT for tracing.

Note: In the current implementation, the JIT decorator enables tracing but
doesn't automatically build execution graphs. For automatic graph building
and parallel execution, you'd still need to use XCSGraph and execute_graph
(shown in the other examples).

To run:
poetry run python src/ember/examples/container_operator_example.py
"""

import logging
import time
from typing import ClassVar, List, Type

from ember.core import non

# ember imports

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel
from ember.xcs.tracer.tracer_decorator import jit

###############################################################################

# Custom Input/Output Models

###############################################################################
class QuestionAnsweringInput(EmberModel):
"""Input for question answering pipeline."""

    query: str

class QuestionAnsweringOutput(EmberModel):
"""Output for question answering pipeline."""

    answer: str
    confidence: float
    model_responses: List[str]

class QuestionAnsweringSpecification(Specification):
"""Specification for question answering pipeline."""

    input_model: Type[EmberModel] = QuestionAnsweringInput
    structured_output: Type[EmberModel] = QuestionAnsweringOutput

###############################################################################

# Container Operator with JIT

###############################################################################
@jit(sample_input={"query": "What is the capital of France?"})
class QuestionAnsweringPipeline(
Operator[QuestionAnsweringInput, QuestionAnsweringOutput]
):
"""Container operator that encapsulates a complete question answering pipeline.

    This operator is decorated with @jit for tracing. The tracing doesn't
    automatically optimize execution in the current implementation, but lays
    the groundwork for future automatic optimization.

    The pipeline internally uses an ensemble of models followed by an aggregation step.
    """

    # Class-level specification declaration
    specification: ClassVar[Specification] = QuestionAnsweringSpecification()

    # Class-level field declarations
    model_name: str
    num_units: int
    temperature: float
    ensemble: non.UniformEnsemble
    aggregator: non.MostCommon

    def __init__(
        self, *, model_name: str, num_units: int = 3, temperature: float = 0.7
    ) -> None:
        """Initialize the pipeline.

        Args:
            model_name: The model to use
            num_units: Number of ensemble members
            temperature: Generation temperature
        """
        self.model_name = model_name
        self.num_units = num_units
        self.temperature = temperature

        # Create internal operators
        self.ensemble = non.UniformEnsemble(
            num_units=self.num_units,
            model_name=self.model_name,
            temperature=self.temperature,
        )
        self.aggregator = non.MostCommon()

    def forward(self, *, inputs: QuestionAnsweringInput) -> QuestionAnsweringOutput:
        """Execute the pipeline.

        Args:
            inputs: The input query

        Returns:
            Structured output with the final answer, confidence, and model responses
        """
        # Run the ensemble to get multiple responses
        ensemble_result = self.ensemble(inputs={"query": inputs.query})
        responses = ensemble_result["responses"]

        # Aggregate responses
        aggregated = self.aggregator(
            inputs={"query": inputs.query, "responses": responses}
        )

        # Extract confidence from aggregation
        confidence = aggregated.get("confidence", 0.0)

        # Return structured output model instance
        return QuestionAnsweringOutput(
            answer=aggregated["final_answer"],
            confidence=confidence,
            model_responses=responses,
        )

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run demonstration of container operator with JIT."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("\n=== Container Operator with JIT ===\n")

    # Create the pipeline
    pipeline = QuestionAnsweringPipeline(model_name="openai:gpt-4o-mini", num_units=3)

    # Example queries
    queries = [
        "What is the capital of France?",
        "How many planets are in our solar system?",
        "Who wrote 'Pride and Prejudice'?",
    ]

    # Process all queries
    for query in queries:
        print(f"\nProcessing query: {query}")

        # Time the execution - using kwargs format for cleaner code
        start_time = time.perf_counter()
        result = pipeline(inputs={"query": query})
        end_time = time.perf_counter()

        # Display results
        print(f"Answer: {result.answer}")
        print(f"Confidence: {result.confidence:.2f}")
        print(f"Execution time: {end_time - start_time:.4f}s")

    print("\nNote: The first execution includes tracing overhead. In the current")
    print("      implementation, this tracing doesn't automatically optimize execution")
    print("      but lays the groundwork for future optimizations.")

if **name** == "**main**":
main()

</code>

src\ember\examples\operators\container_simplified.py:
<code>
"""Simplified Container Operator Example.

This example demonstrates how to create a container operator that encapsulates
a simple processing pipeline, using JIT for tracing.

To run:
uv run python src/ember/examples/container_simplified.py
"""

import logging
import time
from typing import ClassVar, Type

from pydantic import Field

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel
from ember.xcs.tracer.tracer_decorator import jit

###############################################################################

# Custom Input/Output Models

###############################################################################

class ProcessingInput(EmberModel):
"""Input for text processing pipeline.

    Attributes:
        text: The input text to be processed.
    """

    text: str = Field(description="The input text to be processed")

class ProcessingOutput(EmberModel):
"""Output for text processing pipeline.

    Attributes:
        processed_text: The text after processing.
        word_count: The number of words in the processed text.
        processing_time: The time taken to process the text in seconds.
    """

    processed_text: str = Field(description="The text after processing")
    word_count: int = Field(description="Number of words in the processed text")
    processing_time: float = Field(
        description="Time taken to process the text in seconds"
    )

class ProcessingSpecification(Specification):
"""Specification for text processing pipeline."""

    input_model: Type[EmberModel] = ProcessingInput
    structured_output: Type[EmberModel] = ProcessingOutput

###############################################################################

# Component Operators

###############################################################################

@jit()
class TextNormalizer(Operator[ProcessingInput, ProcessingOutput]):
"""Operator that normalizes text (converts to lowercase, etc.)."""

    specification: ClassVar[Specification] = ProcessingSpecification()

    def forward(self, *, inputs: ProcessingInput) -> ProcessingOutput:
        """Normalize the input text.

        Args:
            inputs: Input containing the text to normalize.

        Returns:
            Normalized text with processing statistics.
        """
        start_time = time.time()
        text = inputs.text.lower().strip()
        time.sleep(0.1)  # Simulate processing time

        return ProcessingOutput(
            processed_text=text,
            word_count=len(text.split()),
            processing_time=time.time() - start_time,
        )

@jit()
class TextEnhancer(Operator[ProcessingOutput, ProcessingOutput]):
"""Operator that enhances text (adds formatting, etc.)."""

    specification: ClassVar[Specification] = ProcessingSpecification()

    def forward(self, *, inputs: ProcessingOutput) -> ProcessingOutput:
        """Enhance the input text.

        Args:
            inputs: Input containing already processed text to enhance.

        Returns:
            Enhanced text with updated processing statistics.
        """
        start_time = time.time()
        text = inputs.processed_text.capitalize()
        time.sleep(0.1)  # Simulate processing time

        return ProcessingOutput(
            processed_text=text + "!",
            word_count=inputs.word_count,
            processing_time=inputs.processing_time + (time.time() - start_time),
        )

###############################################################################

# Container Operator

###############################################################################

@jit()
class TextProcessor(Operator[ProcessingInput, ProcessingOutput]):
"""Container operator that applies normalization and enhancement."""

    specification: ClassVar[Specification] = ProcessingSpecification()

    # Define instance attributes with type hints
    normalizer: TextNormalizer
    enhancer: TextEnhancer

    def __init__(self) -> None:
        """Initialize with component operators."""
        self.normalizer = TextNormalizer()
        self.enhancer = TextEnhancer()

    def forward(self, *, inputs: ProcessingInput) -> ProcessingOutput:
        """Process the input text through the pipeline.

        Args:
            inputs: Input containing the text to process.

        Returns:
            Enhanced and normalized text with processing statistics.
        """
        # First normalize
        normalized = self.normalizer(inputs=inputs)

        # Then enhance
        enhanced = self.enhancer(inputs=normalized)

        return enhanced

def main() -> None:
"""Run a demonstration of the container operator."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("\n=== Container Operator Example ===\n")

    # Create the container operator
    processor = TextProcessor()

    # Process a text
    input_text = "Hello, world! This is a CONTAINER operator example."
    result = processor(inputs=ProcessingInput(text=input_text))

    print(f"Input: {input_text}")
    print(f"Output: {result.processed_text}")
    print(f"Word Count: {result.word_count}")
    print(f"Processing Time: {result.processing_time:.4f}s")

    # Show processing steps
    normalized_text = input_text.lower().strip()
    enhanced_text = normalized_text.capitalize() + "!"
    print("\nProcessing Steps:")
    print(f"1. Normalization: '{input_text}' -> '{normalized_text}'")
    print(f"2. Enhancement: '{normalized_text}' -> '{enhanced_text}'")

    # Process another text to demonstrate cached execution
    print("\n--- Second run (should use cached plan) ---")
    input_text2 = "ANOTHER example for DEMONSTRATION."
    start_time = time.time()
    result2 = processor(inputs=ProcessingInput(text=input_text2))
    elapsed = time.time() - start_time

    print(f"Input: {input_text2}")
    print(f"Output: {result2.processed_text}")
    print(f"Word Count: {result2.word_count}")
    print(f"Processing Time from Operator: {result2.processing_time:.4f}s")
    print(f"Total Wall Time: {elapsed:.4f}s")

    # Show processing steps for second run
    normalized_text2 = input_text2.lower().strip()
    enhanced_text2 = normalized_text2.capitalize() + "!"
    print("\nProcessing Steps:")
    print(f"1. Normalization: '{input_text2}' -> '{normalized_text2}'")
    print(f"2. Enhancement: '{normalized_text2}' -> '{enhanced_text2}'")
    print("Note: Second run is typically faster due to cached execution")

if **name** == "**main**":
main()

</code>

src\ember\examples\operators\custom_prompt_example_caravan.py:
<code>
"""
Usage:
export AVIOR_API_KEY=avior_api_key
export AVIOR_BASE_URL=http://avior_base_url
export AVIOR_CUSTOM_MODEL=custom_model
python custom_prompt_example_caravan.py --non simple

Example:
python custom_prompt_example_caravan.py --non caravan

Overview: 1) 'simple': minimal single-sentence Q&A pipeline. 2) 'caravan': more advanced prompt that references the UNSW-NB15 dataset
flows, providing labeled references and then labeling new flows.

To run:
uv run python src/ember/examples/custom_prompt_example_caravan.py
"""

import argparse
import logging
import os
import sys
from typing import ClassVar, Type

from ember.core.context import current_context
from ember.core.types.ember_model import EmberModel, Field

# ------------------------------------------------------------------------------------

# Logging Setup

# ------------------------------------------------------------------------------------

logging.basicConfig(
level=logging.INFO,
format="%(asctime)s - %(levelname)s - %(message)s",
stream=sys.stdout,
)
logger = logging.getLogger(**name**)

# ------------------------------------------------------------------------------------

# Constants & Env

# ------------------------------------------------------------------------------------

req_env_vars = ["AVIOR_CUSTOM_MODEL", "AVIOR_API_KEY", "AVIOR_BASE_URL"]
SIMPLE_NON = "simple"
CARAVAN_NON = "caravan"

sample_flow_stream = (
" (1) 0.001104000024497509,120.0,146.0,178.0,31.0,29.0,528985.5,644927.5,2.0,2.0,73.0,89.0,"
"0.010999999940395355,0.009999999776482582,0.0,0.0,0.0,2.0,1.0,1.0 "
"(2) 0.0009689999860711396,119.0,132.0,164.0,31.0,29.0,544891.625,676986.625,2.0,2.0,66.0,"
"82.0,0.004000000189989805,0.010999999940395355,0.0,0.0,0.0,8.0,4.0,2.0 "
"(3) 3.000000106112566e-06,119.0,114.0,0.0,254.0,0.0,152000000.0,0.0,2.0,0.0,57.0,0.0,"
"0.003000000026077032,0.0,0.0,0.0,0.0,8.0,8.0,8.0 "
"(4) 9.000000318337698e-06,119.0,264.0,0.0,60.0,0.0,117333328.0,0.0,2.0,0.0,132.0,0.0,"
"0.008999999612569809,0.0,0.0,0.0,0.0,12.0,12.0,25.0 "
"(5) 4.999999873689376e-06,119.0,114.0,0.0,254.0,0.0,91200000.0,0.0,2.0,0.0,57.0,0.0,"
"0.004999999888241291,0.0,0.0,0.0,0.0,22.0,22.0,31.0 "
"(6) 1.1568700075149536,113.0,1684.0,10168.0,31.0,29.0,10815.3896484375,66413.6875,14.0,"
"18.0,120.0,565.0,88.96299743652344,68.01847076416016,0.0007060000207275152,"
"0.0005520000122487545,0.0001539999939268455,4.0,4.0,1.0 "
"(7) 0.0017600000137463212,119.0,528.0,304.0,31.0,29.0,1800000.0,1036363.625,4.0,4.0,"
"132.0,76.0,0.45466700196266174,0.19200000166893005,0.0,0.0,0.0,9.0,3.0,6.0 "
"(8) 0.0069240001030266285,113.0,3680.0,2456.0,31.0,29.0,4016175.5,2680531.5,18.0,18.0,"
"204.0,136.0,0.3875879943370819,0.37882399559020996,0.0006150000263005495,"
"0.0004799999878741801,0.00013499999477062374,4.0,5.0,3.0 "
"(9) 0.005369000136852264,120.0,568.0,320.0,31.0,29.0,634755.0625,357608.46875,4.0,4.0,"
"142.0,80.0,1.255666971206665,1.277999997138977,0.0,0.0,0.0,4.0,4.0,3.0 "
"(10) 0.5125219821929932,114.0,8928.0,320.0,31.0,29.0,129414.9375,4167.6259765625,"
"14.0,6.0,638.0,53.0,39.424766540527344,102.36280059814453,0.0007179999956861138,"
"0.0005740000051446259,0.00014400000509340316,6.0,6.0,5.0 "
)

# ------------------------------------------------------------------------------------

# Environment Validation

# ------------------------------------------------------------------------------------

def check_env() -> None:
"""Ensure all required environment variables are set."""
missing = [e for e in req_env_vars if not os.getenv(e)]
if missing:
logger.error(f"Missing env vars: {missing}")
sys.exit(1)

# ------------------------------------------------------------------------------------

# Model Registration

# ------------------------------------------------------------------------------------

from ember.core.registry.model.base.schemas.cost import ModelCost, RateLimit
from ember.core.registry.model.base.schemas.model_info import ModelInfo
from ember.core.registry.model.base.schemas.provider_info import ProviderInfo
from ember.core.registry.specification.specification import Specification

def register_custom_model() -> ModelInfo:
"""
Registers the user-specified custom model with the global context's registry.
This must be done before creating any LMModule referencing `AVIOR_CUSTOM_MODEL`.
"""
custom_model = os.getenv("AVIOR_CUSTOM_MODEL", "")
base_url = os.getenv("AVIOR_BASE_URL", "")
api_key = os.getenv("AVIOR_API_KEY", "")

    # Get the registry from the global context
    context = current_context()
    registry = context.registry

    model_info = ModelInfo(
        model_id=custom_model,
        model_name=custom_model,
        cost=ModelCost(input_cost_per_thousand=0.0, output_cost_per_thousand=0.0),
        rate_limit=RateLimit(tokens_per_minute=0, requests_per_minute=0),
        provider=ProviderInfo(
            name="foundry", default_api_key=api_key, base_url=base_url
        ),
        api_key=api_key,
    )
    registry.register_model(model_info)
    logger.info(
        f"Registered custom model '{custom_model}' with base_url='{base_url}'. "
        f"Models now in registry: {registry.list_models()}"
    )
    return model_info

# ------------------------------------------------------------------------------------

# Prompt Pieces (Old Context, Broken Down)

# ------------------------------------------------------------------------------------

CARAVAN_PROMPT_INTRO = (
"You are an expert in network security. The user is now labeling a network intrusion "
"detection dataset (UNSW-NB15). He wants to assign a binary label (0=benign, 1=malicious) "
"to each traffic flow based on its features."
)

CARAVAN_PROMPT_FEATURES = (
"Features include: dur (duration), proto (protocol), sbytes/dbytes (src->dst/dst->src bytes), "
"sttl/dttl (time to live), sload/dload (bits/sec), spkts/dpkts (packet counts), smean/dmean "
"(mean packet sizes), sinpkt/dinpkt (interpacket arrival times), tcprtt/synack/ackdat "
"(TCP handshake times), ct_src_ltm/ct_dst_ltm/ct_dst_src_ltm (connection counts), etc."
)

CARAVAN_PROMPT_REFERENCES = (
"He provides some labeled flows for reference (the last field is the binary label). "
"Next, he'll provide unlabeled flows and wants you to give a label for each, with no explanation."
)

CARAVAN_PROMPT_INSTRUCTIONS = (
"Please output a label (0 or 1) per line in the format: (flow number) label. "
"No explanation or analysis needed, label only."
)

CARAVAN_PROMPT_FULL = (
f"{CARAVAN_PROMPT_INTRO}\n"
f"{CARAVAN_PROMPT_FEATURES}\n"
f"{CARAVAN_PROMPT_REFERENCES}\n"
f"{CARAVAN_PROMPT_INSTRUCTIONS}\n"
f"UNLABELED FLOWS:\n"
f"{{question}}\n"
)

# ------------------------------------------------------------------------------------

# Minimal 'Specification' & 'Inputs' for Our Caravan Prompt

# ------------------------------------------------------------------------------------

class CaravanLabelingInputs(EmberModel):
"""Input model for network traffic flow labeling.

    Attributes:
        question: The unlabeled flows to be classified.
    """

    question: str = Field(description="Unlabeled network flows to be classified")

class CaravanLabelingOutput(EmberModel):
"""Output model for network traffic flow labeling.

    Attributes:
        final_answer: The labeled flow classifications.
    """

    final_answer: str = Field(
        description="Labeled flow classifications (0=benign, 1=malicious)"
    )

class CaravanLabelingSpecification(Specification):
"""Specification for the CaravanLabelingOperator.

    Defines input/output models and the multi-part prompt template
    for network traffic flow labeling.
    """

    input_model: Type[EmberModel] = CaravanLabelingInputs
    structured_output: Type[EmberModel] = CaravanLabelingOutput
    prompt_template: str = CARAVAN_PROMPT_FULL

# ------------------------------------------------------------------------------------

# A Simple 'Specification' & 'Inputs' for the "simple" pipeline

# ------------------------------------------------------------------------------------

class SimplePromptInputs(EmberModel):
"""The request for a simple question like "What is the capital of India?"

    Attributes:
        question: The question to be answered.
    """

    question: str = Field(description="Question to be answered")

class SimplePromptOutput(EmberModel):
"""Output model for simple question answering.

    Attributes:
        final_answer: The concise answer to the question.
    """

    final_answer: str = Field(description="Concise answer to the question")

class SimplePromptSpecification(Specification):
"""Specification for the SimplePromptOperator.

    Defines input/output models and prompt template for single-sentence Q&A.
    """

    input_model: Type[EmberModel] = SimplePromptInputs
    structured_output: Type[EmberModel] = SimplePromptOutput
    prompt_template: str = (
        "Provide a concise single-sentence answer to the following question:\n"
        "QUESTION: {question}\n"
    )

from ember.core.non import JudgeSynthesis, UniformEnsemble

# ------------------------------------------------------------------------------------

# Operators (Single-step LM calls using these specifications)

# ------------------------------------------------------------------------------------

from ember.core.registry.operator.base.operator_base import Operator

class SimplePromptOperator(Operator[SimplePromptInputs, SimplePromptOutput]):
"""Single-step operator for simple question answering.

    This operator uses a single-instance ensemble to process a question
    and produce a concise answer.

    Attributes:
        specification: The specification defining input/output models and prompt template.
        ensemble: The UniformEnsemble operator with a single LM instance.
    """

    specification: ClassVar[Specification] = SimplePromptSpecification()
    ensemble: UniformEnsemble

    def __init__(self, model_name: str) -> None:
        """Initialize with a specific model name.

        Args:
            model_name: Name of the model to use for answering.
        """
        self.ensemble = UniformEnsemble(
            num_units=1, model_name=model_name, temperature=0.2, max_tokens=64
        )

    def forward(self, *, inputs: SimplePromptInputs) -> SimplePromptOutput:
        """Process the input question and produce a concise answer.

        Args:
            inputs: The validated input containing the question.

        Returns:
            A SimplePromptOutput with the final answer.
        """
        # Construct prompt from input
        prompt = self.specification.render_prompt(inputs=inputs)

        # Process through ensemble
        ensemble_result = self.ensemble(inputs={"query": prompt})

        # Extract the final_answer from the ensemble result
        final_answer = ensemble_result["final_answer"]

        # Return structured output
        return SimplePromptOutput(final_answer=final_answer)

class CaravanLabelingOperator(Operator[CaravanLabelingInputs, CaravanLabelingOutput]):
"""Operator that labels network flows as benign (0) or malicious (1).

    This operator uses a multi-part prompt with domain-specific context
    and aggregates results from multiple LMs via a judge synthesis step.

    Attributes:
        specification: The specification defining input/output models and prompt template.
        ensemble: The ensemble operator for generating multiple candidate labels.
        judge: The synthesis operator for combining and refining ensemble results.
    """

    specification: ClassVar[Specification] = CaravanLabelingSpecification()
    ensemble: UniformEnsemble
    judge: JudgeSynthesis

    def __init__(self, model_name: str) -> None:
        """Initialize with a specific model name.

        Args:
            model_name: Name of the model to use for labeling.
        """
        self.ensemble = UniformEnsemble(
            num_units=3, model_name=model_name, temperature=0.0, max_tokens=256
        )
        self.judge = JudgeSynthesis(
            model_name=model_name, temperature=0.0, max_tokens=256
        )

    def forward(self, *, inputs: CaravanLabelingInputs) -> CaravanLabelingOutput:
        """Process network flows and produce labeled classifications.

        Args:
            inputs: The validated input containing unlabeled flows.

        Returns:
            A CaravanLabelingOutput with the classified flows.
        """
        # Construct prompt from input
        prompt = self.specification.render_prompt(inputs=inputs)

        # Process through ensemble to get multiple labeling attempts
        ensemble_output = self.ensemble(inputs={"query": prompt})

        # Extract the responses from the ensemble output
        responses = ensemble_output["responses"]

        # Synthesize results using judge
        judge_output = self.judge(inputs={"query": prompt, "responses": responses})

        # Extract the final answer from the judge output
        final_labels = judge_output["final_answer"]

        # Return structured output
        return CaravanLabelingOutput(final_answer=final_labels)

# ------------------------------------------------------------------------------------

# Graph/Pipeline Constructors

# ------------------------------------------------------------------------------------

def create_simple_pipeline(
model_name: str,
) -> Operator[SimplePromptInputs, SimplePromptOutput]:
"""Create a single-step operator for simple question answering.

    Args:
        model_name: Name of the model to use.

    Returns:
        A strongly-typed SimplePromptOperator instance.
    """
    return SimplePromptOperator(model_name)

def create_caravan_pipeline(
model_name: str,
) -> Operator[CaravanLabelingInputs, CaravanLabelingOutput]:
"""Create a single-step operator for network flow labeling.

    Args:
        model_name: Name of the model to use.

    Returns:
        A strongly-typed CaravanLabelingOperator instance.
    """
    return CaravanLabelingOperator(model_name)

# ------------------------------------------------------------------------------------

# Main + Arg Parsing

# ------------------------------------------------------------------------------------

def parse_arguments() -> argparse.Namespace:
parser = argparse.ArgumentParser(
description="Refactored custom prompt example (caravan)."
)
parser.add_argument(
"--non",
type=str,
default=SIMPLE_NON,
help="Which pipeline to run: 'simple' or 'caravan'.",
)
return parser.parse_args()

def main():
logger.info("Starting refactored custom prompt with old context ...")
check_env()
register_custom_model()

    args = parse_arguments()
    chosen_non = args.non.lower().strip()
    model_name = os.getenv("AVIOR_CUSTOM_MODEL", "")

    if chosen_non == SIMPLE_NON:
        operator = create_simple_pipeline(model_name)
        # Example question:
        question_data = "What is the capital of India?"
        # Using kwargs format for cleaner input
        response = operator(inputs={"question": question_data})
        print(f"[SIMPLE] Final Answer:\n{response.final_answer}\n")

    elif chosen_non == CARAVAN_NON:
        operator = create_caravan_pipeline(model_name)
        # We'll pass the flows into the 'question' field:
        flows = sample_flow_stream
        # Using kwargs format for cleaner input
        response = operator(inputs={"question": flows})
        print(f"[CARAVAN] Final Labeled Output:\n{response.final_answer}\n")

    else:
        logger.error(
            f"Invalid --non={chosen_non}. Must be '{SIMPLE_NON}' or '{CARAVAN_NON}'."
        )
        sys.exit(1)

if **name** == "**main**":
main()

</code>

src\ember\examples\operators\diverse_ensemble_operator_example.py:
<code>
"""Diverse Ensemble Operator Example

This example demonstrates using the MultiPrefixEnsembleOperator with distinct prefixes
for each language model to generate diverse responses to a query.

To run:
uv run python src/ember/examples/diverse_ensemble_operator_example.py
"""

from random import sample
from typing import ClassVar, List, Type

from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel, Field

def usage_example() -> None:
"""Demonstrates usage of MultiPrefixEnsembleOperator with distinct prefixes for each language model.

    This function creates a MultiPrefixEnsembleOperator with example prefixes and language model modules,
    constructs sample input, executes the operator, and prints the aggregated responses.

    Returns:
        None.
    """

    # Define example prefixes to guide different response styles
    example_prefixes: List[str] = [
        "Analyze this from a scientific perspective:",
        "Consider this from a philosophical angle:",
        "Provide a practical approach to:",
    ]

    # Create LM modules with different models for diversity
    lm_modules = [
        LMModule(
            config=LMModuleConfig(
                model_name="anthropic:claude-3-opus", temperature=0.5, max_tokens=256
            )
        ),
        LMModule(
            config=LMModuleConfig(
                model_name="openai:gpt-4", temperature=0.7, max_tokens=256
            )
        ),
        LMModule(
            config=LMModuleConfig(
                model_name="anthropic:claude-3-haiku", temperature=0.3, max_tokens=256
            )
        ),
    ]

    # Instantiate the operator with named parameters.
    operator: MultiPrefixEnsembleOperator = MultiPrefixEnsembleOperator(
        lm_modules=lm_modules,
        prefixes=example_prefixes,
        name="MultiPrefixEnsembleExample",
    )

    # Create input data with a more substantive query
    inputs: MultiPrefixOperatorInputs = MultiPrefixOperatorInputs(
        query="How can we effectively combat climate change while balancing economic needs?"
    )

    # Execute the operator using __call__ with named parameters
    result = operator(inputs=inputs)

    # Display structured results
    print(f'Original query: "{inputs.query}"')
    print(f"\nNumber of responses: {len(result.responses)}")

    # Display each response with its corresponding prefix
    for i, (prefix, response) in enumerate(zip(example_prefixes, result.responses), 1):
        # Show the prefix and a truncated response for readability
        truncated = response[:100] + "..." if len(response) > 100 else response
        print(f"\nResponse {i}:")
        print(f'  Prefix: "{prefix}"')
        print(f'  Response: "{truncated}"')

    print(
        "\nNote: In a real application, these responses would be further processed or aggregated."
    )

class MultiPrefixOperatorInputs(EmberModel):
"""Input model for MultiPrefixEnsembleOperator.

    Attributes:
        query: The query string to be processed by the operator.
    """

    query: str = Field(description="The query to be processed by multiple LM modules")

class MultiPrefixOperatorOutputs(EmberModel):
"""Output model for MultiPrefixEnsembleOperator.

    Attributes:
        responses: The list of responses from different LM modules.
    """

    responses: List[str] = Field(description="Responses from different LM modules")

class MultiPrefixEnsembleSpecification(Specification):
"""Specification for MultiPrefixEnsembleOperator."""

    input_model: Type[EmberModel] = MultiPrefixOperatorInputs
    structured_output: Type[EmberModel] = MultiPrefixOperatorOutputs

class MultiPrefixEnsembleOperator(
Operator[MultiPrefixOperatorInputs, MultiPrefixOperatorOutputs]
):
"""Operator that applies different prefixes using multiple LM modules.

    This operator randomly selects prefixes from a predefined list and applies them
    to the user query before sending to different language model modules.
    """

    specification: ClassVar[Specification] = MultiPrefixEnsembleSpecification()
    lm_modules: List[LMModule]
    prefixes: List[str]

    def __init__(
        self,
        lm_modules: List[LMModule],
        prefixes: List[str],
        name: str = "MultiPrefixEnsemble",
    ) -> None:
        """Initializes a MultiPrefixEnsembleOperator instance.

        Args:
            lm_modules: A list of language model callables.
            prefixes: A list of prefix strings to be used for each LM call.
            name: The name identifier for this operator instance.
        """
        self.prefixes = prefixes
        self.lm_modules = lm_modules

    def forward(
        self, *, inputs: MultiPrefixOperatorInputs
    ) -> MultiPrefixOperatorOutputs:
        """Apply different prefixes to the query and process through LM modules.

        Args:
            inputs: Validated input data containing the query.

        Returns:
            Structured output containing responses from all LM modules.
        """
        # Randomly select prefixes to match the number of LM modules
        chosen_prefixes = sample(self.prefixes, len(self.lm_modules))

        # Process each query with a different prefix through its LM module
        responses = []
        for prefix, lm in zip(chosen_prefixes, self.lm_modules):
            # Generate prompt with prefix
            prompt = f"{prefix}\n{inputs.query}"

            # Call LM module
            response = lm(prompt=prompt)

            # Get text from response
            text = response if isinstance(response, str) else str(response)

            # Ensure we have a valid string
            text = text.strip() if text else ""

            responses.append(text)

        # Return structured output
        return MultiPrefixOperatorOutputs(responses=responses)

def main() -> None:
"""Main entry point that runs the usage example.

    Returns:
        None.
    """
    usage_example()

if **name** == "**main**":
main()

</code>

src\ember\examples\operators\README.md:
<code>

# Ember Operator Examples

This directory contains examples demonstrating how to create, compose, and use Ember operators for building AI systems.

## Examples

- `minimal_operator_example.py` - Basic operator creation and usage
- `composition_example.py` - Composing multiple operators together
- `container_operator_example.py` - Using container operators for complex logic
- `diverse_ensemble_operator_example.py` - Building ensembles of different models
- `container_simplified.py` - Simplified container operator pattern
- `custom_prompt_example_caravan.py` - Using custom prompts with operators

## Running Examples

To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/operators/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/operators/example_name.py
```

Replace `example_name.py` with the desired example file.

## Operator Concepts

Ember operators are the fundamental building blocks for AI systems, similar to PyTorch's `nn.Module`. They provide:

- Type-safe input/output interfaces
- Specification-driven execution
- Support for composition and nesting
- Automatic optimization via XCS

## Next Steps

After mastering operators, explore:

- `xcs/` - For advanced execution optimization
- `advanced/` - For complex operator patterns and architectures

</code>

src\ember\examples\operators\simplified_ensemble_example.py:
<code>
"""Simplified Ensemble Operator Example

This example demonstrates an ensemble operator implementation that doesn't require
API access. It shows how to create, compose, and execute ensemble operators with proper
typing and simulation of multiple model responses.

To run:
uv run python src/ember/examples/operators/simplified_ensemble_example.py
"""

import logging
import random
from typing import Any, ClassVar, List, Type

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel, Field

###############################################################################

# Custom Input/Output Models

###############################################################################

class EnsembleInput(EmberModel):
"""Input for ensemble operator."""

    query: str = Field(description="The query to be processed")

class EnsembleOutput(EmberModel):
"""Output for ensemble operator."""

    responses: List[str] = Field(description="Multiple responses from the ensemble")
    final_answer: str = Field(description="The selected final answer")
    confidence: float = Field(description="Confidence in the final answer")

class EnsembleSpecification(Specification):
"""Specification for ensemble operator."""

    input_model: Type[EmberModel] = EnsembleInput
    structured_output: Type[EmberModel] = EnsembleOutput

###############################################################################

# Mock LM Module (Simulated Model API)

###############################################################################

class MockLMModule:
"""Simulates a language model without requiring API access."""

    def __init__(self, response_style: str = "normal", response_quality: str = "good"):
        """Initialize with configuration for response generation style.

        Args:
            response_style: The style of response to generate
            response_quality: The quality level of responses
        """
        self.response_style = response_style
        self.response_quality = response_quality

        # Some variety for each model
        self.style_phrases = {
            "concise": ["In short", "Briefly", "To summarize"],
            "detailed": [
                "In a comprehensive analysis",
                "Considering all factors",
                "In detail",
            ],
            "creative": ["Imagine", "Creatively speaking", "From a unique perspective"],
            "normal": ["", "Well,", "In response to your question"],
        }

        # Quality variants for simulation
        self.quality_modifiers = {
            "good": 0.9,  # High chance of correct answer
            "medium": 0.7,  # Moderate chance of correct answer
            "poor": 0.4,  # Low chance of correct answer
        }

        # Sample answers for common questions
        self.known_answers = {
            "capital of france": "Paris",
            "largest planet": "Jupiter",
            "closest star": "The Sun",
            "author of hamlet": "William Shakespeare",
            "tallest mountain": "Mount Everest",
            "number of planets": "Eight planets (or nine including Pluto)",
        }

    def __call__(self, prompt: str, **kwargs: Any) -> str:
        """Generate a simulated response to the prompt.

        Args:
            prompt: The input text prompt
            **kwargs: Additional configuration parameters

        Returns:
            A simulated model response
        """
        # Select intro phrase based on style
        intro = random.choice(self.style_phrases.get(self.response_style, ["Well,"]))

        # Deterministically select the "correct" answer based on the prompt
        normalized_prompt = prompt.lower()
        answer = None

        # Check for known answers in the prompt
        for key, value in self.known_answers.items():
            if key in normalized_prompt:
                answer = value
                break

        # Default answer if no matching pattern
        if answer is None:
            answer = (
                "I don't have enough information to answer that question accurately."
            )

        # Based on quality, maybe give incorrect answer
        quality_score = self.quality_modifiers.get(self.response_quality, 0.7)
        if random.random() > quality_score:
            # Get a random wrong answer
            wrong_answers = list(self.known_answers.values())
            if answer in wrong_answers:
                wrong_answers.remove(answer)
            if wrong_answers:
                answer = random.choice(wrong_answers)

        # Add some style variations
        if self.response_style == "concise":
            return f"{intro}, {answer}"
        elif self.response_style == "detailed":
            return f"{intro}, I can tell you that {answer}. This is based on well-established facts."
        elif self.response_style == "creative":
            return f"{intro}, let's explore the answer: {answer} - which opens up interesting possibilities!"
        else:
            return f"{intro} {answer}"

###############################################################################

# Ensemble Operator

###############################################################################

class SimpleEnsembleOperator(Operator[EnsembleInput, EnsembleOutput]):
"""Operator that generates multiple responses using different simulated LM modules."""

    # Class-level specification declaration
    specification: ClassVar[Specification] = EnsembleSpecification()

    # Instance attributes
    lm_modules: List[MockLMModule]

    def __init__(self, num_units: int = 3) -> None:
        """Initialize with multiple simulated LM modules.

        Args:
            num_units: Number of simulated models to use
        """
        # Create a variety of LM modules with different styles and qualities
        styles = ["normal", "concise", "detailed", "creative"]
        qualities = ["good", "good", "medium", "poor"]  # Weighted toward good answers

        self.lm_modules = []
        for i in range(num_units):
            style = styles[i % len(styles)]
            quality = qualities[i % len(qualities)]
            self.lm_modules.append(
                MockLMModule(response_style=style, response_quality=quality)
            )

    def forward(self, *, inputs: EnsembleInput) -> EnsembleOutput:
        """Generate multiple responses for the input query.

        Args:
            inputs: The validated input containing a query

        Returns:
            Ensemble output containing multiple responses and aggregation
        """
        # Generate responses from all modules
        responses = [lm(inputs.query) for lm in self.lm_modules]

        # Simple aggregation - select most common response (like MostCommon operator)
        # Count occurrences of each response
        response_counts = {}
        for response in responses:
            response_counts[response] = response_counts.get(response, 0) + 1

        # Find the most common response
        max_count = 0
        final_answer = ""
        for response, count in response_counts.items():
            if count > max_count:
                max_count = count
                final_answer = response

        # If we couldn't find a most common, use the first response
        if not final_answer and responses:
            final_answer = responses[0]

        # Calculate confidence based on agreement ratio
        confidence = max_count / len(responses) if responses else 0.0

        return EnsembleOutput(
            responses=responses, final_answer=final_answer, confidence=confidence
        )

###############################################################################

# Main Demonstration

###############################################################################

def main() -> None:
"""Run the ensemble operator example."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("\n=== Simplified Ensemble Operator Example ===\n")

    # Create the ensemble
    ensemble = SimpleEnsembleOperator(num_units=5)

    # Example queries
    queries = [
        "What is the capital of France?",
        "What is the largest planet in our solar system?",
        "Who wrote Hamlet?",
        "What is the answer to an unknown question?",
    ]

    # Process all queries
    for query in queries:
        print(f"\nProcessing query: {query}")

        # Execute the operator with the query
        result = ensemble(inputs=EnsembleInput(query=query))

        # Display results
        print(f"Final answer: {result.final_answer}")
        print(f"Confidence: {result.confidence:.2f}")
        print(f"All responses ({len(result.responses)}):")
        for i, response in enumerate(result.responses, 1):
            print(f"  {i}. {response}")

    print("\nNote: This example demonstrates the ensemble pattern without requiring")
    print("      actual API access, using simulated model responses instead.")

if **name** == "**main**":
main()

</code>

src\ember\examples\operators\_\_init\_\_.py:
<code>

</code>

src\ember\examples\xcs\auto_graph_example.py:
<code>
"""Automatic Graph Building Example.

This example demonstrates the enhanced JIT API with automatic graph building.
It shows how applying @jit to operators enables automatic graph building and
parallel execution without requiring manual graph construction.

To run:
uv run python -m src.ember.examples.xcs.auto_graph_example
"""

import logging
import time
from typing import ClassVar, Optional, Type

# simplified import from ember API

from ember.api.xcs import execution_options, jit

# Import pre-built operators from standard library

from ember.core import non

# ember imports

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel

###############################################################################

# Input/Output Models

###############################################################################

class QueryInput(EmberModel):
"""Input model for query operations.

    Attributes:
        query: The text query to be processed.
    """

    query: str

class AggregationOutput(EmberModel):
"""Output model for aggregation operations.

    Attributes:
        final_answer: The aggregated response.
    """

    final_answer: Optional[str]

# Specification for pipeline

class PipelineSpecification(Specification):
"""Specification for the full pipeline."""

    input_model: Type[EmberModel] = QueryInput
    structured_output: Type[EmberModel] = AggregationOutput

###############################################################################

# Pipeline Classes

###############################################################################

class SimplePipeline(Operator[QueryInput, AggregationOutput]):
"""Pipeline that demonstrates automatic graph building.

    This pipeline internally uses an ensemble and aggregator, but doesn't
    use JIT yet. It will be a baseline for comparison.
    """

    # Class-level specification
    specification: ClassVar[Specification] = PipelineSpecification()

    # Field declarations
    ensemble: non.UniformEnsemble
    aggregator: non.MostCommon

    def __init__(
        self,
        *,
        model_name: str = "openai:gpt-4o-mini",
        num_units: int = 3,
        temperature: float = 0.7,
    ) -> None:
        """Initialize the pipeline with configurable parameters.

        Args:
            model_name: The model to use
            num_units: Number of ensemble units
            temperature: Temperature for generation
        """
        self.ensemble = non.UniformEnsemble(
            num_units=num_units, model_name=model_name, temperature=temperature
        )
        self.aggregator = non.MostCommon()

    def forward(self, *, inputs: QueryInput) -> AggregationOutput:
        """Execute the pipeline on the given inputs.

        Args:
            inputs: Contains the query to process

        Returns:
            The aggregated result with final answer
        """
        # Create dictionary input for the ensemble
        ensemble_inputs = {"query": inputs.query}

        # Execute the ensemble operator
        ensemble_result = self.ensemble.forward(inputs=ensemble_inputs)

        # Create input for the aggregator
        aggregator_inputs = {
            "query": inputs.query,
            "responses": ensemble_result["responses"],
        }

        # Execute the aggregator operator
        aggregator_result = self.aggregator.forward(inputs=aggregator_inputs)

        # Return the final result
        return AggregationOutput(final_answer=aggregator_result["final_answer"])

###############################################################################

# JIT-Decorated Pipeline

###############################################################################

@jit
class JITPipeline(Operator[QueryInput, AggregationOutput]):
"""Pipeline with JIT optimization.

    This pipeline uses the @jit decorator to enable automatic graph building and
    optimization.
    """

    # Class-level specification
    specification: ClassVar[Specification] = PipelineSpecification()

    # Field declarations
    ensemble: non.UniformEnsemble
    aggregator: non.MostCommon

    def __init__(
        self,
        *,
        model_name: str = "openai:gpt-4o-mini",
        num_units: int = 3,
        temperature: float = 0.7,
    ) -> None:
        """Initialize the pipeline with configurable parameters.

        Args:
            model_name: The model to use
            num_units: Number of ensemble units
            temperature: Temperature for generation
        """
        # Create component operators
        self.ensemble = non.UniformEnsemble(
            num_units=num_units, model_name=model_name, temperature=temperature
        )
        self.aggregator = non.MostCommon()

    def forward(self, *, inputs: QueryInput) -> AggregationOutput:
        """Execute the pipeline on the given inputs with JIT optimization.

        Args:
            inputs: Contains the query to process

        Returns:
            The aggregated result with final answer
        """
        # Create dictionary input for the ensemble
        ensemble_inputs = {"query": inputs.query}

        # Execute the ensemble operator
        ensemble_result = self.ensemble.forward(inputs=ensemble_inputs)

        # Create input for the aggregator
        aggregator_inputs = {
            "query": inputs.query,
            "responses": ensemble_result["responses"],
        }

        # Execute the aggregator operator
        aggregator_result = self.aggregator.forward(inputs=aggregator_inputs)

        # Return the final result
        return AggregationOutput(final_answer=aggregator_result["final_answer"])

###############################################################################

# Main Demonstration

###############################################################################

def main() -> None:
"""Run demonstration of automatic graph building."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("\n=== Automatic Graph Building Example ===\n")

    # Example queries to demonstrate caching and reuse
    queries = [
        "What is the capital of France?",
        "What is the tallest mountain in the world?",
        "Who wrote Romeo and Juliet?",
    ]

    # First, regular pipeline without JIT
    print("Running baseline (no JIT):")
    baseline_pipeline = SimplePipeline(model_name="openai:gpt-4o-mini", num_units=3)

    for i, query in enumerate(queries[:1]):  # Just run one query as baseline
        print(f"\nQuery {i+1}: {query}")

        # Create typed input
        query_input = QueryInput(query=query)

        start_time = time.perf_counter()
        result = baseline_pipeline(inputs=query_input)
        elapsed = time.perf_counter() - start_time

        print(f"Answer: {result.final_answer}")
        print(f"Time: {elapsed:.4f}s")

    baseline_time = elapsed

    # Now with JIT
    print("\nRunning with JIT optimization:")
    jit_pipeline = JITPipeline(model_name="openai:gpt-4o-mini", num_units=3)

    print("\nFirst run - expect graph building overhead:")

    for i, query in enumerate(queries):
        print(f"\nQuery {i+1}: {query}")

        # Create typed input
        query_input = QueryInput(query=query)

        start_time = time.perf_counter()
        result = jit_pipeline(inputs=query_input)
        elapsed = time.perf_counter() - start_time

        print(f"Answer: {result.final_answer}")
        print(f"Time: {elapsed:.4f}s")

        # Save time of first query for comparison
        if i == 0:
            first_jit_time = elapsed

    # Repeat first query to demonstrate caching benefits
    print("\nRepeat first query to demonstrate cached execution:")
    query_input = QueryInput(query=queries[0])

    start_time = time.perf_counter()
    result = jit_pipeline(inputs=query_input)
    cached_time = time.perf_counter() - start_time

    print(f"Answer: {result.final_answer}")
    print(f"Time: {cached_time:.4f}s")

    # With execution options for sequential execution
    print("\nUsing execution_options to control execution:")
    with execution_options(use_parallel=False):
        query_input = QueryInput(query="What is the speed of light?")

        start_time = time.perf_counter()
        result = jit_pipeline(inputs=query_input)
        seq_time = time.perf_counter() - start_time

        print(f"Answer: {result.final_answer}")
        print(f"Time: {seq_time:.4f}s (sequential execution)")

    # Print summary
    print("\n=== Performance Summary ===")
    print(f"Baseline time: {baseline_time:.4f}s")
    print(f"First JIT run: {first_jit_time:.4f}s (includes compilation)")
    print(f"Cached JIT run: {cached_time:.4f}s")
    print(f"Sequential JIT: {seq_time:.4f}s")

    if cached_time > 0 and baseline_time > 0:
        speedup = (baseline_time - cached_time) / baseline_time * 100
        print(f"\nJIT Speedup: {speedup:.1f}% faster than baseline")

    if cached_time > 0 and first_jit_time > 0:
        cache_benefit = (first_jit_time - cached_time) / first_jit_time * 100
        print(f"Caching Benefit: {cache_benefit:.1f}% faster after first run")

    print("\n=== Key Benefits of JIT ===")
    print("1. Automatic operator dependency analysis")
    print("2. Optimized execution with caching")
    print("3. Reduced overhead for complex pipelines")
    print("4. No manual graph construction required")

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\auto_graph_simplified.py:
<code>
"""Simplified Automatic Graph Building Example.

This example demonstrates the enhanced JIT API with automatic graph building
without requiring LLM API calls. It shows how the @jit decorator can automatically
optimize operator execution by tracing execution patterns and building optimized
execution graphs.

This example highlights the trace-based JIT approach using the @jit decorator,
which is one of three complementary approaches in Ember's JIT system (the others
being structural_jit for structure-based analysis and autograph for manual graph
construction).

For a comprehensive explanation of the relationship between these approaches,
see docs/xcs/JIT_OVERVIEW.md.

To run:
uv run python src/ember/examples/xcs/auto_graph_simplified.py
"""

import logging
import time
from typing import ClassVar, Type

from pydantic import Field

from ember.api.xcs import execution_options, jit
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel

###############################################################################

# Input/Output Models

###############################################################################

class AdditionInput(EmberModel):
"""Input model for math operations.

    Attributes:
        value: The integer value to be processed.
    """

    value: int = Field(description="The integer value to be processed")

class AdditionOutput(EmberModel):
"""Output model for math operations.

    Attributes:
        value: The processed integer value.
    """

    value: int = Field(description="The processed integer value")

class AdditionSpec(Specification):
"""Specification for math operation operators."""

    input_model: Type[EmberModel] = AdditionInput
    structured_output: Type[EmberModel] = AdditionOutput

###############################################################################

# JIT-Decorated Operators

###############################################################################

@jit()
class AddTenOperator(Operator[AdditionInput, AdditionOutput]):
"""Operator that adds 10 to the input."""

    specification: ClassVar[Specification] = AdditionSpec()

    def forward(self, *, inputs: AdditionInput) -> AdditionOutput:
        """Add 10 to the input value.

        Args:
            inputs: The input containing the value to increment by 10.

        Returns:
            Output with the value incremented by 10.
        """
        time.sleep(0.1)  # Simulate processing time
        return AdditionOutput(value=inputs.value + 10)

@jit()
class MultiplyByTwoOperator(Operator[AdditionInput, AdditionOutput]):
"""Operator that multiplies the input by 2."""

    specification: ClassVar[Specification] = AdditionSpec()

    def forward(self, *, inputs: AdditionInput) -> AdditionOutput:
        """Multiply the input value by 2.

        Args:
            inputs: The input containing the value to multiply by 2.

        Returns:
            Output with the value multiplied by 2.
        """
        time.sleep(0.1)  # Simulate processing time
        return AdditionOutput(value=inputs.value * 2)

###############################################################################

# Pipeline Class

###############################################################################

@jit()
class MathPipeline(Operator[AdditionInput, AdditionOutput]):
"""Pipeline that demonstrates automatic graph building."""

    specification: ClassVar[Specification] = AdditionSpec()

    # Define instance attributes with type hints
    add_ten: AddTenOperator
    multiply: MultiplyByTwoOperator

    def __init__(self) -> None:
        """Initialize with add_ten and multiply operators."""
        self.add_ten = AddTenOperator()
        self.multiply = MultiplyByTwoOperator()

    def forward(self, *, inputs: AdditionInput) -> AdditionOutput:
        """Execute the pipeline with automatic graph building.

        Args:
            inputs: The input containing the initial value for processing.

        Returns:
            Output with the value after both operations are applied.
        """
        # First add 10
        add_result = self.add_ten(inputs=inputs)

        # Then multiply by 2
        final_result = self.multiply(inputs=add_result)

        return final_result

def main() -> None:
"""Run demonstration of automatic graph building."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("\n=== Simplified Automatic Graph Building Example ===\n")

    # Create the pipeline
    pipeline = MathPipeline()

    # Test with different inputs
    inputs = [
        AdditionInput(value=5),  # Should result in (5+10)*2 = 30
        AdditionInput(value=10),  # Should result in (10+10)*2 = 40
        AdditionInput(value=15),  # Should result in (15+10)*2 = 50
    ]

    # First run - trace and build graph
    print("First run - expect graph building overhead:")
    for i, input_data in enumerate(inputs):
        start_time = time.time()
        result = pipeline(inputs=input_data)
        elapsed = time.time() - start_time
        print(f"\nInput {i+1}: {input_data}")
        print(f"Result: {result}")
        print(f"Value from result: {result.value}")
        # Show computation steps
        print(
            f"Computation: {input_data.value} + 10 = {input_data.value + 10}, then × 2 = {(input_data.value + 10) * 2}"
        )
        print(f"Time: {elapsed:.4f}s")

    # Second run - should use cached graph
    print("\nRepeat first input to demonstrate cached execution:")
    start_time = time.time()
    result = pipeline(inputs=inputs[0])
    elapsed = time.time() - start_time
    print(f"Result: {result}")
    print(f"Value from result: {result.value}")
    print(f"Time: {elapsed:.4f}s")
    print("Note: Second run is typically faster due to cached execution")

    # With execution options
    print("\nUsing execution_options to control execution:")
    with execution_options(scheduler="sequential"):
        start_time = time.time()
        result = pipeline(
            inputs=AdditionInput(value=20)
        )  # Should result in (20+10)*2 = 60
        elapsed = time.time() - start_time
        print(f"Result: {result}")
        print(f"Value from result: {result.value}")
        # Show computation steps
        print("Computation: 20 + 10 = 30, then × 2 = 60")
        print(f"Time: {elapsed:.4f}s (sequential execution)")

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\enhanced_jit_example.py:
<code>
"""Enhanced JIT API Demonstration.

This script demonstrates nested operator analysis using the enhanced JIT system.
It shows how the hierarchical analysis in the AutoGraphBuilder correctly identifies
dependencies between operators while respecting their hierarchical relationships.

This is a key component of both the @jit decorator (which uses trace-based graph building)
and the @structural_jit decorator (which uses structure-based graph building).

For a comprehensive explanation of the relationship between these approaches,
see docs/xcs/JIT_OVERVIEW.md.

To run:
uv run python src/ember/examples/xcs/enhanced_jit_example.py
"""

import logging

from ember.xcs.graph.xcs_graph import XCSGraph

# For tracing with the autograph module

from ember.xcs.tracer.autograph import AutoGraphBuilder
from ember.xcs.tracer.xcs_tracing import TraceRecord

###############################################################################

# Mock Execution Setup

###############################################################################

def build_graph_example() -> None:
"""Demonstrate graph building from trace records with nested operators."""

    # Create trace records that represent a nested execution pattern:
    # Top level pipeline contains nested operators
    records = [
        TraceRecord(
            operator_name="MainPipeline",
            node_id="pipeline1",
            inputs={"query": "What is machine learning?"},
            outputs={"result": "pipeline_result"},
            timestamp=1.0,
        ),
        TraceRecord(
            operator_name="Refiner",
            node_id="refiner1",
            inputs={"query": "What is machine learning?"},
            outputs={"refined_query": "Improved: What is machine learning?"},
            timestamp=1.1,  # Executed during pipeline
        ),
        TraceRecord(
            operator_name="Ensemble",
            node_id="ensemble1",
            inputs={"refined_query": "Improved: What is machine learning?"},
            outputs={"responses": ["Answer 1", "Answer 2"]},
            timestamp=1.2,  # Executed during pipeline
        ),
        TraceRecord(
            operator_name="Generator1",
            node_id="gen1",
            inputs={"refined_query": "Improved: What is machine learning?"},
            outputs={"answer": "Answer 1"},
            timestamp=1.21,  # Executed during ensemble
        ),
        TraceRecord(
            operator_name="Generator2",
            node_id="gen2",
            inputs={"refined_query": "Improved: What is machine learning?"},
            outputs={"answer": "Answer 2"},
            timestamp=1.22,  # Executed during ensemble
        ),
        TraceRecord(
            operator_name="Aggregator",
            node_id="agg1",
            inputs={"responses": ["Answer 1", "Answer 2"]},
            outputs={"final_answer": "Machine learning is..."},
            timestamp=1.3,  # Executed during pipeline
        ),
        TraceRecord(
            operator_name="NextQuery",
            node_id="next1",
            inputs={
                "previous_result": "Machine learning is...",
                "answer": "Answer 1",  # This creates a data dependency with Generator1's output
            },
            outputs={"new_query": "Tell me more about supervised learning"},
            timestamp=2.0,  # Executed after pipeline
        ),
    ]

    # Build graph with standard dependency analysis (no hierarchy awareness)
    basic_builder = AutoGraphBuilder()
    # Disable hierarchy analysis by providing empty map
    basic_builder._build_hierarchy_map = lambda records: {}
    basic_graph = basic_builder.build_graph(records)

    # Build graph with hierarchical dependency analysis
    enhanced_builder = AutoGraphBuilder()
    # Explicitly define hierarchy to demonstrate the point more clearly
    hierarchy_map = {
        "pipeline1": ["refiner1", "ensemble1", "agg1"],
        "ensemble1": ["gen1", "gen2"],
    }
    enhanced_builder._build_hierarchy_map = lambda records: hierarchy_map
    enhanced_graph = enhanced_builder.build_graph(records)

    # Print the results
    print("\n--- BASIC GRAPH (without hierarchical analysis) ---")
    print_graph_dependencies(basic_graph)

    print("\n--- ENHANCED GRAPH (with hierarchical analysis) ---")
    print_graph_dependencies(enhanced_graph)

    # Print the key differences
    print("\n--- KEY DIFFERENCES (EXPECTED) ---")
    print("In a correctly implemented hierarchical analysis:")
    print("1. NextQuery should NOT depend on Generator1 and Generator2 directly")
    print("   (since they are nested inside Ensemble)")
    print("2. Aggregator should NOT depend on Generator1 and Generator2 directly")
    print("   (should depend only on Ensemble)")

    # Analyze both graphs to find actual differences
    basic_deps = {}
    enhanced_deps = {}

    for node_id, node in basic_graph.nodes.items():
        basic_deps[node_id] = list(node.inbound_edges)

    for node_id, node in enhanced_graph.nodes.items():
        enhanced_deps[node_id] = list(node.inbound_edges)

    print("\n--- ACTUAL DIFFERENCES DETECTED ---")
    for node_id in basic_deps.keys():
        basic_deps_set = set(basic_deps.get(node_id, []))
        enhanced_deps_set = set(enhanced_deps.get(node_id, []))

        if basic_deps_set != enhanced_deps_set:
            print(f"Node: {node_id}")
            print(
                f"  Basic dependencies: {', '.join(basic_deps_set) if basic_deps_set else 'None'}"
            )
            print(
                f"  Enhanced dependencies: {', '.join(enhanced_deps_set) if enhanced_deps_set else 'None'}"
            )
            print(
                f"  Removed in enhanced: {', '.join(basic_deps_set - enhanced_deps_set) if basic_deps_set - enhanced_deps_set else 'None'}"
            )
            print(
                f"  Added in enhanced: {', '.join(enhanced_deps_set - basic_deps_set) if enhanced_deps_set - basic_deps_set else 'None'}"
            )
            print()

def print_graph_dependencies(graph: XCSGraph) -> None:
"""Print the dependencies in a graph."""
for node_id, node in graph.nodes.items():
if node.inbound_edges:
print(f"{node_id} depends on: {', '.join(node.inbound_edges)}")
else:
print(f"{node_id}: No dependencies")

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run the nested operator analysis demonstration."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("Enhanced JIT Example - Testing Hierarchical Dependency Analysis")
    print(
        "This demonstrates how the enhanced JIT system correctly handles nested operators.\n"
    )

    build_graph_example()

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\example_simplified_xcs.py:
<code>
"""
Example demonstrating the simplified XCS import structure.

This example shows how to use the new top-level imports for XCS functionality
with the ember.api package.

To run:
poetry run python src/ember/examples/example_simplified_xcs.py
"""

from typing import ClassVar, Optional, Type

from ember.api.xcs import jit, pmap, vmap
from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.core.types.ember_model import EmberModel

# Import the API for advanced configuration

# Create input/output models for our operators

class QueryInput(EmberModel):
query: str

class QueryOutput(EmberModel):
result: str

# Define specifications for our operators

class SimpleOperatorSpec(Specification[QueryInput, QueryOutput]):
input_model: Optional[Type[QueryInput]] = QueryInput
structured_output: Optional[Type[QueryOutput]] = QueryOutput

# Create a simple operator

@jit # Simple JIT usage
class SimpleOperator(Operator[QueryInput, QueryOutput]): # Define the specification
specification: ClassVar[Specification] = SimpleOperatorSpec()

    def forward(self, *, inputs: QueryInput) -> QueryOutput:
        return QueryOutput(result=inputs.query.upper())

# Define specification for advanced operator

class AdvancedOperatorSpec(Specification[QueryInput, QueryOutput]):
input_model: Optional[Type[QueryInput]] = QueryInput
structured_output: Optional[Type[QueryOutput]] = QueryOutput

# Use advanced JIT options

@jit(sample_input={"query": "precompile"})
class AdvancedOperator(Operator[QueryInput, QueryOutput]): # Define the specification
specification: ClassVar[Specification] = AdvancedOperatorSpec()

    def forward(self, *, inputs: QueryInput) -> QueryOutput:
        return QueryOutput(result=inputs.query + "!")

def main():
"""Run the example demonstrating simplified XCS imports."""
print("\n=== Simplified XCS Import Example ===\n")

    # Create and use the operators
    simple_op = SimpleOperator()
    advanced_op = AdvancedOperator()

    # Demonstrate the operators in action
    print("Simple Operator Demo:")
    result1 = simple_op(inputs={"query": "hello world"})
    print("  Input: 'hello world'")
    print(f"  Output: '{result1['result']}'")  # Should be "HELLO WORLD"

    print("\nAdvanced Operator Demo:")
    result2 = advanced_op(inputs={"query": "precompiled input"})
    print("  Input: 'precompiled input'")
    print(f"  Output: '{result2['result']}'")  # Should be "precompiled input!"

    # Vectorization example
    def process_item(inputs):
        # Process the input and return a dictionary
        return {"result": inputs["values"] * 2}

    # Vectorize the function
    print("\nVectorization Example:")
    batch_process = vmap(process_item)
    input_values = [1, 2, 3]
    # Note: vmap expects a dict with keyword 'inputs'
    batch_result = batch_process(inputs={"values": input_values})
    print(f"  Inputs: {input_values}")
    print(f"  Vectorized Output: {batch_result}")  # Should be {"result": [2, 4, 6]}

    # Parallelize the function
    print("\nParallelization Example:")
    parallel_process = pmap(process_item)
    print("  The pmap decorator enables parallel processing across multiple cores")
    print("  Usage: parallel_process(inputs={'values': [1, 2, 3]})")

    # Show autograph example
    print("\nAutograph Example:")
    print("  The autograph decorator captures function calls as a computational graph")
    print("  @autograph")
    print("  def my_function(x):")
    print("      return process1(process2(x))")

    # Show execution options
    print("\nExecution Options Example:")
    print("  with execution_options(scheduler='parallel'):")
    print("      result = my_complex_operation(data)")

    print("\nXCS API import example complete!")
    print(
        "These APIs provide a simple, intuitive interface to Ember's execution framework."
    )

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\jit_example.py:
<code>
"""JIT Ensemble Demonstration.

This module demonstrates three variants of a "LargeEnsemble" operator using Ember's
API: 1) BaselineEnsemble: Executes eagerly without parallelism 2) ParallelEnsemble: Leverages concurrency via a scheduling plan 3) JITEnsemble: Combines parallel execution with JIT tracing to cache the concurrency plan

It measures the total and per-query execution times for each approach, highlighting the
performance benefits of JIT compilation with the @jit decorator. This example focuses
on the trace-based JIT approach, which is one of three complementary approaches in Ember's
JIT system (the others being structural_jit and autograph).

For a comprehensive explanation of the relationship between these approaches,
see docs/xcs/JIT_OVERVIEW.md.

To run:
uv run python src/ember/examples/xcs/jit_example.py
"""

import logging
import time
from typing import ClassVar, List, Tuple, Type

# ember API imports

from ember.api.xcs import execution_options, jit
from ember.core.registry.operator.base.operator_base import Operator, Specification
from ember.core.types.ember_model import EmberModel, Field

###############################################################################

# Input/Output Models

###############################################################################
class EnsembleInput(EmberModel):
"""Input for the ensemble operators."""

    query: str = Field(description="The query to send to the ensemble")

class EnsembleOutput(EmberModel):
"""Output from the ensemble operators."""

    query: str = Field(description="The original query")
    responses: List[str] = Field(description="List of responses from the ensemble")

# Proper specification for the ensemble operators

class EnsembleSpecification(Specification):
"""Specification for the ensemble operators."""

    input_model: Type[EmberModel] = EnsembleInput
    structured_output: Type[EmberModel] = EnsembleOutput

###############################################################################

# BaselineEnsemble - Eager Execution (No Concurrency)

###############################################################################
class BaselineEnsemble(Operator):
"""Ensemble implementation that forces fully eager (serial) execution.

    This subclass configures the execution to run serially rather than in parallel
    by using appropriate execution options.
    """

    specification: ClassVar[Specification] = EnsembleSpecification()

    def __init__(
        self,
        *,
        num_units: int = 3,
        model_name: str = "openai:gpt-4o-mini",
        temperature: float = 0.7,
    ) -> None:
        """Initialize with sequential execution options."""
        self.num_units = num_units
        self.model_name = model_name
        self.temperature = temperature

    def forward(self, *, inputs: EnsembleInput) -> EnsembleOutput:
        """Process inputs in a sequential manner."""
        # Simulate execution by introducing a delay
        time.sleep(0.1 * self.num_units)

        # Generate mock responses
        responses = [
            f"Response {i} to query: {inputs.query}" for i in range(self.num_units)
        ]

        return EnsembleOutput(query=inputs.query, responses=responses)

###############################################################################

# ParallelEnsemble - Standard Concurrency

###############################################################################
class ParallelEnsemble(Operator):
"""Ensemble implementation that leverages standard concurrency.

    This implementation prepares for parallel execution with proper XCS integration.
    """

    specification: ClassVar[Specification] = EnsembleSpecification()

    def __init__(
        self,
        *,
        num_units: int = 3,
        model_name: str = "openai:gpt-4o-mini",
        temperature: float = 0.7,
    ) -> None:
        """Initialize with standard configuration."""
        self.num_units = num_units
        self.model_name = model_name
        self.temperature = temperature

    def forward(self, *, inputs: EnsembleInput) -> EnsembleOutput:
        """Process inputs with potential for parallelism."""
        # Simulate execution by introducing a delay (smaller for parallel)
        time.sleep(0.05 * self.num_units)

        # Generate mock responses
        responses = [
            f"Parallel response {i} to query: {inputs.query}"
            for i in range(self.num_units)
        ]

        return EnsembleOutput(query=inputs.query, responses=responses)

###############################################################################

# JITEnsemble - Parallel Execution with JIT Tracing

###############################################################################
@jit
class JITEnsemble(ParallelEnsemble):
"""Ensemble implementation with JIT tracing for optimized concurrency.

    Uses the same parallel approach as ParallelEnsemble but with JIT decoration. The first call
    triggers tracing and caching of the execution plan, reducing overhead for subsequent
    invocations.
    """

    specification: ClassVar[Specification] = EnsembleSpecification()

    def __init__(
        self,
        *,
        num_units: int = 3,
        model_name: str = "openai:gpt-4o-mini",
        temperature: float = 0.7,
    ) -> None:
        """Initialize with JIT capabilities."""
        super().__init__(
            num_units=num_units, model_name=model_name, temperature=temperature
        )
        # The @jit decorator will handle caching the execution plan

    def forward(self, *, inputs: EnsembleInput) -> EnsembleOutput:
        """Process inputs with JIT optimization."""
        # Simulate execution by introducing an even smaller delay (JIT is fastest)
        time.sleep(0.02 * self.num_units)

        # Generate mock responses
        responses = [
            f"JIT response {i} to query: {inputs.query}" for i in range(self.num_units)
        ]

        return EnsembleOutput(query=inputs.query, responses=responses)

def run_operator_queries(
\*,
operator_instance: Operator,
queries: List[str],
name: str,
mode: str = "parallel",
) -> Tuple[List[float], float, List[EnsembleOutput]]:
"""Execute the given ensemble operator for each query and measure execution times.

    Args:
        operator_instance: The ensemble operator instance to run.
        queries: List of query strings.
        name: Name for logging purposes.
        mode: Execution mode ("parallel" or "sequential")

    Returns:
        Tuple containing:
            1. A list of per-query execution times.
            2. The total execution time for all queries.
            3. A list of result objects.
    """
    execution_times: List[float] = []
    results: List[EnsembleOutput] = []
    total_start_time: float = time.perf_counter()

    # Set the execution options based on the mode
    with execution_options(scheduler=mode):
        for query in queries:
            query_start_time: float = time.perf_counter()
            result = operator_instance(inputs={"query": query})
            query_end_time: float = time.perf_counter()

            elapsed_time: float = query_end_time - query_start_time
            execution_times.append(elapsed_time)
            results.append(result)

            # Type-safe way to access responses length
            response_count = (
                len(result.responses) if isinstance(result, EnsembleOutput) else 0
            )

            logging.info(
                "[%s] Query='%s' => #responses=%d | time=%.4fs",
                name.upper(),
                query,
                response_count,
                elapsed_time,
            )

    total_end_time: float = time.perf_counter()
    total_elapsed_time: float = total_end_time - total_start_time
    return execution_times, total_elapsed_time, results

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run demonstrations comparing Baseline, Parallel, and JIT ensembles.

    This function constructs ensemble operator instances, executes a series of queries
    using each ensemble variant, and prints a consolidated timing summary with visualization.
    """
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    # Define ensemble configuration parameters.
    model_name: str = "openai:gpt-4o-mini"  # You can change this to any available model
    temperature: float = 0.7
    num_units: int = 5  # Number of ensemble units (sub-calls)

    # Create ensemble operator instances.
    baseline_op = BaselineEnsemble(
        num_units=num_units, model_name=model_name, temperature=temperature
    )
    parallel_op = ParallelEnsemble(
        num_units=num_units, model_name=model_name, temperature=temperature
    )
    jit_op = JITEnsemble(
        num_units=num_units, model_name=model_name, temperature=temperature
    )

    # List of queries to execute.
    queries: List[str] = [
        "What is 2 + 2?",
        "Summarize quantum entanglement in simple terms.",
        "What is the longest river in Europe?",
        "Explain synergy in a business context.",
        "Who wrote Pride and Prejudice?",
    ]

    print(f"\n=== JIT Ensemble Comparison ({num_units} units per ensemble) ===")
    print(f"Model: {model_name}")
    print(f"Temperature: {temperature}")
    print(f"Number of queries: {len(queries)}")

    # Execute queries for each ensemble variant.
    print("\nRunning baseline ensemble (sequential execution)...")
    baseline_times, total_baseline_time, baseline_results = run_operator_queries(
        operator_instance=baseline_op,
        queries=queries,
        name="Baseline",
        mode="sequential",
    )

    print("\nRunning parallel ensemble...")
    parallel_times, total_parallel_time, parallel_results = run_operator_queries(
        operator_instance=parallel_op, queries=queries, name="Parallel", mode="parallel"
    )

    print("\nRunning JIT ensemble...")
    jit_times, total_jit_time, jit_results = run_operator_queries(
        operator_instance=jit_op, queries=queries, name="JIT", mode="parallel"
    )

    # Create a simple table for displaying results
    try:
        from prettytable import PrettyTable

        # Use PrettyTable if available
        summary_table = PrettyTable()
        summary_table.field_names = [
            "Query",
            "Baseline (s)",
            "Parallel (s)",
            "JIT (s)",
            "Speedup",
        ]

        for index in range(len(queries)):
            # Calculate speedup percentage of JIT over baseline safely
            if baseline_times[index] > 0:
                speedup = (
                    (baseline_times[index] - jit_times[index]) / baseline_times[index]
                ) * 100
            else:
                speedup = 0.0

            # Truncate query for display
            query_display = (
                queries[index][:30] + "..."
                if len(queries[index]) > 30
                else queries[index]
            )

            summary_table.add_row(
                [
                    query_display,
                    f"{baseline_times[index]:.4f}",
                    f"{parallel_times[index]:.4f}",
                    f"{jit_times[index]:.4f}",
                    f"{speedup:.1f}%",
                ]
            )

        print("\n=== Timing Results ===")
        print(summary_table)

    except ImportError:
        # Fallback to text formatting if PrettyTable isn't available
        print("\n=== Timing Results ===")
        print(
            f"{'Query':<35} {'Baseline (s)':<15} {'Parallel (s)':<15} {'JIT (s)':<15} {'Speedup':<10}"
        )
        print("-" * 90)

        for index in range(len(queries)):
            # Calculate speedup percentage of JIT over baseline safely
            if baseline_times[index] > 0:
                speedup = (
                    (baseline_times[index] - jit_times[index]) / baseline_times[index]
                ) * 100
            else:
                speedup = 0.0

            # Truncate query for display
            query_display = (
                queries[index][:30] + "..."
                if len(queries[index]) > 30
                else queries[index]
            )

            print(
                f"{query_display:<35} {baseline_times[index]:<15.4f} {parallel_times[index]:<15.4f} {jit_times[index]:<15.4f} {speedup:<10.1f}%"
            )

    # Calculate and print summary statistics
    avg_baseline: float = sum(baseline_times) / len(baseline_times)
    avg_parallel: float = sum(parallel_times) / len(parallel_times)
    avg_jit: float = sum(jit_times) / len(jit_times)

    print("\n=== Performance Summary ===")
    print(f"Total Baseline time: {total_baseline_time:.4f}s")
    print(f"Total Parallel time: {total_parallel_time:.4f}s")
    print(f"Total JIT time:      {total_jit_time:.4f}s")

    print("\nAverage per-query time:")
    print(f"  Baseline: {avg_baseline:.4f}s")
    print(f"  Parallel: {avg_parallel:.4f}s")
    print(f"  JIT:      {avg_jit:.4f}s")

    # Calculate overall speedups with safety checks for division by zero
    if avg_baseline > 0:
        parallel_speedup = ((avg_baseline - avg_parallel) / avg_baseline) * 100
        jit_speedup = ((avg_baseline - avg_jit) / avg_baseline) * 100
    else:
        parallel_speedup = 0.0
        jit_speedup = 0.0

    if avg_parallel > 0:
        jit_vs_parallel_speedup = ((avg_parallel - avg_jit) / avg_parallel) * 100
    else:
        jit_vs_parallel_speedup = 0.0

    print("\nSpeedup percentages:")
    print(f"  Parallel vs Baseline: {parallel_speedup:.1f}%")
    print(f"  JIT vs Baseline:      {jit_speedup:.1f}%")
    print(f"  JIT vs Parallel:      {jit_vs_parallel_speedup:.1f}%")

    print("\n=== Key Benefits of JIT ===")
    print("1. Automatic tracing and optimization of execution paths")
    print("2. Cached execution plan for repeated queries")
    print("3. Reduced overhead for complex pipelines")
    print("4. Optimization across operator boundaries")

    print(
        "\nTo use JIT in your code, simply add the @jit decorator to your operator class:"
    )
    print("@jit")
    print("class MyOperator(Operator):")
    print("    def forward(self, *, inputs):")
    print("        # Your implementation here")
    print("        return result")

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\README.md:
<code>

# Ember XCS Examples

This directory contains examples demonstrating the XCS (Accelerated Compound Systems execution engine) capabilities of Ember, which provide performance optimization and advanced execution features.

## Example Guide by Concept

### JIT Compilation Systems

Ember provides three complementary approaches to JIT compilation:

1. **jit** - Trace-based JIT compilation

   - `jit_example.py` - Demonstrates the `@jit` decorator with performance comparisons
   - Shows how tracing and caching work for optimizing operator execution

2. **structural_jit** - Structure-based JIT compilation

   - `enhanced_jit_example.py` - Demonstrates hierarchical operator analysis and optimization

3. **autograph** - Explicit graph building
   - `auto_graph_example.py` - Shows manual graph building with LLM examples
   - `auto_graph_simplified.py` - Simplified version without LLM dependencies
   - `simple_autograph_example.py` - Basic introduction to the autograph context manager

### Transform Integration

- `transforms_integration_example.py` - Demonstrates combining multiple transforms (vmap, pmap, structural_jit) with execution options for optimized processing pipelines
  - Shows proper operator definition and transform application
  - Compares performance of different transform combinations
  - Illustrates best practices for memory-efficient processing

### General Examples

- `test_xcs_implementation.py` - Testing XCS core functionality
- `example_simplified_xcs.py` - Simplified XCS API examples

## Choosing the Right Example

- **New to XCS?** Start with `jit_example.py` for a basic introduction
- **Want to understand JIT vs. structural_jit?** See `enhanced_jit_example.py`
- **Need manual graph control?** Check out `auto_graph_simplified.py`
- **Building complex operator compositions?** The hierarchical analysis in `enhanced_jit_example.py` is most relevant
- **Need to optimize batch & parallel processing?** See `transforms_integration_example.py` for combining transforms

## Running Examples

To run any example, use the following command format:

```bash
# Using uv (recommended)
uv run python src/ember/examples/xcs/example_name.py

# Or if in an activated virtual environment
python src/ember/examples/xcs/example_name.py
```

Replace `example_name.py` with the desired example file.

## XCS Core Concepts

The XCS system provides several key capabilities:

- **Just-In-Time Compilation**: Three approaches for different use cases

  - Trace-based JIT: Analyzes actual execution patterns
  - Structural JIT: Examines operator composition without tracing
  - Autograph: Explicit graph construction and execution

- **Execution Optimization**:

  - Automatic parallelization of independent operations
  - Intelligent scheduling based on dependencies
  - Caching of execution plans for repeated use

- **Function Transformations**:

  - vmap: Vectorization of operations
  - pmap: Parallelization across worker threads
  - mesh_sharded: Distribution across device meshes

- **Execution Control**:
  - execution_options: Fine-grained control over execution behavior
  - Adaptive scheduling based on workload characteristics
  - Resource management for memory and compute

For detailed documentation on these components, see:

- `docs/xcs/JIT_OVERVIEW.md` - Understanding the JIT system
- `docs/xcs/TRANSFORMS.md` - Working with transforms
- `docs/xcs/EXECUTION_OPTIONS.md` - Controlling execution behavior

## Next Steps

After exploring these examples, consider:

- `advanced/` - For complex systems using XCS features
- `integration/` - For examples of integrating XCS with other systems

</code>

src\ember\examples\xcs\simple_autograph_example.py:
<code>
"""Simple Auto Graph Example with Mock Operators.

This example demonstrates the enhanced JIT API with automatic graph building
using simple mock operators that don't require external dependencies.
It uses the new ember.api package structure.

To run:
uv run python src/ember/examples/xcs/simple_autograph_example.py
"""

import logging
import time
from typing import Any, Dict

from ember.core.registry.operator.base.operator_base import Operator
from ember.core.registry.specification.specification import Specification
from ember.xcs import JITMode, execution_options, get_jit_stats, jit

###############################################################################

# Mock Operators

###############################################################################

@jit(mode=JITMode.ENHANCED)
class AddOperator(Operator):
"""Simple operator that adds a value to the input."""

    specification = Specification(input_model=None, structured_output=None)

    def __init__(self, *, value: int = 1) -> None:
        self.value = value

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        # Simulate computation with a small delay to show timing differences
        # In a real-world scenario, this would be substantial computation
        time.sleep(0.01)
        result = inputs.get("value", 0) + self.value
        return {"value": result}

@jit()
class MultiplyOperator(Operator):
"""Simple operator that multiplies the input by a value."""

    specification = Specification(input_model=None, structured_output=None)

    def __init__(self, *, value: int = 2) -> None:
        self.value = value

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        result = inputs.get("value", 0) * self.value
        return {"value": result}

@jit()
class DelayOperator(Operator):
"""Simple operator that introduces a delay."""

    specification = Specification(input_model=None, structured_output=None)

    def __init__(self, *, delay: float = 0.1) -> None:
        self.delay = delay
        # Counter to verify execution count
        self.call_count = 0

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        self.call_count += 1
        time.sleep(self.delay)
        return inputs

###############################################################################

# Pipeline with Auto Graph Building

###############################################################################

@jit(sample_input={"value": 1}, mode=JITMode.ENHANCED)
class CalculationPipeline(Operator):
"""Pipeline that demonstrates automatic graph building.

    This pipeline composes multiple operators but doesn't require
    manual graph construction. The @jit decorator handles this
    automatically, building a graph based on the actual execution trace.

    The ENHANCED mode enables additional optimizations and properly
    reports JIT statistics, making it ideal for performance analysis.
    """

    specification = Specification(input_model=None, structured_output=None)

    def __init__(
        self,
        *,
        add_value: int = 5,
        multiply_value: int = 2,
        num_delay_ops: int = 3,
        delay: float = 0.1,
    ) -> None:
        """Initialize the pipeline with configurable parameters."""
        self.add_op = AddOperator(value=add_value)
        self.multiply_op = MultiplyOperator(value=multiply_value)

        # Create multiple delay operators to demonstrate parallel execution
        self.delay_ops = [DelayOperator(delay=delay) for _ in range(num_delay_ops)]

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the pipeline on the given inputs."""
        # First, add
        added = self.add_op(inputs=inputs)

        # Then, apply delays in "parallel" (in a real scenario, these would be executed concurrently)
        delay_results = []
        for op in self.delay_ops:
            delay_results.append(op(inputs=added))

        # Finally, multiply
        return self.multiply_op(inputs=added)

###############################################################################

# Main Demonstration

###############################################################################
def main() -> None:
"""Run demonstration of automatic graph building."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logging.getLogger("ember.xcs.jit").setLevel(logging.DEBUG) # Enable JIT debug logs

    print("\n=== Automatic Graph Building Example ===\n")

    # Create the pipeline
    pipeline = CalculationPipeline(
        add_value=10, multiply_value=3, num_delay_ops=5, delay=0.1
    )

    # Example inputs to demonstrate caching and reuse
    inputs = [{"value": 5}, {"value": 10}, {"value": 15}]

    print("First run - expect graph building overhead:")
    first_run_times = []

    for i, inp in enumerate(inputs):
        print(f"\nInput {i+1}: {inp}")

        start_time = time.perf_counter()
        result = pipeline(inputs=inp)
        elapsed = time.perf_counter() - start_time
        first_run_times.append(elapsed)

        # Show calculation steps
        input_value = inp["value"]
        expected_value = (input_value + 10) * 3  # add_value=10, multiply_value=3

        print(f"Result: {result}")
        print(f"Value: {result['value']}")
        print(
            f"Expected calculation: {input_value} + 10 = {input_value + 10}, then × 3 = {expected_value}"
        )
        print(f"Time: {elapsed:.4f}s")

    # Cached execution demonstration
    print("\nRepeat first input to demonstrate cached execution:")
    start_time = time.perf_counter()
    result = pipeline(inputs=inputs[0])
    cached_time = time.perf_counter() - start_time

    print(f"Result: {result}")
    print(f"Value: {result['value']}")
    print(f"Time: {cached_time:.4f}s")

    # Calculate speedup
    speedup = (first_run_times[0] - cached_time) / first_run_times[0]
    print(f"Speedup from caching: {speedup:.1%} faster")

    # Sequential execution demonstration
    print("\nUsing execution_options to control execution:")
    with execution_options(scheduler="sequential"):
        start_time = time.perf_counter()
        result = pipeline(inputs={"value": 20})
        sequential_time = time.perf_counter() - start_time

        print(f"Result: {result}")
        print(f"Value: {result['value']}")
        print("Expected calculation: 20 + 10 = 30, then × 3 = 90")
        print(f"Time: {sequential_time:.4f}s (sequential execution)")

    # Get JIT statistics from various components
    pipeline_stats = get_jit_stats(pipeline)
    add_op_stats = get_jit_stats(pipeline.add_op)
    mult_op_stats = get_jit_stats(pipeline.multiply_op)

    # Combine stats from components for a more complete picture
    combined_hits = (
        pipeline_stats.get('cache_hits', 0) +
        add_op_stats.get('cache_hits', 0) +
        mult_op_stats.get('cache_hits', 0)
    )
    combined_misses = (
        pipeline_stats.get('cache_misses', 0) +
        add_op_stats.get('cache_misses', 0) +
        mult_op_stats.get('cache_misses', 0)
    )

    # Get strategy from any available component
    strategy = pipeline_stats.get('strategy', 'unknown')
    if strategy == 'unknown' and hasattr(pipeline, '_jit_strategy'):
        strategy = pipeline._jit_strategy

    print("\nJIT Statistics:")
    print(f"Cache hits: {combined_hits}")
    print(f"Cache misses: {combined_misses}")
    print(f"Strategy: {strategy}")

    # Add a summary
    print("\n=== Summary ===")
    print(f"First run time: {first_run_times[0]:.4f}s (includes tracing overhead)")
    print(f"Cached run time: {cached_time:.4f}s")
    print(f"Sequential execution time: {sequential_time:.4f}s")
    print("\nKey benefits of autograph with JIT:")
    print("1. Automatic operator dependency discovery")
    print("2. Optimized execution with caching")
    print("3. Flexible execution strategies (parallel, sequential)")
    print("4. No manual graph construction required")

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\transforms_integration_example.py:
<code>
"""Transforms Integration Example.

This example demonstrates how to effectively combine Ember XCS transforms
(vmap, pmap, structural_jit) with execution options to create optimized
processing pipelines. It shows different integration patterns and
provides performance comparisons.

The example illustrates:

1. Proper operator definition following Ember conventions
2. Applying transforms in effective combinations
3. Controlling execution with execution_options
4. Performance patterns and best practices

For a comprehensive explanation of the transforms and execution options, see:

- docs/xcs/TRANSFORMS.md
- docs/xcs/EXECUTION_OPTIONS.md
- docs/xcs/JIT_OVERVIEW.md

To run:
uv run python src/ember/examples/xcs/transforms_integration_example.py
"""

import logging
import random
import time
from typing import Any, ClassVar, Dict

from ember.core.registry.operator.base.operator_base import Operator

# Import Ember APIs

from ember.core.registry.specification.specification import Specification
from ember.xcs.engine import execution_options
from ember.xcs.jit import jit
from ember.xcs.transforms import pmap, vmap

# Use the structural JIT strategy directly

structural_jit = jit.structural

###############################################################################

# Basic Component Operators

###############################################################################

class TextProcessor(Operator[Dict[str, Any], Dict[str, Any]]):
"""Processes text with configurable operations.

    This operator demonstrates proper field declarations and type annotations.
    """

    # Class-level specification
    specification: ClassVar[Specification] = Specification()

    # Field declarations
    process_type: str
    max_length: int

    def __init__(
        self, *, process_type: str = "tokenize", max_length: int = 100
    ) -> None:
        """Initialize the text processor.

        Args:
            process_type: The type of processing to perform
            max_length: Maximum length of text to process
        """
        self.process_type = process_type
        self.max_length = max_length

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Process text according to configured options.

        This operator works on a single text item, not a batch. When used with
        transforms like vmap or pmap, those transforms are responsible for
        properly batching or distributing the inputs and combining the outputs.

        Args:
            inputs: Dictionary containing the text to process

        Returns:
            Dictionary with processed text and metadata
        """
        # Get input text with validation - expect a string, not a collection
        text = inputs.get("text", "")

        # Validate input type - provide clear errors for misuse
        if not isinstance(text, str):
            # This should never happen with properly configured transforms
            # But we provide a clear error message if it does
            raise TypeError(
                f"TextProcessor expects text input to be a string, got {type(text).__name__}. "
                "This may indicate incorrect transform configuration."
            )

        # Simulate different processing types with varying costs
        if self.process_type == "tokenize":
            # Fast operation
            time.sleep(0.01)
            tokens = text[: self.max_length].split() if text else []
            processed = f"Tokens: {tokens}"
        elif self.process_type == "analyze":
            # Medium cost operation
            time.sleep(0.05)
            processed = f"Analysis: {text[:self.max_length]}"
        elif self.process_type == "summarize":
            # Expensive operation
            time.sleep(0.1)
            summary_length = min(20, len(text)) if text else 0
            processed = f"Summary: {text[:summary_length]}..."
        else:
            processed = f"Unknown operation: {text[:self.max_length]}"

        return {
            "processed": processed,
            "original_length": len(text),
            "process_type": self.process_type,
        }

class FeatureExtractor(Operator[Dict[str, Any], Dict[str, Any]]):
"""Extracts features from processed text.

    This operator works as a secondary stage in a processing pipeline.
    """

    # Class-level specification
    specification: ClassVar[Specification] = Specification()

    # Field declarations
    feature_type: str
    num_features: int

    def __init__(self, *, feature_type: str = "basic", num_features: int = 5) -> None:
        """Initialize the feature extractor.

        Args:
            feature_type: Type of features to extract
            num_features: Maximum number of features to extract
        """
        self.feature_type = feature_type
        self.num_features = num_features

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Extract features from processed text.

        Args:
            inputs: Dictionary containing processed text

        Returns:
            Dictionary with extracted features
        """
        # Get processed text from previous stage
        processed = inputs.get("processed", "")
        process_type = inputs.get("process_type", "unknown")

        # Simulate feature extraction based on feature_type
        time.sleep(0.03)  # Base cost

        # Generate simulated features
        features = []
        for i in range(min(self.num_features, 10)):
            # Using hash of combination to ensure deterministic but varied output
            feature_hash = hash(f"{processed}_{i}_{self.feature_type}")
            random.seed(feature_hash)
            feature_val = random.random()
            features.append((f"feature_{i}", feature_val))

        return {
            "features": features,
            "feature_type": self.feature_type,
            "source_type": process_type,
            "feature_count": len(features),
        }

###############################################################################

# Pipeline Definition (for structural_jit example)

###############################################################################

@structural_jit
class TextAnalysisPipeline(Operator[Dict[str, Any], Dict[str, Any]]):
"""Two-stage text analysis pipeline with JIT optimization.

    This composite operator demonstrates proper structure for @structural_jit,
    with properly declared fields for sub-operators.
    """

    # Class-level specification
    specification: ClassVar[Specification] = Specification()

    # Field declarations for sub-operators
    processor: TextProcessor
    extractor: FeatureExtractor

    def __init__(
        self, *, process_type: str = "tokenize", feature_type: str = "basic"
    ) -> None:
        """Initialize the pipeline with configured sub-operators.

        Args:
            process_type: Type of text processing to perform
            feature_type: Type of features to extract
        """
        # Create sub-operators
        self.processor = TextProcessor(process_type=process_type, max_length=150)
        self.extractor = FeatureExtractor(feature_type=feature_type, num_features=8)

    def forward(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the full pipeline on inputs.

        The @structural_jit decorator will optimize this execution flow
        by analyzing the operator structure and dependencies.

        Args:
            inputs: Dictionary containing the text to process

        Returns:
            Dictionary with processed text and extracted features
        """
        # Two-stage pipeline that will be optimized by structural_jit
        processed = self.processor(inputs=inputs)
        features = self.extractor(inputs=processed)

        # Combine results (optional, just to show final combination)
        result = {
            "text_length": processed.get("original_length", 0),
            "process_type": processed.get("process_type", "unknown"),
            "features": features.get("features", []),
            "feature_type": features.get("feature_type", "unknown"),
            "feature_count": features.get("feature_count", 0),
        }

        return result

###############################################################################

# Transform Integration Examples

###############################################################################

def demonstrate_basic_transforms():
"""Demonstrate basic transforms applied individually."""
print("\n=== BASIC TRANSFORM EXAMPLES ===")

    # Create a simple text processor
    processor = TextProcessor(process_type="tokenize", max_length=100)

    # Sample input
    sample_text = "This is a sample text for demonstrating transforms in Ember XCS."

    # 1. No transforms (baseline)
    start_time = time.time()
    result = processor(inputs={"text": sample_text})
    baseline_time = time.time() - start_time
    print(f"Baseline (no transforms): {baseline_time:.4f}s")
    print(f"Result: {result['processed']}")

    # 2. With vmap (batch processing)
    print("\n--- Vectorized Mapping (vmap) Example ---")
    vectorized_processor = vmap(processor)
    batch_texts = [
        "First sample text for batch processing.",
        "Second sample with different content.",
        "Third sample to demonstrate batching capabilities.",
    ]

    # Explain vmap input/output structure
    print("vmap transforms a single-item operator to process multiple inputs at once.")
    print(f"Input: {len(batch_texts)} text items to be processed in parallel")

    start_time = time.time()
    try:
        batch_results = vectorized_processor(inputs={"text": batch_texts})
        vmap_time = time.time() - start_time
        print(f"Processing time: {vmap_time:.4f}s for {len(batch_texts)} items")

        # Inspect the output structure
        print(f"Output keys: {list(batch_results.keys())}")

        # Show processed results from each item
        for key, value in batch_results.items():
            if isinstance(value, list) and value:
                print(f"First '{key}' result: {value[0]}")
                print(f"Number of '{key}' results: {len(value)}")
    except Exception as e:
        print(f"Error in vmap processing: {e}")

    # 3. With pmap (parallel processing)
    print("\n--- Parallel Mapping (pmap) Example ---")
    print("pmap distributes processing across multiple workers.")
    print("Each worker processes one complete input independently.")

    # For pmap, we need a wrapper that can handle single texts properly
    class TextProcessorWrapper:
        def __init__(self, processor):
            self.processor = processor

        def __call__(self, *, inputs):
            """Process a batch of inputs serially, but with a parallel-compatible interface.

            When used with pmap, this wrapper ensures each worker gets a properly
            formatted single input.
            """
            # Check if we received the entire batch or a single item
            if "batch_index" in inputs and "text" in inputs:
                # We're in pmap context, each worker will get a single text at specific index
                batch_index = inputs["batch_index"]
                texts = inputs["text"]

                # Handle case where batch_index is a list (pmap dispatch)
                if isinstance(batch_index, list):
                    if batch_index:  # Non-empty list
                        batch_index = batch_index[0]  # Take first index
                    else:
                        return {"processed": "Empty batch index list"}

                # Now we have a scalar index
                if isinstance(batch_index, int) and 0 <= batch_index < len(texts):
                    # Process the text at our assigned index
                    return self.processor(inputs={"text": texts[batch_index]})
                else:
                    return {"processed": f"Error: Invalid batch index {batch_index}"}
            else:
                # Direct processing (not in pmap context)
                return self.processor(inputs=inputs)

    # Create wrapper and distribute processing
    wrapper = TextProcessorWrapper(processor)
    parallelized_wrapper = pmap(wrapper, num_workers=3)

    # Prepare parallel inputs with batch indices
    texts = ["Text for worker 1", "Text for worker 2", "Text for worker 3"]

    # Create inputs for each worker
    parallel_inputs = {
        "text": texts,
        "batch_index": list(range(len(texts))),  # Each worker gets their index
    }
    print(f"Input: {len(texts)} texts to be processed by different workers")

    start_time = time.time()
    try:
        parallel_results = parallelized_wrapper(inputs=parallel_inputs)
        pmap_time = time.time() - start_time
        print(f"Processing time: {pmap_time:.4f}s with {min(3, len(texts))} workers")

        # Inspect the output structure
        print(f"Output keys: {list(parallel_results.keys())}")

        # Show results from each worker
        for key, value in parallel_results.items():
            if isinstance(value, list) and value:
                print(f"First '{key}' result: {value[0]}")
                print(f"Number of '{key}' results: {len(value)}")
    except Exception as e:
        print(f"Error in pmap processing: {e}")

def compare_transform_combinations():
"""Compare different combinations of transforms for performance."""
print("\n=== TRANSFORM COMBINATION COMPARISON ===")

    # Create base operators
    base_processor = TextProcessor(process_type="analyze", max_length=100)

    # Create a wrapper for pmap operations
    class TextProcessorWrapper:
        def __init__(self, processor):
            self.processor = processor

        def __call__(self, *, inputs):
            """Process a batch of inputs with a parallel-compatible interface."""
            try:
                # In pmap, inputs might contain both batch_index and text
                if "batch_index" in inputs and "text" in inputs:
                    batch_index = inputs["batch_index"]
                    texts = inputs["text"]

                    # Handle both scalar and list batch_index (pmap may distribute differently)
                    if isinstance(batch_index, list):
                        if batch_index:  # Non-empty list
                            batch_index = batch_index[0]  # Take first index
                        else:
                            return {"processed": "Empty batch index list"}

                    # Now we have a scalar index
                    if isinstance(batch_index, int) and 0 <= batch_index < len(texts):
                        selected_text = texts[batch_index]
                        # Handle item selection for strings or lists
                        if isinstance(selected_text, str):
                            return self.processor(inputs={"text": selected_text})
                        else:
                            # We might have gotten a nested structure
                            return {
                                "processed": f"Complex selected text: {type(selected_text)}"
                            }
                    else:
                        return {"processed": f"Invalid index {batch_index}"}

                # Direct text handling
                elif "text" in inputs:
                    if isinstance(inputs["text"], str):
                        # Process a single string directly
                        return self.processor(inputs=inputs)
                    elif isinstance(inputs["text"], list) and len(inputs["text"]) > 0:
                        # Process the first item from a list
                        return self.processor(inputs={"text": inputs["text"][0]})
                    else:
                        return {"processed": "Invalid text format"}
                else:
                    # No recognizable input format
                    return {"processed": "Missing text input"}

            except Exception as e:
                # Provide detailed error info
                return {"processed": f"Error: {str(e)}", "error": str(e)}

    # Create transformed variants
    vectorized = vmap(base_processor)
    wrapper = TextProcessorWrapper(base_processor)
    parallelized = pmap(wrapper, num_workers=4)
    # For combined transforms, we need to ensure interface compatibility
    vectorized_wrapper = TextProcessorWrapper(vectorized)
    vectorized_then_parallelized = pmap(vectorized_wrapper, num_workers=4)

    # Generate test dataset (100 items)
    texts = [
        f"Sample text number {i} for transformation comparison." for i in range(100)
    ]

    # Run with different configurations and measure performance
    results = []

    def time_execution(transform_name, operation_fn, data):
        """Time the execution of an operation and return results."""
        print(f"\nTesting: {transform_name}")

        try:
            start_time = time.time()
            result = operation_fn(inputs=data)
            elapsed = time.time() - start_time

            # Find the actual count of processed items by looking for list results
            count = 0
            for key, value in result.items():
                if isinstance(value, list):
                    count = max(count, len(value))

            # Default to 1 if no lists found
            if count == 0:
                count = 1

            throughput = count / max(elapsed, 0.0001)  # Avoid division by zero
            print(
                f"  Time: {elapsed:.4f}s for {count} items ({throughput:.2f} items/sec)"
            )
            return (transform_name, elapsed, count)
        except Exception as e:
            print(f"  Error: {e}")
            return (transform_name, float("inf"), 0)

    # 1. No transforms, sequential processing (properly matched interface)
    class SequentialProcessor:
        """Sequential processor that matches transform interface."""

        def __init__(self, processor):
            self.processor = processor

        def __call__(self, *, inputs):
            """Process inputs sequentially, matching transform interface.

            Args:
                inputs: Dict with text items, or list of texts

            Returns:
                Dict with processed results combined
            """
            if not isinstance(inputs, dict):
                # Handle direct list inputs
                texts = inputs if isinstance(inputs, list) else [inputs]
                inputs = {"text": texts}

            texts = inputs.get("text", [])
            if not isinstance(texts, list):
                texts = [texts]

            # Process each text individually
            results = {"processed": []}
            for text in texts:
                try:
                    result = self.processor(inputs={"text": text})
                    results["processed"].append(result.get("processed", ""))
                except Exception as e:
                    print(f"  Error processing item: {e}")
                    results["processed"].append(f"Error: {str(e)}")

            # Add other metadata
            results["process_type"] = self.processor.process_type
            results["original_length"] = [len(t) for t in texts]

            return results

    # Create sequential processor matching transform interface
    sequential_processor = SequentialProcessor(base_processor)

    # Benchmark sequential processing
    results.append(
        time_execution(
            "Sequential (no transforms)", sequential_processor, {"text": texts}
        )
    )

    # 2. vmap only (batch process all texts at once)
    print("\nExplaining vmap: Batch processing all inputs simultaneously")
    results.append(time_execution("vmap only", vectorized, {"text": texts}))

    # 3. pmap only (distribute texts across workers)
    print("\nExplaining pmap: Distributing inputs across parallel workers")
    # For pmap, provide indices for the workers
    pmap_inputs = {"text": texts, "batch_index": list(range(len(texts)))}
    results.append(time_execution("pmap only", parallelized, pmap_inputs))

    # 4. vmap + pmap (first batch, then distribute batches)
    print(
        "\nExplaining pmap(vmap()): Batching inputs, then distributing batches across workers"
    )
    # For combined transforms, we ensure interface compatibility
    # The wrapper expects batch_index
    pmap_vmap_inputs = {
        "text": texts,
        "batch_index": [0],  # We're sending the whole batch to one worker
    }
    results.append(
        time_execution("pmap(vmap())", vectorized_then_parallelized, pmap_vmap_inputs)
    )

    # 5. With execution options (controlling parallelism)
    print("\nDemonstrating execution_options: Controlling execution parameters")
    with execution_options(max_workers=2):
        results.append(
            time_execution(
                "pmap(vmap()) with max_workers=2",
                vectorized_then_parallelized,
                pmap_vmap_inputs,
            )
        )

    # Filter out failed executions
    valid_results = [
        (name, time_taken, count)
        for name, time_taken, count in results
        if time_taken != float("inf")
    ]

    if valid_results:
        # Find fastest execution
        fastest_result = min(valid_results, key=lambda x: x[1])
        fastest_time = fastest_result[1]

        # Display summary
        print("\nPerformance comparison (from slowest to fastest):")
        sorted_results = sorted(valid_results, key=lambda x: x[1], reverse=True)

        for name, time_taken, count in sorted_results:
            if time_taken > 0 and fastest_time > 0:
                speedup = time_taken / fastest_time
                print(
                    f"  {name}: {time_taken:.4f}s for {count} items ({speedup:.2f}x vs fastest)"
                )
            else:
                print(f"  {name}: {time_taken:.4f}s for {count} items (N/A)")
    else:
        print("\nNo valid performance results to compare.")

def demonstrate_structural_jit_with_vmap():
"""Demonstrate combining structural_jit with vmap."""
print("\n=== STRUCTURAL JIT + VMAP EXAMPLE ===")

    # Create the optimized pipeline
    pipeline = TextAnalysisPipeline(process_type="analyze", feature_type="advanced")

    # Apply vmap to handle batches
    vectorized_pipeline = vmap(pipeline)

    # Generate a batch of inputs
    batch_texts = [
        "This is the first sample text for the JIT + vmap example.",
        "Here is a second sample with completely different content.",
        "A third sample text to ensure we have a good batch size.",
    ]

    # First run: Will include JIT compilation overhead
    print("First run (includes JIT compilation)...")
    start_time = time.time()
    results = vectorized_pipeline(inputs={"text": batch_texts})
    first_run_time = time.time() - start_time
    print(f"First run: {first_run_time:.4f}s for {len(batch_texts)} items")

    # Second run: Should be faster due to cached compilation
    print("\nSecond run (uses cached compilation)...")
    start_time = time.time()
    results = vectorized_pipeline(inputs={"text": batch_texts})
    second_run_time = time.time() - start_time
    print(f"Second run: {second_run_time:.4f}s for {len(batch_texts)} items")

    # Show improvement
    if first_run_time > 0:
        improvement = (first_run_time - second_run_time) / first_run_time * 100
        print(f"Improvement: {improvement:.1f}% faster with cached compilation")

    # Show feature results
    print("\nExtracted features for first item:")
    if results.get("features") and len(results["features"]) > 0:
        for feature_name, feature_value in results["features"][0][:3]:
            print(f"  {feature_name}: {feature_value:.4f}")

        if len(results["features"][0]) > 3:
            print(f"  ... and {len(results['features'][0]) - 3} more features")

def demonstrate_execution_options():
"""Demonstrate controlling transform behavior with execution options."""
print("\n=== EXECUTION OPTIONS EXAMPLE ===")

    # Create a pipeline with both vmap and pmap
    base_pipeline = TextAnalysisPipeline(
        process_type="summarize",  # More expensive operation
        feature_type="comprehensive",
    )

    # First vectorize, then parallelize
    # This distributes batches across workers
    vectorized = vmap(base_pipeline)
    distributed = pmap(vectorized, num_workers=4)

    # Generate larger dataset (50 items)
    texts = [
        f"Sample document {i} with content for execution options testing."
        for i in range(50)
    ]
    data = {"text": texts}

    # Different execution options configurations
    configurations = [
        ("Default settings", {}),
        ("Sequential execution", {"use_parallel": False}),
        ("Limited workers", {"max_workers": 2}),
        ("With caching", {"enable_caching": True}),
        ("Sequential + caching", {"use_parallel": False, "enable_caching": True}),
        (
            "Full parallel + caching",
            {"use_parallel": True, "max_workers": 8, "enable_caching": True},
        ),
    ]

    # Test each configuration
    results = []

    for name, options in configurations:
        print(f"\nTesting: {name}")

        # Apply the execution options
        with execution_options(**options):
            start_time = time.time()
            result = distributed(inputs=data)
            elapsed = time.time() - start_time

            count = len(result.get("text_length", []))
            print(
                f"  Processed {count} items in {elapsed:.4f}s ({count/elapsed:.2f} items/sec)"
            )

            results.append((name, elapsed, count))

    # Show comparative summary
    print("\nExecution options performance comparison:")
    sorted_results = sorted(results, key=lambda x: x[1])

    for name, time_taken, count in sorted_results:
        speedup = results[0][1] / time_taken if time_taken > 0 else float("inf")
        print(f"  {name}: {time_taken:.4f}s ({speedup:.2f}x vs default)")

def demonstrate_chunked_processing():
"""Demonstrate memory-efficient chunked processing for large datasets."""
print("\n=== CHUNKED PROCESSING EXAMPLE ===")

    # Create a simpler minimal processor specifically for this example
    # This avoids the complexity of the JIT integration
    class SimpleTextProcessor:
        """Simple text processor for chunked processing demo."""

        def __init__(self):
            """Initialize the processor."""
            pass

        def __call__(self, *, inputs):
            """Process a text input to demonstrate chunking."""
            text = inputs.get("text", "")
            if isinstance(text, str):
                # Process one text item
                return {"processed": f"Processed: {text[:20]}...", "length": len(text)}
            else:
                # Return empty for non-string inputs
                return {"processed": "Invalid input", "length": 0}

    # Create the processor and vectorized version
    processor = SimpleTextProcessor()

    # Create a simple vectorized version
    def batch_process(*, inputs):
        """Simple batch processor that mimics vmap behavior."""
        texts = inputs.get("text", [])
        if not isinstance(texts, list):
            texts = [texts]

        results = []
        for text in texts:
            results.append(processor(inputs={"text": text}))

        # Combine results by key (similar to vmap)
        combined = {}
        for key in ["processed", "length"]:
            combined[key] = [r.get(key, "") for r in results]

        return combined

    # Generate "large" dataset that we'll process in chunks
    large_dataset = [
        f"Document {i} with content for chunked processing example." for i in range(200)
    ]

    print(
        "This example demonstrates processing a large dataset in chunks to manage memory."
    )
    print(
        "For large language model processing, this pattern helps avoid GPU out-of-memory errors."
    )

    def process_in_chunks(texts, chunk_size=20):
        """Process a dataset in chunks to control memory usage.

        This demonstrates how to efficiently process large datasets by breaking
        them into manageable chunks, avoiding memory issues.

        Args:
            texts: List of texts to process
            chunk_size: Number of items to process in each chunk

        Returns:
            Combined results from all chunks
        """
        # Initialize results container
        all_results = {"processed": [], "length": []}

        # Get total size
        total_items = len(texts)
        chunk_count = (total_items + chunk_size - 1) // chunk_size

        print(
            f"Processing {total_items} items in {chunk_count} chunks of size {chunk_size}..."
        )

        # Process in chunks
        total_processed = 0
        for i in range(0, total_items, chunk_size):
            # Create chunk
            end_idx = min(i + chunk_size, total_items)
            chunk = texts[i:end_idx]
            current_chunk_size = len(chunk)

            # Process chunk
            chunk_start = time.time()
            try:
                chunk_results = batch_process(inputs={"text": chunk})
                chunk_elapsed = time.time() - chunk_start

                # Calculate throughput (with safety for very fast execution)
                throughput = current_chunk_size / max(chunk_elapsed, 0.0001)

                print(
                    f"  Chunk {i//chunk_size + 1}/{chunk_count}: "
                    f"{current_chunk_size} items in {chunk_elapsed:.4f}s "
                    f"({throughput:.2f} items/sec)"
                )

                # Append results
                for key in all_results:
                    if key in chunk_results and isinstance(chunk_results[key], list):
                        all_results[key].extend(chunk_results[key])
                        total_processed += len(chunk_results[key])

            except Exception as e:
                print(f"  Error processing chunk {i//chunk_size + 1}: {e}")

        print(f"Successfully processed {total_processed} items in chunks")
        return all_results

    # Process with chunking and measure total time
    start_time = time.time()
    results = process_in_chunks(large_dataset, chunk_size=20)
    total_elapsed = max(time.time() - start_time, 0.0001)  # Avoid div by zero

    print(
        f"\nTotal processing time: {total_elapsed:.4f}s for {len(large_dataset)} items"
    )
    print(f"Average processing rate: {len(large_dataset)/total_elapsed:.2f} items/sec")
    print(f"Results count: {len(results['processed'])} items processed")

    # Show sample results
    if results["processed"]:
        print(f"\nSample result: {results['processed'][0]}")
        print(f"Total bytes processed: {sum(results['length'])}")
    else:
        print("\nNo processed results found")

    # Compare with non-chunked (if small enough to run all at once)
    if len(large_dataset) <= 50:  # Only do this comparison for smaller datasets
        print("\nComparing to non-chunked processing:")
        start_time = time.time()
        full_results = batch_process(
            inputs={"text": large_dataset[:10]}
        )  # Just use 10 items for the test
        full_elapsed = max(time.time() - start_time, 0.0001)  # Avoid div by zero
        print(
            f"Non-chunked processing: {full_elapsed:.4f}s ({10/full_elapsed:.2f} items/sec)"
        )

###############################################################################

# Main Demonstration

###############################################################################

def main():
"""Run the transforms integration demonstrations."""
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    print("Ember XCS Transforms Integration Example")
    print("=======================================")
    print("This example demonstrates how to combine vmap, pmap, and structural_jit")
    print("transforms with execution options for optimized processing pipelines.")

    # Run the demonstrations
    demonstrate_basic_transforms()
    compare_transform_combinations()
    demonstrate_structural_jit_with_vmap()
    demonstrate_execution_options()
    demonstrate_chunked_processing()

    print("\n=== KEY TAKEAWAYS ===")
    print("1. vmap is best for batch processing of homogeneous data")
    print("2. pmap works well for distributing work across CPU cores")
    print("3. structural_jit optimizes complex operator composition")
    print("4. pmap(vmap()) is typically more efficient than vmap(pmap())")
    print("5. execution_options provide fine-grained control over execution behavior")
    print("6. Chunked processing helps manage memory for large datasets")
    print("7. JIT compilation has upfront cost but amortizes over repeated calls")

    print("\nFor more information, see:")
    print("- docs/xcs/TRANSFORMS.md")
    print("- docs/xcs/EXECUTION_OPTIONS.md")
    print("- docs/xcs/JIT_OVERVIEW.md")

if **name** == "**main**":
main()

</code>

src\ember\examples\xcs\_\_init\_\_.py:
<code>

</code>

src\ember\examples\README.md:
<code>

# Ember Examples

This directory contains example code demonstrating the Ember framework capabilities, from simple examples to advanced use cases.

## Getting Started

If you're new to Ember, start with the basic examples that demonstrate core concepts:

```bash
# Run a minimal example
uv run python src/ember/examples/basic/minimal_example.py

# Try a simple model example
uv run python src/ember/examples/models/model_registry_example.py

# Or using the uvx shorthand for running tools
uvx pytest tests/unit/  # Run unit tests
```

## Directory Structure

- **[basic/](./basic/)** - Simple examples for beginners to get started
- **[models/](./models/)** - Examples for working with LLM models and the model registry
- **[operators/](./operators/)** - Examples showing how to create and compose operators
- **[data/](./data/)** - Examples for data loading, transformation, and dataset usage
- **[xcs/](./xcs/)** - Examples for using the XCS (Accelerated Compound Systems) features
- **[advanced/](./advanced/)** - Advanced examples with complex workflows and optimizations
- **[integration/](./integration/)** - Examples showing integration with other systems

## Example Difficulty Levels

Examples are categorized by difficulty level in their docstrings:

- **Basic**: Fundamental concepts and minimal working examples
- **Intermediate**: More complex but common use cases
- **Advanced**: Complex workflows and specialized features

## Popular Examples

- [Minimal Example](./basic/minimal_example.py) - The simplest possible Ember operator
- [Model Registry Example](./models/model_registry_example.py) - Working with LLM models
- [JIT Example](./xcs/jit_example.py) - Using the JIT system for performance optimization
- [Ensemble Operator Example](./operators/ensemble_operator_example.py) - Creating multi-model ensembles
- [Data API Example](./data/data_api_example.py) - Working with datasets

## Complete Application Examples

For complete application examples that show how to build real-world AI systems with Ember, see:

- [Advanced Reasoning System](./advanced/reasoning_system.py)
- [Parallel Pipeline Example](./advanced/parallel_pipeline_example.py)
- [Auto Graph Simplified](./xcs/auto_graph_simplified.py)

## Documentation

For more information, see the [project README](../../../../README.md) and documentation files in the [docs/](../../../../docs/) directory.

</code>

src\ember\examples\_\_init\_\_.py:
<code>

</code>

src\ember\xcs\api\core.py:
<code>
"""
Core implementation of the XCS API.

Entry point for simplified access to XCS functionality, providing an abstraction
over the underlying execution engine, tracing, and transformation capabilities.
"""

from **future** import annotations

# Standard library imports

import time
from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, Union, cast

# Local imports

from ember.core.exceptions import RegistryError
from ember.xcs.api.types import (
ExecutionResult,
JITOptions,
TransformOptions,
XCSExecutionOptions,
)
from ember.xcs.engine.unified_engine import ExecutionOptions, execute_graph
from ember.xcs.exceptions import (
CompilationError,
DataFlowError,
ExecutionError,
TraceError,
TransformError,
XCSError,
)
from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.jit import jit as raw_jit
from ember.xcs.tracer.autograph import AutoGraphBuilder
from ember.xcs.tracer.xcs_tracing import TraceRecord
from ember.xcs.transforms.mesh import DeviceMesh, PartitionSpec
from ember.xcs.transforms.mesh import mesh_sharded as raw_mesh_sharded
from ember.xcs.transforms.pmap import pmap as raw_pmap
from ember.xcs.transforms.vmap import vmap as raw_vmap

# Type variables

T = TypeVar("T")
ResultT = TypeVar("ResultT")

class XCSAPI:
"""
Main API class for XCS functionality.

    Providing a unified interface to the XCS (eXecutable Computation System)
    functionality, abstracting away the details of the implementation.

    This class provides a simplified interface to the system with a
    consistent interface across different components.

    Example:
        ```python
        from ember.xcs.api.core import XCSAPI

        # Creating API instance
        xcs = XCSAPI()

        # Using JIT compilation
        @xcs.jit
        class MyOperator(Operator):
            def forward(self, *, inputs):
                return {"result": process(inputs)}

        # Using vectorization
        batch_fn = xcs.vmap(single_item_fn)

        # Using parallelization
        parallel_fn = xcs.pmap(compute_intensive_fn)

        # Building and executing a graph
        graph = xcs.autograph(trace_records)
        result = xcs.execute(graph, inputs={"query": "test"})
        ```
    """

    def __init__(self) -> None:
        """Initialize the XCS API."""
        self._graph_builder = AutoGraphBuilder()

    def jit(
        self,
        target: Optional[Type[T]] = None,
        *,
        options: Optional[JITOptions] = None,
        **kwargs: Any,
    ) -> Union[Type[T], Callable[[Type[T]], Type[T]]]:
        """
        Just-In-Time compilation decorator for Ember Operators.

        Transforming Operator classes to automatically trace their execution
        and compile optimized execution plans. This brings significant performance benefits
        for complex operations and operator pipelines.

        Args:
            target: The operator class to decorate (when used directly)
            options: Configuration options for JIT compilation
            **kwargs: Additional options passed directly to the underlying implementation

        Returns:
            The decorated operator class or a decorator function

        Example:
            ```python
            from ember.xcs import xcs

            # Using as a direct decorator
            @xcs.jit
            class MyOperator(Operator):
                def forward(self, *, inputs):
                    # Complex logic here
                    return {"result": process(inputs["query"])}

            # Creating an instance and executing
            op = MyOperator()
            result = op(inputs={"query": "example"})

            # Using with advanced configuration options
            @xcs.jit(options=JITOptions(
                sample_input={"query": "Example"},
                force_trace=False,
                recursive=True
            ))
            class OptimizedOperator(Operator):
                def __init__(self):
                    self.sub_op1 = SubOperator1()
                    self.sub_op2 = SubOperator2()

                def forward(self, *, inputs):
                    # Multi-stage processing with optimized execution
                    intermediate = self.sub_op1(inputs=inputs)
                    return self.sub_op2(inputs=intermediate)
            ```
        """
        # Handle options
        opts = options or JITOptions()

        # Prepare kwargs for raw_jit
        jit_kwargs = {
            "sample_input": opts.sample_input,
            "force_trace": opts.force_trace,
            "recursive": opts.recursive,
            **kwargs,
        }

        # When used directly as @xcs.jit
        if target is not None:
            return raw_jit(**jit_kwargs)(target)

        # When used as @xcs.jit() or @xcs.jit(options=...)
        return lambda cls: raw_jit(**jit_kwargs)(cls)

    def autograph(self, records: List[TraceRecord]) -> XCSGraph:
        """Build execution graph from trace records.

        Analyzes trace records to create a dependency graph for execution.

        Args:
            records: Trace records from execution

        Returns:
            XCS execution graph

        Raises:
            TraceError: When trace records are invalid or cannot be analyzed
            DataFlowError: When data flow issues are detected in the graph

        Example:
            ```python
            with TracerContext() as tracer:
                op(inputs={"query": "Example"})
            graph = xcs.autograph(tracer.records)
            ```
        """
        if not records:
            raise TraceError(
                message="Cannot build graph from empty trace records",
                context={"record_count": 0}
            )

        try:
            return self._graph_builder.build_graph(records=records)
        except Exception as e:
            # Convert generic exceptions to proper XCS exceptions
            if "cycle" in str(e).lower() or "circular" in str(e).lower():
                raise DataFlowError(
                    message="Circular dependency detected in execution graph",
                    context={"record_count": len(records)},
                    cause=e
                )
            raise TraceError(
                message=f"Failed to build execution graph: {e}",
                context={"record_count": len(records)},
                cause=e
            )

    def execute(
        self,
        graph: XCSGraph,
        inputs: Dict[str, Any],
        options: Optional[XCSExecutionOptions] = None,
    ) -> ExecutionResult:
        """Execute XCS graph with given inputs.

        Args:
            graph: XCS graph to execute
            inputs: Input values for the graph
            options: Execution options for parallelism and timeout

        Returns:
            Execution result with outputs and timing

        Raises:
            ExecutionError: When graph execution fails

        Example:
            ```python
            exec_options = XCSExecutionOptions(max_workers=4, timeout=10000)
            result = xcs.execute(
                graph,
                inputs={"query": "Example"},
                options=exec_options
            )
            ```
        """
        opts = options or XCSExecutionOptions()

        engine_options = ExecutionOptions(
            scheduler_type="parallel",  # Always use parallel execution for performance
            max_workers=opts.max_workers,
            timeout_seconds=opts.timeout / 1000 if opts.timeout else None,
            collect_metrics=True,
        )

        start_time = time.time()

        try:
            outputs = execute_graph(
                graph=graph, global_input=inputs, options=engine_options
            )
        except Exception as e:
            # Convert generic exceptions to proper XCS exceptions
            if "timeout" in str(e).lower():
                timeout_value = opts.timeout / 1000 if opts.timeout else "default"
                raise ExecutionError(
                    message=f"Graph execution timed out after {timeout_value} seconds",
                    context={
                        "timeout_seconds": timeout_value,
                        "graph_node_count": len(graph.nodes) if hasattr(graph, "nodes") else 0,
                    },
                    cause=e
                )

            if "memory" in str(e).lower():
                raise ExecutionError(
                    message="Graph execution failed due to memory constraints",
                    context={"graph_input_size": sum(len(str(v)) for v in inputs.values())},
                    cause=e
                )

            raise ExecutionError(
                message=f"Graph execution failed: {str(e)}",
                context={
                    "graph_node_count": len(graph.nodes) if hasattr(graph, "nodes") else 0,
                    "input_keys": list(inputs.keys()),
                },
                cause=e
            )

        execution_time = time.time() - start_time
        return ExecutionResult(outputs=outputs, execution_time=execution_time)

    def vmap(
        self,
        fn: Callable[..., ResultT],
        options: Optional[TransformOptions] = None,
        **kwargs: Any,
    ) -> Callable[..., Dict[str, Any]]:
        """Vectorize function across its inputs.

        Args:
            fn: Function to vectorize
            options: Vectorization configuration
            **kwargs: Additional options for implementation

        Returns:
            Vectorized function

        Raises:
            TransformError: When vectorization fails

        Example:
            ```python
            def process_item(item):
                return item * 2

            batch_process = xcs.vmap(process_item)
            results = batch_process([1, 2, 3])  # [2, 4, 6]
            ```
        """
        opts = options or TransformOptions()
        vmap_kwargs = {"in_axes": opts.in_axes, "out_axes": opts.out_axes, **kwargs}

        try:
            return cast(
                Callable[..., Dict[str, Any]],
                raw_vmap(fn, **vmap_kwargs)
            )
        except Exception as e:
            raise TransformError.for_transform(
                transform_name="vmap",
                message=f"Failed to apply vmap transform: {str(e)}",
                details={"function_name": fn.__name__ if hasattr(fn, "__name__") else "unknown"},
                cause=e
            )

    def pmap(
        self,
        fn: Callable[..., ResultT],
        options: Optional[TransformOptions] = None,
        **kwargs: Any,
    ) -> Callable[..., Dict[str, Any]]:
        """Parallelize function across multiple devices.

        Args:
            fn: Function to parallelize
            options: Parallelization configuration
            **kwargs: Additional options for implementation

        Returns:
            Parallelized function

        Raises:
            TransformError: When parallelization fails

        Example:
            ```python
            def process_item(item):
                return item * 2

            parallel_process = xcs.pmap(process_item)
            results = parallel_process([1, 2, 3])  # [2, 4, 6]
            ```
        """
        opts = options or TransformOptions()
        pmap_kwargs = {
            "in_axes": opts.in_axes,
            "out_axes": opts.out_axes,
            "devices": opts.devices,
            **kwargs,
        }

        try:
            return cast(
                Callable[..., Dict[str, Any]],
                raw_pmap(fn, **pmap_kwargs)
            )
        except Exception as e:
            # Create context with detailed device information
            context = {
                "function_name": fn.__name__ if hasattr(fn, "__name__") else "unknown",
            }

            if opts.devices:
                context["devices"] = str(opts.devices)

            raise TransformError.for_transform(
                transform_name="pmap",
                message=f"Failed to apply pmap transform: {str(e)}",
                details=context,
                cause=e
            )

    def mesh_sharded(
        self,
        fn: Callable[..., ResultT],
        mesh: DeviceMesh,
        partition_spec: PartitionSpec,
        **kwargs: Any,
    ) -> Callable[..., Dict[str, Any]]:
        """Apply mesh-based sharding to a function.

        Args:
            fn: Function to shard
            mesh: Device mesh to use
            partition_spec: Partition specification
            **kwargs: Additional options for implementation

        Returns:
            Sharded function

        Raises:
            TransformError: When mesh sharding fails

        Example:
            ```python
            mesh = xcs.DeviceMesh(devices=[0, 1, 2, 3], mesh_shape=(2, 2))
            pspec = xcs.PartitionSpec(0, 1)
            sharded_fn = xcs.mesh_sharded(fn, mesh, pspec)
            ```
        """
        try:
            return cast(
                Callable[..., Dict[str, Any]],
                raw_mesh_sharded(fn, mesh, partition_spec, **kwargs)
            )
        except Exception as e:
            # Create detailed context for diagnostics
            context = {
                "function_name": fn.__name__ if hasattr(fn, "__name__") else "unknown",
                "mesh_shape": str(mesh.mesh_shape),
                "device_count": len(mesh.devices),
                "partition_spec": str(partition_spec),
            }

            raise TransformError.for_transform(
                transform_name="mesh_sharded",
                message=f"Failed to apply mesh sharding: {str(e)}",
                details=context,
                cause=e
            )

    # Re-export classes for convenience
    DeviceMesh = DeviceMesh
    PartitionSpec = PartitionSpec

</code>

src\ember\xcs\api\types.py:
<code>
"""
Type definitions for the XCS API.

This module provides type definitions for the XCS API, ensuring type safety and
proper interface contracts. These types are used throughout the XCS system to
maintain consistency and clarity.
"""

from **future** import annotations

from dataclasses import dataclass
from typing import (
Any,
Callable,
Dict,
List,
Optional,
Protocol,
TypeVar,
Union,
runtime_checkable,
)

from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.tracer.xcs_tracing import TraceRecord

# Type variables for generic operators

T = TypeVar("T")
U = TypeVar("U")
V = TypeVar("V")

# Precise option value types for configuration systems

# These define the actual value types that flow through execution contexts

# and configuration objects throughout the XCS system

OptionValue = Union[ # Primitive types
str,
int,
float,
bool,
None, # Container types (recursive definition, using Any for circularity)
Dict[str, Any],
List[Any],
tuple, # Function references
Callable[..., Any],
]

# Strong typing for execution context dictionaries

ContextDict = Dict[str, OptionValue]

@dataclass
class XCSExecutionOptions:
"""Configuration options for XCS execution."""

    max_workers: int = 10
    """Maximum number of concurrent workers for parallel execution."""

    timeout: Optional[float] = None
    """Optional timeout in seconds for execution."""

    cache_results: bool = True
    """Whether to cache execution results."""

    debug_mode: bool = False
    """Whether to enable debug mode with additional logging."""

@dataclass
class ExecutionResult:
"""Result of executing a graph or operation."""

    outputs: Dict[str, Any]
    """The outputs from the execution."""

    execution_time: float
    """Time taken for execution in seconds."""

    node_stats: Optional[Dict[str, Dict[str, Any]]] = None
    """Optional statistics about individual node execution."""

@dataclass
class JITOptions:
"""Configuration options for JIT compilation."""

    sample_input: Optional[Dict[str, Any]] = None
    """Sample input for eager compilation."""

    force_trace: bool = False
    """Whether to force tracing on every execution."""

    recursive: bool = True
    """Whether to trace recursively."""

    cache_key_fn: Optional[Callable[[Dict[str, Any]], str]] = None
    """Optional function to create cache keys from inputs."""

@dataclass
class TransformOptions:
"""Configuration options for transforms like vmap and pmap."""

    in_axes: Optional[Union[int, Dict[str, int]]] = 0
    """Input axes for vectorization/parallelization."""

    out_axes: Optional[int] = 0
    """Output axes for vectorization/parallelization."""

    devices: Optional[List[Any]] = None
    """Devices for parallelization."""

@runtime_checkable
class GraphBuilder(Protocol):
"""Protocol for graph builders."""

    def build_graph(self, records: List[TraceRecord]) -> XCSGraph:
        """
        Build a graph from trace records.

        Args:
            records: List of trace records

        Returns:
            An XCS graph for execution
        """
        ...

**all** = [

# Type variables

"T",
"U",
"V",

# Option and context types

"OptionValue",
"ContextDict",

# Classes

"XCSExecutionOptions",
"ExecutionResult",
"JITOptions",
"TransformOptions",

# Protocols

"GraphBuilder",
]

</code>

src\ember\xcs\api\_\_init\_\_.py:
<code>
"""
XCS API

This module provides a unified interface to the XCS (Accelerated Compound Systems)
functionality. It follows the ember pattern of providing a simplified, intuitive
interface on top of the core implementation.
"""

from **future** import annotations

from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, Union

from ember.xcs.api.core import XCSAPI
from ember.xcs.api.types import (
ExecutionResult,
GraphBuilder,
JITOptions,
TransformOptions,
XCSExecutionOptions,
)

# Create a singleton instance

xcs = XCSAPI()

# Re-export types for convenience

**all** = [
"xcs",
"XCSExecutionOptions",
"ExecutionResult",
"GraphBuilder",
"JITOptions",
"TransformOptions",
]

</code>

src\ember\xcs\common\plans.py:
<code>
"""Execution planning structures for XCS.

Defines data structures for representing execution plans and tasks in the
unified execution engine. These structures are crucial for translating
computation graphs into executable tasks.
"""

import dataclasses
from typing import Any, Callable, Dict, List, Optional, Set

@dataclasses.dataclass
class XCSTask:
"""A single unit of execution within a plan.

    Represents a task to be executed by the scheduler, with its inputs,
    dependencies, and associated function/operator.

    Attributes:
        node_id: Unique identifier for this task
        operator: Function or operator to execute
        inputs: Input values for the operator
        dependencies: Task IDs that must complete before this one
        is_input_node: Whether this is an input node in the graph
        is_output_node: Whether this is an output node in the graph
    """

    node_id: str
    operator: Optional[Callable] = None
    inputs: Optional[Dict[str, Any]] = None
    dependencies: Set[str] = dataclasses.field(default_factory=set)
    is_input_node: bool = False
    is_output_node: bool = False

    def mark_as_input(self) -> None:
        """Mark this task as an input node."""
        self.is_input_node = True

    def mark_as_output(self) -> None:
        """Mark this task as an output node."""
        self.is_output_node = True

    def add_dependency(self, node_id: str) -> None:
        """Add a dependency to this task.

        Args:
            node_id: ID of the task that must complete before this one
        """
        self.dependencies.add(node_id)

@dataclasses.dataclass
class XCSPlan:
"""An execution plan built from a computation graph.

    Represents the full execution plan to be run by a scheduler, containing
    all tasks, their dependencies, and global input/output mappings.

    Attributes:
        tasks: Dictionary mapping node IDs to their task definitions
        input_nodes: Set of input node IDs
        output_nodes: Set of output node IDs
        global_input_mapping: Mapping from global inputs to specific node inputs
        global_output_mapping: Mapping from node outputs to global outputs
    """

    tasks: Dict[str, XCSTask] = dataclasses.field(default_factory=dict)
    input_nodes: Set[str] = dataclasses.field(default_factory=set)
    output_nodes: Set[str] = dataclasses.field(default_factory=set)
    global_input_mapping: Dict[str, Dict[str, str]] = dataclasses.field(
        default_factory=dict
    )
    global_output_mapping: Dict[str, str] = dataclasses.field(default_factory=dict)

    def add_task(self, task: XCSTask) -> None:
        """Add a task to the execution plan.

        Args:
            task: Task to add
        """
        self.tasks[task.node_id] = task
        if task.is_input_node:
            self.input_nodes.add(task.node_id)
        if task.is_output_node:
            self.output_nodes.add(task.node_id)

    def get_execution_order(self) -> List[str]:
        """Calculate a valid execution order for tasks.

        Performs a topological sort to determine a valid order for executing
        tasks respecting dependencies.

        Returns:
            List of node IDs in a valid execution order
        """
        visited = set()
        temp_visited = set()
        order = []

        def visit(node_id: str) -> None:
            """Recursive visit function for topological sort."""
            if node_id in visited:
                return
            if node_id in temp_visited:
                raise ValueError(f"Cycle detected in execution graph at node {node_id}")

            temp_visited.add(node_id)

            # Visit dependencies first
            for dep_id in self.tasks[node_id].dependencies:
                visit(dep_id)

            temp_visited.remove(node_id)
            visited.add(node_id)
            order.append(node_id)

        # Visit all nodes
        for node_id in self.tasks:
            if node_id not in visited:
                visit(node_id)

        return order

    def get_waves(self) -> List[List[str]]:
        """Calculate execution waves for parallel scheduling.

        Groups tasks into "waves" that can be executed in parallel, where
        each wave depends only on previous waves.

        Returns:
            List of waves, where each wave is a list of node IDs
        """
        # Calculate node depths based on dependencies
        depths: Dict[str, int] = {}

        # Start with input nodes at depth 0
        for node_id in self.input_nodes:
            depths[node_id] = 0

        # Helper function to calculate depth
        def get_depth(node_id: str) -> int:
            """Calculate the depth of a node in the graph."""
            if node_id in depths:
                return depths[node_id]

            # Calculate as 1 + max depth of dependencies
            task = self.tasks[node_id]
            if not task.dependencies:
                depths[node_id] = 0
                return 0

            max_dep_depth = max(get_depth(dep_id) for dep_id in task.dependencies)
            depths[node_id] = max_dep_depth + 1
            return depths[node_id]

        # Calculate depths for all nodes
        for node_id in self.tasks:
            get_depth(node_id)

        # Group nodes by depth into waves
        waves: Dict[int, List[str]] = {}
        for node_id, depth in depths.items():
            if depth not in waves:
                waves[depth] = []
            waves[depth].append(node_id)

        # Convert to list of waves
        max_depth = max(waves.keys()) if waves else 0
        return [waves.get(depth, []) for depth in range(max_depth + 1)]

@dataclasses.dataclass
class ExecutionResult:
"""Result of executing a computation graph.

    Contains the outputs of each node in the graph as well as metrics
    about the execution.

    Attributes:
        node_outputs: Dictionary mapping node IDs to their outputs
        metrics: Execution metrics (timing, etc.)
        errors: Dictionary of errors encountered during execution
    """

    node_outputs: Dict[str, Dict[str, Any]] = dataclasses.field(default_factory=dict)
    metrics: Dict[str, Any] = dataclasses.field(default_factory=dict)
    errors: Dict[str, Exception] = dataclasses.field(default_factory=dict)

    def get_result(self, node_id: str) -> Optional[Dict[str, Any]]:
        """Get the result for a specific node.

        Args:
            node_id: ID of the node to retrieve results for

        Returns:
            Node's output or None if not found
        """
        return self.node_outputs.get(node_id)

    def get_error(self, node_id: str) -> Optional[Exception]:
        """Get the error for a specific node.

        Args:
            node_id: ID of the node to retrieve error for

        Returns:
            Node's error or None if no error occurred
        """
        return self.errors.get(node_id)

    def has_error(self) -> bool:
        """Check if any errors occurred during execution.

        Returns:
            True if at least one node had an error
        """
        return len(self.errors) > 0

    def is_complete(self) -> bool:
        """Check if execution completed without errors.

        Returns:
            True if execution completed successfully
        """
        return not self.has_error()

    def update_metrics(self, metrics: Dict[str, Any]) -> None:
        """Update execution metrics.

        Args:
            metrics: New metrics to add
        """
        self.metrics.update(metrics)

</code>

src\ember\xcs\common\_\_init\_\_.py:
<code>
"""
Common utilities and shared structures for the XCS module.

This package contains core data structures and utilities used across
different components of the XCS system, including execution plans,
shared type definitions, and helper functions.
"""

from ember.xcs.common.plans import ExecutionResult, XCSPlan, XCSTask

**all** = [
"XCSPlan",
"XCSTask",
"ExecutionResult",
]

</code>

src\ember\xcs\engine\execution_context.py:
<code>
"""Thread-local execution context for computational graphs.

Provides thread-isolated configuration with zero-cost read paths and copy-on-write
semantics. Designed for high-performance operator execution in XCS pipelines.
"""

import contextlib
import threading
import types
from typing import Dict, Iterator, Optional, TypeVar

from ember.xcs.api.types import ContextDict, OptionValue

T = TypeVar("T")

class ExecutionContext:
"""Thread-local context system with hierarchical option inheritance.

    Provides a thread-safe configuration mechanism with the following properties:
    - Thread isolation using thread-local storage
    - Context nesting with proper option inheritance
    - Immutable context objects for safe sharing across threads
    - Zero-allocation on read paths
    """

    _thread_local = threading.local()
    _EMPTY_CONTEXT: ContextDict = types.MappingProxyType({})

    @classmethod
    def _initialize(cls) -> None:
        """Initialize thread-local state.

        Sets up the thread-local storage with default values.
        Called lazily only when needed.
        """
        cls._thread_local.stack = [cls._EMPTY_CONTEXT]
        cls._thread_local.buffer = {}

    @classmethod
    def current(cls) -> ContextDict:
        """Get current context dictionary.

        Returns:
            An immutable mapping for the current context.
            Creates default context if none exists in this thread.
        """
        if not hasattr(cls._thread_local, "stack"):
            cls._initialize()
        return cls._thread_local.stack[-1]

    @classmethod
    @contextlib.contextmanager
    def options(cls, **kwargs: OptionValue) -> Iterator[None]:
        """Set context options within a lexical scope.

        Creates a new context inheriting from the current one with the
        provided options added. Automatically restores the previous context
        when exiting the scope.

        Args:
            **kwargs: Options to set in this context scope

        Yields:
            None
        """
        # Fast path for empty options - avoid allocations
        if not kwargs:
            yield
            return

        # Initialize thread-local state if needed
        if not hasattr(cls._thread_local, "stack"):
            cls._initialize()

        # Use pre-allocated buffer to create new context
        buffer = cls._thread_local.buffer
        buffer.clear()
        buffer.update(cls.current())
        buffer.update(kwargs)

        # Create immutable view and push to context stack
        # Copy to ensure buffer can be modified while context is active
        immutable_context: ContextDict = types.MappingProxyType(buffer.copy())
        cls._thread_local.stack.append(immutable_context)

        try:
            yield
        finally:
            # Restore previous context
            if len(cls._thread_local.stack) > 1:
                cls._thread_local.stack.pop()

    @classmethod
    def get_option(cls, key: str, default: Optional[T] = None) -> OptionValue:
        """Get option value from current context.

        Retrieves a named option from the current context or returns
        the default value if not found.

        Args:
            key: Option name
            default: Default value if option not found

        Returns:
            Option value or default
        """
        return cls.current().get(key, default)

    @classmethod
    def get_all_options(cls) -> Dict[str, OptionValue]:
        """Get all options as a dictionary.

        Returns:
            Mutable copy of all current context options
        """
        return dict(cls.current())

    @classmethod
    def reset(cls) -> None:
        """Reset the thread-local context to initial state.

        Useful for testing and to ensure a clean context state.
        """
        if hasattr(cls._thread_local, "stack"):
            cls._thread_local.stack = [cls._EMPTY_CONTEXT]

        if hasattr(cls._thread_local, "buffer"):
            cls._thread_local.buffer = {}

# Compatibility layer

def get_execution_context() -> ContextDict:
"""Get current thread's execution context dictionary.

    Returns:
        Current execution context as an immutable mapping
    """
    return ExecutionContext.current()

def set_execution_options(\*\*options: OptionValue) -> None:
"""Set options in current execution context.

    This is primarily for backward compatibility. Options are only active during
    the function call which immediately returns.

    Args:
        **options: Context options to set
    """
    with ExecutionContext.options(**options):
        pass

@contextlib.contextmanager
def execution_scope(\*\*options: OptionValue) -> Iterator[None]:
"""Context manager for setting temporary execution options.

    Args:
        **options: Context options to set

    Yields:
        None
    """
    with ExecutionContext.options(**options):
        yield

</code>

src\ember\xcs\engine\execution_options.py:
<code>
"""Configuration context for XCS graph execution.

Provides a thread-safe, context-based API for controlling computational graph execution.
Manages parallelism, scheduling strategy, and resource allocation through
immutable configuration objects and thread-local storage.

Usage:

1. Context-specific execution:

   ```python
   with execution_options(scheduler="parallel", max_workers=4):
       result = my_operator(inputs=data)
   ```

2. Global configuration:

   ```python
   set_execution_options(scheduler="parallel")

   # Operations inherit settings
   result1 = operator1(inputs=data1)
   result2 = operator2(inputs=data2)
   ```

3. With JIT compilation:

   ```python
   @jit
   def process(data):
       return transformed_data

   with execution_options(scheduler="parallel", max_workers=8):
       result = process(input_data)  # Uses parallel execution
   ```

   """

import dataclasses
import threading
from typing import Any, Dict, FrozenSet, Literal, Optional, Set, Union

from ember.core.exceptions import InvalidArgumentError

# =============================================================================

# Execution configuration constants

# =============================================================================

# Valid scheduler strategies

SchedulerType = Literal["sequential", "parallel", "wave", "auto", "noop"]
VALID_SCHEDULER_TYPES: FrozenSet[str] = frozenset(
["sequential", "parallel", "wave", "auto", "noop"]
)

# Valid executor types

VALID_EXECUTORS: FrozenSet[str] = frozenset(["auto", "async", "thread"])

# =============================================================================

# Option validation utilities

# =============================================================================

def validate_option(option_name: str, value: str, valid_values: FrozenSet[str]) -> None:
"""Validate that an option value is allowed.

    Args:
        option_name: Name of the option being validated
        value: The option value
        valid_values: Set of valid values

    Raises:
        InvalidArgumentError: If value is not valid
    """
    if value.lower() not in valid_values:
        raise InvalidArgumentError.with_context(
            f"Invalid {option_name}: '{value}'. Valid values: {', '.join(sorted(valid_values))}",
            context={
                "option": option_name,
                "value": value,
                "valid_values": sorted(valid_values),
            },
        )

@dataclasses.dataclass(frozen=True)
class ExecutionOptions:
"""Immutable configuration for XCS graph execution.

    Controls scheduling, parallelism, and optimization of computational graphs.
    Thread-safe through immutability.

    Attributes:
        use_parallel: Controls parallel execution of operations
        max_workers: Thread pool size for parallel execution
        device_strategy: Execution backend selection ('auto', 'cpu')
        enable_caching: Toggles intermediate result caching
        trace_execution: Enables detailed execution tracing
        timeout_seconds: Maximum execution time before termination
        collect_metrics: Whether to collect detailed performance metrics
        debug: Whether to enable debug output
        scheduler: Scheduler strategy or instance (overrides use_parallel)
            Valid values: "sequential", "parallel", "wave", "auto", "noop"

        # Execution options
        executor: Executor selection mode
            Valid values: "auto", "async", "thread"
        fail_fast: If True, fails immediately on errors
                 If False, continues execution after errors
    """

    # Original options preserved for backward compatibility
    use_parallel: bool = True
    max_workers: Optional[int] = None
    device_strategy: str = "auto"
    enable_caching: bool = True
    trace_execution: bool = False
    timeout_seconds: Optional[float] = None
    collect_metrics: bool = False
    debug: bool = False
    # Scheduler can be:
    # - String with strategy name: "sequential", "parallel", "auto", "noop", "wave"
    # - BaseScheduler instance for direct control
    # - None to use default based on use_parallel
    scheduler: Optional[Union[SchedulerType, Any]] = None

    # Execution options with simplified naming
    executor: str = "auto"
    fail_fast: bool = True

    def __post_init__(self) -> None:
        """Validates configuration values."""
        # Validate max_workers
        if self.max_workers is not None and (
            not isinstance(self.max_workers, int) or self.max_workers <= 0
        ):
            raise InvalidArgumentError.with_context(
                "max_workers must be a positive integer or None",
                max_workers=self.max_workers,
            )

        # Validate scheduler
        if (
            isinstance(self.scheduler, str)
            and self.scheduler.lower() not in _SCHEDULER_MAP
        ):
            raise InvalidArgumentError.with_context(
                f"Unknown scheduler: {self.scheduler}",
                scheduler=self.scheduler,
                valid_schedulers=list(_SCHEDULER_MAP.keys()),
            )

        # Validate executor type
        if isinstance(self.executor, str):
            validate_option("executor", self.executor, VALID_EXECUTORS)

        # fail_fast is a boolean, no validation needed

# Scheduler name to parallel setting mapping

\_SCHEDULER_MAP: Dict[str, bool] = {
"sequential": False,
"parallel": True,
"auto": True,
"noop": False,
"wave": True,
}

# Thread-local storage

\_LOCAL = threading.local()

# Global options and lock

\_GLOBAL_OPTIONS = ExecutionOptions()
\_GLOBAL_LOCK = threading.RLock()

def get_execution_options() -> ExecutionOptions:
"""Retrieves current execution configuration.

    Returns thread-local options if set, otherwise global options.
    Never returns a reference to shared state.

    Returns:
        Current execution configuration (immutable)
    """
    return getattr(_LOCAL, "options", _GLOBAL_OPTIONS)

def set_execution_options(\*\*kwargs: Any) -> ExecutionOptions:
"""Updates global execution configuration atomically.

    Creates a new immutable configuration that applies to all subsequent
    graph executions unless overridden by a context.

    Args:
        **kwargs: Configuration parameters to update.
                 Must match ExecutionOptions attributes.

    Returns:
        New global execution options

    Raises:
        InvalidArgumentError: When provided invalid option or value
    """
    global _GLOBAL_OPTIONS

    # Validate option names against dataclass fields
    fields: Set[str] = {f.name for f in dataclasses.fields(ExecutionOptions)}
    invalid_keys = set(kwargs.keys()) - fields
    if invalid_keys:
        raise InvalidArgumentError.with_context(
            f"Invalid execution option(s): {', '.join(invalid_keys)}",
            invalid_options=sorted(invalid_keys),
            valid_options=sorted(fields),
        )

    # Handle scheduler-to-parallel mapping for backward compatibility
    if "scheduler" in kwargs and isinstance(kwargs["scheduler"], str):
        scheduler_name = kwargs["scheduler"].lower()

        # Only set use_parallel if not already in kwargs
        if "use_parallel" not in kwargs and scheduler_name in _SCHEDULER_MAP:
            kwargs["use_parallel"] = _SCHEDULER_MAP[scheduler_name]

    # Create new immutable options in a thread-safe manner
    with _GLOBAL_LOCK:
        current = dataclasses.asdict(_GLOBAL_OPTIONS)
        updated = {**current, **kwargs}
        new_options = ExecutionOptions(**updated)
        _GLOBAL_OPTIONS = new_options

    return new_options

def reset_execution_options() -> ExecutionOptions:
"""Resets execution options to system defaults.

    Clears thread-local options and resets global options.

    Returns:
        Default execution options
    """
    global _GLOBAL_OPTIONS

    # Clear thread-local options if set
    if hasattr(_LOCAL, "options"):
        delattr(_LOCAL, "options")

    # Reset global options atomically
    with _GLOBAL_LOCK:
        _GLOBAL_OPTIONS = ExecutionOptions()

    return _GLOBAL_OPTIONS

class \_ExecutionContext:
"""Thread-safe context manager for scoped execution configuration.

    Applies temporary settings within a code block using thread-local storage,
    automatically restoring previous settings when exiting.
    """

    def __init__(self, **kwargs: Any) -> None:
        """Initializes context with validated configuration.

        Args:
            **kwargs: Execution parameters to apply within context.
                    Must match ExecutionOptions attributes.

        Raises:
            InvalidArgumentError: When provided invalid options/values
        """
        # Validate options early
        fields: Set[str] = {f.name for f in dataclasses.fields(ExecutionOptions)}
        invalid_keys = set(kwargs.keys()) - fields
        if invalid_keys:
            raise InvalidArgumentError.with_context(
                f"Invalid execution option(s): {', '.join(invalid_keys)}",
                invalid_options=sorted(invalid_keys),
                valid_options=sorted(fields),
            )

        # Handle scheduler-to-parallel mapping for backward compatibility
        if "scheduler" in kwargs and isinstance(kwargs["scheduler"], str):
            scheduler_name = kwargs["scheduler"].lower()

            # Only set use_parallel if not explicitly provided
            if "use_parallel" not in kwargs and scheduler_name in _SCHEDULER_MAP:
                kwargs["use_parallel"] = _SCHEDULER_MAP[scheduler_name]

        self.kwargs = kwargs
        self.previous = None

    def __enter__(self) -> ExecutionOptions:
        """Sets thread-local options for the context duration.

        Returns:
            New execution options for this context
        """
        # Save previous thread-local options if they exist
        self.previous = getattr(_LOCAL, "options", None)

        # Create new options based on current settings
        current = dataclasses.asdict(get_execution_options())
        updated = {**current, **self.kwargs}

        # Set thread-local options
        _LOCAL.options = ExecutionOptions(**updated)

        return _LOCAL.options

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Restores previous thread-local options on exit."""
        if self.previous is not None:
            # Restore previous thread-local options
            _LOCAL.options = self.previous
        else:
            # Clear thread-local options
            if hasattr(_LOCAL, "options"):
                delattr(_LOCAL, "options")

def execution_options(\*\*kwargs: Any) -> \_ExecutionContext:
"""Creates a thread-safe context for scoped execution settings.

    Settings apply only within the context and automatically revert
    when exiting. Thread-local storage ensures contexts are properly
    isolated between threads.

    Args:
        **kwargs: Execution parameters for the context.
                Must match ExecutionOptions attributes.

    Returns:
        Context manager with specified settings

    Raises:
        InvalidArgumentError: If options/values are invalid

    Example:
        ```python
        # Parallel execution with 4 workers
        with execution_options(scheduler="parallel", max_workers=4):
            result = my_operator(inputs=data)

        # Sequential execution
        with execution_options(scheduler="sequential"):
            result = process(data)
        ```
    """
    return _ExecutionContext(**kwargs)

</code>

src\ember\xcs\engine\test_xcs_parallel_scheduler.py:
<code>
"""Unit tests for TopologicalSchedulerWithParallelDispatch.

This module tests parallel execution using the TopologicalSchedulerWithParallelDispatch.
"""

from typing import Any, Dict

from ember.xcs.engine.xcs_engine import (
TopologicalSchedulerWithParallelDispatch,
compile_graph,
)
from ember.xcs.graph.xcs_graph import XCSGraph

def dummy*operator(*, inputs: Dict[str, Any]) -> Dict[str, Any]:
"""A simple operator that multiplies input 'value' by 2."""
return {"out": inputs["value"] \_ 2}

def test_parallel_scheduler() -> None:
"""Tests parallel execution with TopologicalSchedulerWithParallelDispatch."""
graph = XCSGraph()
graph.add_node(operator=dummy_operator, node_id="node1")
plan = compile_graph(graph=graph)
scheduler = TopologicalSchedulerWithParallelDispatch(max_workers=2)
results = scheduler.run_plan(plan=plan, global_input={"value": 3}, graph=graph)
assert results["node1"] == {"out": 6}

</code>

src\ember\xcs\engine\unified_engine.py:
<code>
"""Unified execution engine for XCS graphs.

Provides a comprehensive execution system for computational graphs with
support for various scheduling strategies and execution modes.
"""

import logging
import time
from typing import Any, Dict, Optional, Union

from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.schedulers.base_scheduler import BaseScheduler
from ember.xcs.schedulers.factory import create_scheduler

logger = logging.getLogger(**name**)

# Import the canonical ExecutionOptions from execution_options.py

# to maintain a single source of truth

from ember.xcs.engine.execution_options import ExecutionOptions as BaseExecutionOptions

# Define a class that adapts BaseExecutionOptions for use in unified_engine.py

class ExecutionOptions:
"""Options controlling graph execution behavior.

    This adapter class converts between the unified engine's execution options
    and the core engine's execution options, ensuring compatibility between systems
    while maintaining a single configuration point.
    """

    def __init__(
        self,
        scheduler: str = "auto",
        max_workers: Optional[int] = None,
        timeout_seconds: Optional[float] = None,
        use_parallel: bool = True,
        continue_on_error: bool = False,
        return_partial_results: bool = True,
        **additional_options: Any,
    ):
        """Initialize execution options, compatible with unified engine.

        Args:
            scheduler: Type of scheduler to use
            max_workers: Maximum number of worker threads
            timeout_seconds: Maximum execution time before timeout
            use_parallel: Whether to enable parallelism at all
            continue_on_error: Whether to continue after node errors
            return_partial_results: Whether to return partial results on timeout/error
            **additional_options: Additional options for specialized schedulers
        """
        # Handle scheduler_type for backward compatibility
        if "scheduler_type" in additional_options:
            scheduler = additional_options.pop("scheduler_type")

        # Create a base options instance with compatible parameters
        self._base_options = BaseExecutionOptions(
            scheduler=scheduler,
            max_workers=max_workers,
            timeout_seconds=timeout_seconds,
            use_parallel=use_parallel,
            **additional_options,
        )

        # Store engine-specific options
        self.continue_on_error = continue_on_error
        self.return_partial_results = return_partial_results

    # Forward compatibility properties
    @property
    def scheduler(self) -> str:
        return self._base_options.scheduler

    @property
    def scheduler_type(self) -> str:
        # For backward compatibility
        return self._base_options.scheduler

    @property
    def max_workers(self) -> Optional[int]:
        return self._base_options.max_workers

    @property
    def timeout_seconds(self) -> Optional[float]:
        return self._base_options.timeout_seconds

    @property
    def use_parallel(self) -> bool:
        return self._base_options.use_parallel

    def __getattr__(self, name: str) -> Any:
        """Delegate attribute access to underlying options."""
        if hasattr(self._base_options, name):
            return getattr(self._base_options, name)
        raise AttributeError(f"'ExecutionOptions' has no attribute '{name}'")

    def to_base_options(self) -> BaseExecutionOptions:
        """Convert to the base options type for interoperability."""
        return self._base_options

class GraphExecutor:
"""Core execution engine for XCS graphs.

    Manages the execution of computational graphs using various scheduling
    strategies, handling errors, timeouts, and partial results.
    """

    def __init__(self) -> None:
        """Initialize the graph executor."""
        pass

    def execute(
        self,
        graph: XCSGraph,
        inputs: Dict[str, Any],
        options: Optional[ExecutionOptions] = None,
        scheduler: Optional[BaseScheduler] = None,
    ) -> Dict[str, Dict[str, Any]]:
        """Execute a computational graph with provided inputs.

        Args:
            graph: Graph to execute
            inputs: Input values for graph execution
            options: Execution options
            scheduler: Optional explicit scheduler to use

        Returns:
            Dictionary mapping node IDs to their output results
        """
        # Use default options if none provided
        if options is None:
            options = ExecutionOptions()

        # Create scheduler if not explicitly provided
        if scheduler is None:
            # Check if parallelism is disabled
            if not options.use_parallel and options.scheduler == "auto":
                # Force sequential scheduler for disabled parallelism
                scheduler_type = "sequential"
            else:
                scheduler_type = options.scheduler

            # Create appropriate scheduler
            scheduler = create_scheduler(
                scheduler_type, max_workers=options.max_workers
            )

        # Execute graph with timeout handling
        start_time = time.perf_counter()
        try:
            # Prepare for execution
            scheduler.prepare(graph)

            # Execute with scheduler
            results = scheduler.execute(graph, inputs)

            return results
        except Exception as e:
            logger.error(f"Error executing graph: {e}")
            if options.return_partial_results:
                # Return any results obtained before error
                return scheduler.get_partial_results()
            raise
        finally:
            duration = time.perf_counter() - start_time
            logger.debug(f"Graph execution complete in {duration:.4f} seconds")

# Singleton executor instance

\_executor = GraphExecutor()

def execute_graph(
graph: XCSGraph,
inputs: Dict[str, Any],
options: Optional[
Union[ExecutionOptions, BaseExecutionOptions, Dict[str, Any]]
] = None,
scheduler: Optional[BaseScheduler] = None,
) -> Dict[str, Dict[str, Any]]:
"""Execute a computational graph with provided inputs.

    This is the main entry point for graph execution in the XCS system.

    Args:
        graph: Graph to execute
        inputs: Input values for graph execution
        options: Execution options - can be an ExecutionOptions object, BaseExecutionOptions,
                 or a dict of options
        scheduler: Optional explicit scheduler to use

    Returns:
        Dictionary mapping node IDs to their output results
    """
    # Handle different option types for flexibility
    if options is None:
        # Default options
        effective_options = ExecutionOptions()
    elif isinstance(options, dict):
        # Convert dictionary to options
        effective_options = ExecutionOptions(**options)
    elif isinstance(options, BaseExecutionOptions):
        # Convert base options to engine options
        effective_options = ExecutionOptions(
            scheduler=options.scheduler,
            max_workers=options.max_workers,
            timeout_seconds=options.timeout_seconds,
            use_parallel=options.use_parallel,
        )
    else:
        # Already the right type
        effective_options = options

    # Use the singleton executor for consistency
    return _executor.execute(graph, inputs, effective_options, scheduler)

class ExecutionMetrics:
"""Metrics for graph execution performance."""

    def __init__(self) -> None:
        """Initialize execution metrics."""
        self.execution_time_ms: float = 0.0
        self.node_count: int = 0
        self.scheduler_overhead_ms: float = 0.0

class execution_options:
"""Context manager for graph execution configuration.

    Provides a concise way to set execution parameters for all graph
    operations within a block. Manages a thread-local stack of active
    configurations to support nested contexts.

    Example:
    ```python
    # Configure parallel execution with 4 workers
    with execution_options(scheduler="wave", max_workers=4):
        # All operations use these settings
        result1 = execute_graph(graph1, inputs1)

        # Nested context overrides outer settings
        with execution_options(timeout_seconds=30):
            result2 = execute_graph(graph2, inputs2)
    ```
    """

    # Thread-local option stack
    _context_stack = []

    def __init__(self, **kwargs) -> None:
        """Initialize with execution parameters.

        Args:
            **kwargs: Execution configuration parameters
        """
        self.options = kwargs

    def __enter__(self) -> None:
        """Push options onto context stack."""
        execution_options._context_stack.append(self.options)

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """Pop options from context stack."""
        execution_options._context_stack.pop()

    @staticmethod
    def get_current_options() -> Dict[str, Any]:
        """Get active execution options.

        Returns:
            Currently active options dictionary or empty dict
        """
        if not execution_options._context_stack:
            return {}
        return execution_options._context_stack[-1]

</code>

src\ember\xcs\engine\xcs_engine.py:
<code>
"""XCS execution engine for computational graphs.

Core execution infrastructure for computational graphs in Ember. Provides
mechanisms for compiling, scheduling, and dispatching graph-based computations
across different execution contexts.

This engine forms the foundation for higher-level optimizations including:

1. Just-in-Time (JIT) compilation via tracer_decorator.jit
2. Structure-based parallelization via structural_jit
3. Graph-based execution via autograph

Usage patterns:

1. Direct execution of computational graphs:

   ```python
   graph = XCSGraph()
   node1 = graph.add_node(preprocess_fn)
   node2 = graph.add_node(compute_fn)
   graph.add_edge(node1, node2)

   results = execute(graph, inputs={"data": input_data})
   ```

2. With execution options for performance tuning:

   ```python
   from ember.xcs.engine.execution_options import execution_options

   with execution_options(scheduler="parallel", max_workers=4):
       # All graph executions in this context use parallel execution
       results = my_jit_operator(inputs=data)
   ```

3. As the backend for JIT compilation:
   ```python
   @jit  # JIT uses XCS engine for compiled executions
   class MyOperator(Operator):
       def forward(self, *, inputs):
           return process(inputs)
   ```
   """

import logging
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional, Protocol, Set

from ember.xcs.graph.xcs_graph import XCSGraph

logger = logging.getLogger(**name**)

@dataclass
class XCSTask:
"""Executable task within a computation plan.

    Represents a single operation to be executed as part of a larger
    computational graph, along with its input and output relationships.

    Attributes:
        operator: Function or operator that performs the computation
        inputs: Node IDs that provide inputs to this task
        outputs: Node IDs that consume output from this task
    """

    operator: Callable[..., Dict[str, Any]]
    inputs: List[str] = field(default_factory=list)
    outputs: List[str] = field(default_factory=list)

@dataclass
class XCSPlan:
"""Compiled execution plan for a computation graph.

    Transforms a graph description into a concrete execution plan with
    tasks that can be dispatched by a scheduler. Acts as an intermediate
    representation between graph definition and execution.

    Attributes:
        tasks: Mapping from node IDs to executable task objects
        graph_id: Unique identifier for tracking plan instances
    """

    tasks: Dict[str, XCSTask] = field(default_factory=dict)
    graph_id: str = field(default_factory=lambda: str(uuid.uuid4()))

    def add_task(self, node_id: str, task: XCSTask) -> None:
        """Registers a task in the execution plan.

        Args:
            node_id: Unique identifier for the task
            task: The executable task to register
        """
        self.tasks[node_id] = task

class IScheduler(Protocol):
"""Interface for graph execution schedulers.

    Defines the contract for schedulers that organize graph nodes into
    execution waves based on their dependencies. Different implementations
    can prioritize different execution strategies (sequential, parallel, etc).
    """

    def schedule(self, graph: XCSGraph) -> List[List[str]]:
        """Creates an execution plan from a computation graph.

        Args:
            graph: Computation graph to be scheduled

        Returns:
            List of execution waves, where each wave contains node IDs
            that can be executed concurrently
        """
        ...

class TopologicalScheduler:
"""Sequential scheduler based on dependency ordering.

    Organizes nodes into execution waves where each wave contains nodes
    whose dependencies have been satisfied by previous waves. Creates
    a valid execution order that respects data dependencies.
    """

    def schedule(self, graph: XCSGraph) -> List[List[str]]:
        """Groups nodes into dependency-respecting execution waves.

        Analyzes the graph structure to determine which nodes can be
        executed concurrently without violating data dependencies,
        organizing them into sequential waves.

        Args:
            graph: Computation graph to be scheduled

        Returns:
            List of waves where each wave contains nodes that can be
            executed in parallel after all previous waves complete

        Raises:
            ValueError: If graph contains cycles
        """
        # Get topological order
        topo_order = graph.topological_sort()

        # Organize into waves based on dependencies
        waves: List[List[str]] = []
        completed_nodes: Set[str] = set()

        while topo_order:
            # Find all nodes whose dependencies are satisfied
            current_wave = []
            remaining = []

            for node_id in topo_order:
                node = graph.nodes[node_id]
                if all(dep in completed_nodes for dep in node.inbound_edges):
                    current_wave.append(node_id)
                else:
                    remaining.append(node_id)

            # Add the wave and update completed nodes
            waves.append(current_wave)
            completed_nodes.update(current_wave)
            topo_order = remaining

        return waves

class TopologicalSchedulerWithParallelDispatch(TopologicalScheduler):
"""Parallel execution scheduler using multi-threading.

    Extends the topological scheduler with parallel dispatch capabilities,
    executing each wave of nodes concurrently using a thread pool. Automatically
    adapts execution based on identified parallelization patterns in the graph.
    """

    def __init__(self, max_workers: Optional[int] = None):
        """Initialize with optional worker count limit.

        Args:
            max_workers: Maximum number of worker threads to use for each wave.
                         If None, uses number of nodes in the wave.
        """
        self.max_workers = max_workers
        super().__init__()

    def run_plan(
        self, *, plan: XCSPlan, global_input: Dict[str, Any], graph: XCSGraph
    ) -> Dict[str, Any]:
        """Execute a compiled plan using parallel dispatching.

        Executes a compiled plan with optimized parallelism strategies,
        automatically detecting parallelizable patterns in the graph metadata
        and adjusting thread allocation accordingly.

        Args:
            plan: The XCSPlan to execute
            global_input: Input data for the graph
            graph: The source graph (used for dependency analysis and metadata)

        Returns:
            A dictionary mapping node IDs to their execution results

        Raises:
            Exception: Propagates exceptions from individual node executions
                      after logging them
        """
        results: Dict[str, Dict[str, Any]] = {}
        logger.debug(f"Running plan with {len(plan.tasks)} tasks")

        # Get the waves by running schedule on the graph
        waves = self.schedule(graph)
        logger.debug(f"Execution plan with {len(waves)} waves")

        # Check for known parallel patterns in the graph metadata
        parallel_info = {}
        if hasattr(graph, "metadata") and graph.metadata:
            # Extract parallelization information
            parallelizable_nodes = graph.metadata.get("parallelizable_nodes", [])
            aggregator_nodes = graph.metadata.get("aggregator_nodes", [])
            parallel_groups = graph.metadata.get("parallel_groups", {})

            if parallelizable_nodes:
                logger.debug(f"Found {len(parallelizable_nodes)} parallelizable nodes")
                parallel_info["parallelizable"] = set(parallelizable_nodes)

            if aggregator_nodes:
                logger.debug(f"Found {len(aggregator_nodes)} aggregator nodes")
                parallel_info["aggregators"] = set(aggregator_nodes)

            if parallel_groups:
                logger.debug(f"Found {len(parallel_groups)} parallel groups")
                parallel_info["groups"] = parallel_groups

        # Execute each wave in parallel
        for wave_idx, wave in enumerate(waves):
            logger.debug(
                f"Executing wave {wave_idx+1}/{len(waves)} with {len(wave)} nodes"
            )

            # Check if this wave contains parallelizable operations
            wave_is_parallelizable = parallel_info and any(
                node_id in parallel_info.get("parallelizable", set())
                for node_id in wave
            )

            # Optimize max_workers for this wave if we have parallelization info
            wave_max_workers = min(len(wave), self.max_workers or len(wave))
            if wave_is_parallelizable and wave_max_workers > 1:
                logger.debug(
                    f"Wave {wave_idx+1} contains parallelizable operations - using {wave_max_workers} workers"
                )

            # Create a thread pool for parallel execution with appropriate worker count
            with ThreadPoolExecutor(max_workers=wave_max_workers) as executor:
                futures = {}

                # Submit each node in the wave for execution
                for node_id in wave:
                    if node_id in plan.tasks:
                        task = plan.tasks[node_id]

                        # Collect inputs from predecessors
                        # Check if input is a dictionary or other type
                        try:
                            # Handle both Pydantic models and regular dictionaries
                            if hasattr(global_input, "model_copy"):
                                inputs = global_input.model_copy()
                            else:
                                inputs = global_input.copy()

                            for pred_id in task.inputs:
                                if pred_id in results:
                                    if hasattr(inputs, "update") and callable(
                                        inputs.update
                                    ):
                                        inputs.update(results[pred_id])
                                    else:
                                        # For non-dictionary inputs, we can't update them
                                        # Just pass through global_input
                                        inputs = global_input
                                        break
                        except (AttributeError, TypeError):
                            # If copy or update fails, just use the original input
                            inputs = global_input

                        # Add node name to inputs for tracking
                        if "node_name" in global_input:
                            inputs["node_name"] = node_id

                        # Add execution context for patterns like ensemble-judge
                        # This helps operators adapt their behavior for parallel execution
                        if parallel_info:
                            # Check if this node is part of a known parallel pattern
                            is_parallelizable = node_id in parallel_info.get(
                                "parallelizable", set()
                            )
                            is_aggregator = node_id in parallel_info.get(
                                "aggregators", set()
                            )

                            if is_parallelizable or is_aggregator:
                                # Add execution hints to the input
                                if "execution_context" not in inputs:
                                    inputs["execution_context"] = {}

                                inputs["execution_context"][
                                    "parallelizable"
                                ] = is_parallelizable
                                inputs["execution_context"][
                                    "aggregator"
                                ] = is_aggregator

                                # For aggregators, also add info about which group it aggregates
                                if is_aggregator and "data_flow" in graph.metadata:
                                    data_flow = graph.metadata["data_flow"]
                                    if (
                                        node_id in data_flow
                                        and "aggregates_groups" in data_flow[node_id]
                                    ):
                                        inputs["execution_context"][
                                            "aggregates"
                                        ] = list(
                                            data_flow[node_id][
                                                "aggregates_groups"
                                            ].keys()
                                        )

                        # Submit for execution
                        futures[executor.submit(task.operator, inputs=inputs)] = node_id

                # Collect results
                for future in as_completed(futures):
                    node_id = futures[future]
                    try:
                        results[node_id] = future.result()
                    except Exception as e:
                        logger.exception(f"Error executing node {node_id}: {e}")
                        results[node_id] = {"error": str(e)}

        return results

def execute_graph(
graph: XCSGraph,
global_input: Dict[str, Any],
scheduler: Optional[IScheduler] = None,
) -> Dict[str, Dict[str, Any]]:
"""Execute a computational graph with automatic scheduling.

    Primary entry point for graph execution in XCS. Processes the graph by
    automatically detecting execution patterns, scheduling nodes into waves
    based on dependencies, and dispatching the computation across those waves.

    Args:
        graph: The computational graph to execute
        global_input: Input data for the graph's source nodes
        scheduler: Optional scheduler to determine execution order and strategy;
                  defaults to sequential TopologicalScheduler if not specified

    Returns:
        A dictionary mapping node IDs to their execution results

    Raises:
        ValueError: If the graph contains cycles or other invalid structures
    """
    if scheduler is None:
        scheduler = TopologicalScheduler()

    # Schedule the nodes
    waves = scheduler.schedule(graph)

    # Execute the graph
    results: Dict[str, Dict[str, Any]] = {}

    for wave in waves:
        # Single-threaded execution for simple scheduler
        if isinstance(scheduler, TopologicalScheduler) and not isinstance(
            scheduler, TopologicalSchedulerWithParallelDispatch
        ):
            for node_id in wave:
                node = graph.nodes[node_id]

                # Collect inputs from predecessors or use global input for source nodes
                try:
                    if not node.inbound_edges:
                        # Source node: use global input
                        inputs = (
                            global_input.copy()
                            if hasattr(global_input, "copy")
                            else global_input
                        )
                    else:
                        # Check if graph supports field-level mappings
                        if hasattr(graph, "prepare_node_inputs") and callable(
                            graph.prepare_node_inputs
                        ):
                            # Use field-level mapping for precise data flow
                            inputs = graph.prepare_node_inputs(node_id, results)
                        else:
                            # Fallback to legacy behavior: merge all predecessor outputs
                            inputs = {}
                            for pred_id in node.inbound_edges:
                                if hasattr(inputs, "update") and callable(
                                    inputs.update
                                ):
                                    inputs.update(results[pred_id])
                                else:
                                    # For non-dictionary inputs, set to the first result
                                    inputs = results[pred_id]
                                    break
                except (AttributeError, TypeError) as e:
                    logger.warning(f"Error preparing inputs for node {node_id}: {e}")
                    # If prepare_node_inputs or update fails, use global_input for source nodes
                    # or first predecessor result for non-source nodes
                    inputs = (
                        global_input
                        if not node.inbound_edges
                        else results[node.inbound_edges[0]]
                    )

                # Add node_id to inputs for tracking in test functions
                if "node_name" in global_input:
                    inputs["node_name"] = node_id

                # Execute the node
                try:
                    # Auto-convert dict to EmberModel if needed - clean, minimal, type-safe
                    if isinstance(inputs, dict) and "input_model" in node.metadata:
                        input_model = node.metadata["input_model"]
                        if hasattr(input_model, "from_dict"):
                            inputs = input_model.from_dict(inputs)

                    node_result = node.operator(inputs=inputs)
                    results[node_id] = node_result
                except Exception as e:
                    logger.exception(f"Error executing node {node_id}: {e}")
                    results[node_id] = {"error": str(e)}

        # Parallel execution for parallel scheduler
        else:
            with ThreadPoolExecutor(
                max_workers=getattr(scheduler, "max_workers", None)
            ) as executor:
                futures = {}

                # Start all jobs in the wave
                for node_id in wave:
                    node = graph.nodes[node_id]

                    # Collect inputs from predecessors or use global input for source nodes
                    try:
                        if not node.inbound_edges:
                            # Source node: use global input
                            inputs = (
                                global_input.copy()
                                if hasattr(global_input, "copy")
                                else global_input
                            )
                        else:
                            # Check if graph supports field-level mappings
                            if hasattr(graph, "prepare_node_inputs") and callable(
                                graph.prepare_node_inputs
                            ):
                                # Use field-level mapping for precise data flow
                                inputs = graph.prepare_node_inputs(node_id, results)
                            else:
                                # Fallback to legacy behavior: merge all predecessor outputs
                                inputs = {}
                                for pred_id in node.inbound_edges:
                                    if hasattr(inputs, "update") and callable(
                                        inputs.update
                                    ):
                                        inputs.update(results[pred_id])
                                    else:
                                        # For non-dictionary inputs, set to the first result
                                        inputs = results[pred_id]
                                        break
                    except (AttributeError, TypeError) as e:
                        logger.warning(
                            f"Error preparing inputs for node {node_id}: {e}"
                        )
                        # If prepare_node_inputs or update fails, use global_input for source nodes
                        # or first predecessor result for non-source nodes
                        inputs = (
                            global_input
                            if not node.inbound_edges
                            else results[node.inbound_edges[0]]
                        )

                    # Add node_id to inputs for tracking in test functions
                    if "node_name" in global_input:
                        inputs["node_name"] = node_id

                    # Auto-convert dict to EmberModel if needed - clean, minimal, type-safe
                    if isinstance(inputs, dict) and "input_model" in node.metadata:
                        input_model = node.metadata["input_model"]
                        if hasattr(input_model, "from_dict"):
                            inputs = input_model.from_dict(inputs)

                    # Submit the job
                    futures[executor.submit(node.operator, inputs=inputs)] = node_id

                # Collect results
                for future in as_completed(futures):
                    node_id = futures[future]
                    try:
                        results[node_id] = future.result()
                    except Exception as e:
                        logger.exception(f"Error executing node {node_id}: {e}")
                        results[node_id] = {"error": str(e)}

    return results

def compile_graph(graph: XCSGraph) -> XCSPlan:
"""Compile a graph into an optimized execution plan.

    Transforms a graph definition into a concrete execution plan by analyzing
    dependencies and creating executable tasks. This intermediate representation
    separates graph definition from execution concerns, enabling optimization
    and specialization for different runtime environments.

    Args:
        graph: The XCS graph to compile

    Returns:
        An XCSPlan ready for execution by a scheduler

    Raises:
        ValueError: If the graph contains cycles or invalid nodes that would
                   prevent proper execution
    """
    # Ensure graph is valid with a topological sort
    topo_order = graph.topological_sort()

    # Create a new plan
    plan = XCSPlan()

    # Convert each node into a task with proper inputs and outputs
    for node_id in topo_order:
        node = graph.nodes[node_id]

        # Create a task for this node
        task = XCSTask(
            operator=node.operator,
            inputs=node.inbound_edges.copy(),
            outputs=node.outbound_edges.copy(),
        )

        # Add the task to the plan
        plan.add_task(node_id, task)

    return plan

</code>

src\ember\xcs\engine\xcs_noop_scheduler.py:
<code>
from typing import Any, Dict, List

from ember.xcs.engine.xcs_engine import IScheduler, XCSPlan
from ember.xcs.graph.xcs_graph import XCSGraph

class XCSNoOpScheduler(IScheduler):
"""
A single-thread (no concurrency) scheduler for XCS. It runs tasks sequentially.

    This scheduler creates a wave for each node, ensuring strictly sequential execution
    with no parallelism.
    """

    def schedule(self, graph: XCSGraph) -> List[List[str]]:
        """Schedule graph nodes in a strictly sequential manner.

        Each node is placed in its own execution wave to guarantee sequential
        execution with no parallelism.

        Args:
            graph: The computational graph to schedule

        Returns:
            A list of execution waves, where each wave contains a single node ID

        Raises:
            ValueError: If the graph contains cycles
        """
        # Get nodes in topological order
        topo_order = graph.topological_sort()

        # Create one wave per node for strictly sequential execution
        waves = [[node_id] for node_id in topo_order]
        return waves

    def run_plan(
        self, *, plan: XCSPlan, global_input: Dict[str, Any], graph: XCSGraph
    ) -> Dict[str, Any]:
        """Execute a compiled plan in strictly sequential order.

        Args:
            plan: The XCSPlan to execute
            global_input: Input data for the graph
            graph: The source graph (used for reference)

        Returns:
            A dictionary of results for each node
        """
        results: Dict[str, Any] = {}
        # Iterate over tasks by node_id; call the operator directly with the provided global input.
        for node_id, task in plan.tasks.items():
            result = task.operator(inputs=global_input)
            results[node_id] = result
        return results

</code>

src\ember\xcs\engine\_\_init\_\_.py:
<code>
"""Execution engine for XCS computation graphs.

Provides the core execution engine for running computational graphs with
different scheduling strategies and optimizations. This module forms the
foundation for the XCS system's execution capabilities.

The engine follows a unified architecture with clean separation of concerns
between graph construction, scheduling, and execution.
"""

# Core execution options API

from ember.xcs.engine.execution_options import ExecutionOptions, execution_options

# Core engine functionality

from ember.xcs.engine.unified_engine import (
ExecutionMetrics,
GraphExecutor,
execute_graph,
)

# All scheduler functionality is in the schedulers package

from ember.xcs.schedulers import create_scheduler

**all** = [

# Execution options

"ExecutionOptions",
"execution_options",

# Engine core

"execute_graph",
"GraphExecutor",
"ExecutionMetrics",
"create_scheduler",
]

</code>

src\ember\xcs\graph\dependency_analyzer.py:
<code>
"""
Dependency analysis for XCS computation graphs.

Provides unified dependency tracking and analysis for all graph operations,
supporting topological sorting, transitive closure calculation, and execution
wave computation for parallel scheduling.
"""

from typing import Dict, List, Set

from ember.xcs.graph.xcs_graph import XCSGraph

class DependencyAnalyzer:
"""Unified dependency analyzer for XCS graphs.

    Analyzes node dependencies and constructs dependency graphs for
    scheduling and optimization purposes.
    """

    def analyze(self, graph: XCSGraph) -> Dict[str, Set[str]]:
        """Analyze all dependencies in a graph.

        Computes the complete dependency relationship between all nodes,
        including transitive dependencies.

        Args:
            graph: The graph to analyze

        Returns:
            Dictionary mapping each node to its complete set of dependencies
            (direct and transitive)
        """
        direct_deps = self.build_dependency_graph(graph)
        return self.compute_transitive_closure(direct_deps)

    def build_dependency_graph(self, graph: XCSGraph) -> Dict[str, Set[str]]:
        """Build a direct dependency graph.

        Constructs a mapping of each node to its direct dependencies.

        Args:
            graph: The graph to analyze

        Returns:
            Dictionary mapping each node to its direct dependencies
        """
        direct_deps: Dict[str, Set[str]] = {}

        # For each node, determine its direct dependencies
        for node_id, node in graph.nodes.items():
            direct_deps[node_id] = set(node.inbound_edges)

        return direct_deps

    def compute_transitive_closure(
        self, direct_deps: Dict[str, Set[str]]
    ) -> Dict[str, Set[str]]:
        """Compute the transitive closure of dependencies.

        Determines all direct and indirect dependencies for each node using
        a fixed-point algorithm.

        Args:
            direct_deps: Direct dependency mapping from node IDs to sets of
                         direct dependency node IDs

        Returns:
            Dictionary mapping each node to all dependencies
            (direct and transitive)
        """
        # Initialize with direct dependencies
        all_deps: Dict[str, Set[str]] = {
            node: set(deps) for node, deps in direct_deps.items()
        }

        # Add empty sets for nodes that appear as dependencies but don't have dependencies
        all_deps_nodes = set(all_deps.keys())
        all_dep_targets = {dep for deps in all_deps.values() for dep in deps}
        for node in all_dep_targets:
            if node not in all_deps_nodes:
                all_deps[node] = set()

        # Repeatedly update dependency sets until no more changes
        changed = True
        while changed:
            changed = False
            for node, deps in all_deps.items():
                new_deps = deps.copy()

                # Add transitive dependencies
                for dep in deps:
                    if dep in all_deps:
                        new_deps.update(all_deps[dep])

                # Check if the set changed
                if new_deps != deps:
                    all_deps[node] = new_deps
                    changed = True

        return all_deps

    def topological_sort(self, graph: XCSGraph) -> List[str]:
        """Perform topological sort on the graph nodes.

        Creates a linear ordering of nodes such that for every directed edge
        (A, B), node A comes before node B in the ordering.

        Args:
            graph: The graph to sort

        Returns:
            List of node IDs in topological order

        Raises:
            ValueError: If the graph contains cycles
        """
        # Use the graph's built-in topological_sort if available
        if hasattr(graph, "topological_sort") and callable(graph.topological_sort):
            return graph.topological_sort()

        # Fallback implementation
        direct_deps = self.build_dependency_graph(graph)

        # Build dependency count map
        dependency_count: Dict[str, int] = {
            node: len(deps) for node, deps in direct_deps.items()
        }

        # Build reverse dependency map
        reverse_deps: Dict[str, Set[str]] = {node: set() for node in direct_deps}
        for node, deps in direct_deps.items():
            for dep in deps:
                if dep in reverse_deps:
                    reverse_deps[dep].add(node)
                else:
                    reverse_deps[dep] = {node}

        # Start with nodes that have no dependencies
        sorted_nodes: List[str] = []
        no_deps = [node for node, count in dependency_count.items() if count == 0]

        # Process nodes in topological order
        while no_deps:
            current = no_deps.pop(0)
            sorted_nodes.append(current)

            # Update dependency counts for nodes that depend on current
            for dependent in reverse_deps.get(current, set()):
                dependency_count[dependent] -= 1
                if dependency_count[dependent] == 0:
                    no_deps.append(dependent)

        # Check for cycles
        if len(sorted_nodes) != len(direct_deps):
            raise ValueError("Graph contains cycles and cannot be topologically sorted")

        return sorted_nodes

    def compute_execution_waves(self, graph: XCSGraph) -> List[List[str]]:
        """Compute execution waves for parallel scheduling.

        Groups nodes into waves where nodes in each wave have no dependencies
        on each other and can be executed in parallel.

        Args:
            graph: The graph to analyze

        Returns:
            List of waves, each containing node IDs that can execute in parallel

        Raises:
            ValueError: If the graph contains cycles
        """
        # Get direct dependencies
        direct_deps = self.build_dependency_graph(graph)

        # Group nodes into waves
        waves: List[List[str]] = []
        remaining_nodes = set(direct_deps.keys())

        while remaining_nodes:
            # Find nodes with no dependencies in the remaining set
            current_wave = [
                node
                for node in remaining_nodes
                if all(
                    dep not in remaining_nodes for dep in direct_deps.get(node, set())
                )
            ]

            # Check for cycles
            if not current_wave:
                raise ValueError(
                    "Graph contains cycles and cannot be executed in waves"
                )

            # Add the current wave
            waves.append(current_wave)

            # Remove processed nodes
            remaining_nodes -= set(current_wave)

        return waves

</code>

src\ember\xcs\graph\graph_builder.py:
<code>
"""Graph builder for XCS graphs based on operator structure analysis.

Provides advanced graph building capabilities that analyze operator structure,
dependencies, and execution patterns to generate optimized execution graphs.
"""

import inspect
import logging
from typing import Any, Dict, List, Set

from ember.xcs.graph.xcs_graph import XCSGraph

logger = logging.getLogger(**name**)

class GraphBuilder:
"""Builder for computation graphs with node and edge creation."""

    def build_from_operator(self, operator: Any) -> XCSGraph:
        """Build a graph from an operator's structure.

        Args:
            operator: Operator to build graph from

        Returns:
            Constructed computation graph
        """
        graph = XCSGraph()

        # Get operator attributes for graph building
        op_attrs = self._get_operator_attributes(operator)

        # Create nodes for main operator and nested operators
        root_node_id = graph.add_node(
            operator=operator, name=getattr(operator, "__name__", str(operator))
        )

        # Track as root node for execution
        graph.metadata["root_id"] = root_node_id

        # Return simple graph for now - enhancement will add more structure
        return graph

    def _get_operator_attributes(self, operator: Any) -> Dict[str, Any]:
        """Extract relevant attributes from an operator.

        Args:
            operator: Operator to analyze

        Returns:
            Dictionary of operator attributes
        """
        attrs = {}

        # For class instances, extract non-callable public attributes
        if not inspect.isfunction(operator) and not inspect.ismethod(operator):
            for attr_name in dir(operator):
                if attr_name.startswith("_"):
                    continue

                attr = getattr(operator, attr_name, None)
                if callable(attr):
                    continue

                attrs[attr_name] = attr

        return attrs

class EnhancedTraceGraphBuilder(GraphBuilder):
"""Advanced graph builder with dependency analysis.

    Analyzes execution traces and operator structure to build
    richer graphs with dependency tracking, enabling optimizations
    like automatic parallelization.
    """

    def build_graph(self, records: List[Any]) -> XCSGraph:
        """Build a graph from trace records.

        Args:
            records: Trace records to build graph from

        Returns:
            Constructed computation graph
        """
        graph = XCSGraph()

        if not records:
            return graph

        # Extract operators and call relationships from records
        operators = {}
        call_edges = []

        for record in records:
            op_id = getattr(record, "operator_id", None)
            if op_id and op_id not in operators:
                operators[op_id] = getattr(record, "operator", None)

            caller_id = getattr(record, "caller_id", None)
            if op_id and caller_id:
                call_edges.append((caller_id, op_id))

        # Create nodes for operators
        node_map = {}
        for op_id, operator in operators.items():
            if not operator:
                continue

            node_id = graph.add_node(
                operator=operator, name=getattr(operator, "__name__", str(operator))
            )
            node_map[op_id] = node_id

        # Create edges for call relationships
        for caller_id, callee_id in call_edges:
            if caller_id in node_map and callee_id in node_map:
                graph.add_edge(node_map[caller_id], node_map[callee_id])

        return graph

    def build_from_trace(
        self,
        operator: Any,
        trace_data: Dict[str, Any],
        recorded_calls: Dict[str, List[Dict[str, Any]]],
    ) -> XCSGraph:
        """Build a graph using execution trace data.

        Args:
            operator: Root operator
            trace_data: Trace context data
            recorded_calls: Recorded function calls during tracing

        Returns:
            Traced computation graph
        """
        graph = XCSGraph()

        # Create node for the root operator
        root_id = graph.add_node(
            operator=operator, name=getattr(operator, "__name__", str(operator))
        )

        # Add root ID to graph metadata
        graph.metadata["root_id"] = root_id

        # Process recorded calls to build edges
        call_nodes = {}

        for func_id, calls in recorded_calls.items():
            if not calls:
                continue

            # Create a node for this function
            func = trace_data.get("functions", {}).get(func_id)
            if func is None:
                continue

            node_id = graph.add_node(
                operator=func, name=getattr(func, "__name__", str(func))
            )

            call_nodes[func_id] = node_id

            # Add root-to-function edge if called directly by root
            if trace_data.get("caller_map", {}).get(func_id) == root_id:
                graph.add_edge(root_id, node_id)

        # Add edges between functions based on caller map
        for func_id, caller_id in trace_data.get("caller_map", {}).items():
            if func_id in call_nodes and caller_id in call_nodes:
                from_id = call_nodes[caller_id]
                to_id = call_nodes[func_id]
                graph.add_edge(from_id, to_id)

        return graph

class StructuralGraphBuilder(GraphBuilder):
"""Structure-based graph builder that analyzes operator composition patterns.

    Builds graphs by analyzing the structure of operators, their attributes, and
    their relationships to optimize execution plans without requiring execution tracing.
    """

    def __init__(self) -> None:
        """Initialize the structural graph builder."""
        self.recursive = True

    def set_recursive(self, recursive: bool) -> None:
        """Set whether to recursively analyze nested operators.

        Args:
            recursive: Whether to analyze recursively
        """
        self.recursive = recursive

    def build_graph(self, operator: Any) -> XCSGraph:
        """Build a graph by analyzing an operator's structure.

        Args:
            operator: The operator to analyze

        Returns:
            Constructed computational graph
        """
        graph = XCSGraph()

        # Create node for the root operator
        root_id = graph.add_node(
            operator=operator, name=getattr(operator, "__name__", str(operator))
        )

        # Add root ID to graph metadata
        graph.metadata["root_id"] = root_id

        # Recursively analyze the operator's structure and build the graph
        if self.recursive:
            self._analyze_structure(graph, operator, root_id, visited=set())

        return graph

    def _analyze_structure(
        self,
        graph: XCSGraph,
        obj: Any,
        parent_id: str,
        visited: Set[int],
        attr_path: str = "",
    ) -> None:
        """Recursively analyze an object's structure and add nodes to the graph.

        Args:
            graph: The graph to build
            obj: The object to analyze
            parent_id: ID of the parent node
            visited: Set of object IDs that have already been visited
            attr_path: Attribute path from the root object
        """
        # Avoid cycles with visited set
        obj_id = id(obj)
        if obj_id in visited:
            return
        visited.add(obj_id)

        # Extract attributes
        attrs = self._get_operator_attributes(obj)

        # Look for operator-like objects in attributes
        for attr_name, attr_val in attrs.items():
            # Skip non-object attributes
            if attr_val is None or isinstance(attr_val, (str, int, float, bool)):
                continue

            # Handle list/tuple of operators
            if isinstance(attr_val, (list, tuple)) and attr_val:
                for i, item in enumerate(attr_val):
                    # Check if item has forward method or is callable
                    if hasattr(item, "forward") or callable(item):
                        # Create a node for this item
                        node_name = f"{attr_name}[{i}]"
                        full_path = (
                            f"{attr_path}.{node_name}" if attr_path else node_name
                        )

                        node_id = graph.add_node(operator=item, name=node_name)

                        # Add edge from parent to this node
                        graph.add_edge(parent_id, node_id)

                        # Recursively analyze the item's structure
                        self._analyze_structure(
                            graph, item, node_id, visited, full_path
                        )

            # Handle individual operator-like objects
            elif hasattr(attr_val, "forward") or callable(attr_val):
                # Create a node for this attribute
                full_path = f"{attr_path}.{attr_name}" if attr_path else attr_name

                node_id = graph.add_node(operator=attr_val, name=attr_name)

                # Add edge from parent to this node
                graph.add_edge(parent_id, node_id)

                # Recursively analyze the attribute's structure
                self._analyze_structure(graph, attr_val, node_id, visited, full_path)

</code>

src\ember\xcs\graph\xcs_graph.py:
<code>
"""Computation graph for XCS execution.

Defines a directed acyclic graph structure for representing and executing
computational flows. Operators form nodes in the graph, with edges representing
data dependencies between operations.

Example:

````python
graph = XCSGraph()

    # Add computation nodes
    input_node = graph.add_node(preprocess_fn, name="preprocess")
    compute_node = graph.add_node(compute_fn, name="compute")
    output_node = graph.add_node(postprocess_fn, name="postprocess")

    # Define data flow
    graph.add_edge(input_node, compute_node)
    graph.add_edge(compute_node, output_node)

    # Execute the computation with an execution engine
    from ember.xcs.engine import execute
    results = execute(graph, inputs={"data": input_data})
    ```

"""

import dataclasses
import uuid
from collections import deque
from typing import Any, Callable, Dict, List, Optional

@dataclasses.dataclass
class XCSEdge:
"""Edge connecting two nodes with field-level mapping information.

    Represents a data dependency between nodes with precise information about
    which output fields connect to which input fields.

    Attributes:
        from_node: Source node ID producing data
        to_node: Destination node ID consuming data
        field_mappings: Maps output fields to input fields for precise data flow
    """

    from_node: str
    to_node: str
    field_mappings: Dict[str, str] = dataclasses.field(default_factory=dict)

    def add_field_mapping(self, output_field: str, input_field: str) -> None:
        """Add mapping from output field to input field.

        Args:
            output_field: Field name in the source node's output
            input_field: Field name in the destination node's input
        """
        self.field_mappings[output_field] = input_field

@dataclasses.dataclass
class XCSNode:
"""Single computation node in an execution graph.

    Represents one operation in a computational flow with its connections
    to other nodes. Each node contains an executable operator and maintains
    its position in the graph through edge lists.

    Attributes:
        operator: Callable function or operator executing this node's computation
        node_id: Unique identifier for addressing this node in the graph
        inbound_edges: Node IDs that provide inputs to this node
        outbound_edges: Node IDs that consume output from this node
        name: Human-readable label for debugging and visualization
        metadata: Additional node properties (e.g., cost estimates, device placement)
    """

    operator: Callable[..., Dict[str, Any]]
    node_id: str
    inbound_edges: List[str] = dataclasses.field(default_factory=list)
    outbound_edges: List[str] = dataclasses.field(default_factory=list)
    name: Optional[str] = None
    metadata: Dict[str, Any] = dataclasses.field(default_factory=dict)

# For backward compatibility

XCSGraphNode = XCSNode

class XCSGraph:
"""Directed graph for computational workflows.

    Provides a structure for defining complex computational flows as directed
    graphs. Supports operations needed for graph analysis, transformation, and
    execution by the XCS execution engine.
    """

    def __init__(self) -> None:
        """Creates an empty computation graph."""
        self.nodes: Dict[str, XCSNode] = {}
        self.edges: Dict[str, XCSEdge] = {}  # Edge registry for field mappings
        self.metadata: Dict[str, Any] = {}

    def add_node(
        self,
        operator: Callable[..., Dict[str, Any]],
        node_id: Optional[str] = None,
        name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        node_key: Optional[str] = None,  # Backward compatibility
        input_mapping: Optional[Dict[str, str]] = None,  # Backward compatibility
    ) -> str:
        """Adds a computation node to the graph.

        Args:
            operator: Function or operator to execute at this node
            node_id: Unique identifier (auto-generated if None)
            name: Human-readable label for the node
            metadata: Additional properties for analysis and optimization
            node_key: (Backward compatibility) Alternative name, takes precedence over name
            input_mapping: (Backward compatibility) Field mappings for node inputs
            name: Human-readable label for the node
            metadata: Additional properties for analysis and optimization

        Returns:
            Generated or provided node ID

        Raises:
            ValueError: If node_id already exists in the graph
        """
        # Handle backward compatibility parameters
        if node_key is not None:
            name = node_key  # node_key takes precedence over name for backward compatibility

        if node_id is None:
            node_id = str(uuid.uuid4())

        if node_id in self.nodes:
            raise ValueError(f"Node with ID '{node_id}' already exists.")

        # Create actual node with combined metadata
        node_metadata = metadata or {}
        if input_mapping:
            node_metadata["input_mapping"] = input_mapping

        self.nodes[node_id] = XCSNode(
            operator=operator, node_id=node_id, name=name, metadata=node_metadata
        )

        return node_id

    def add_edge(
        self, from_id: str, to_id: str, field_mappings: Optional[Dict[str, str]] = None
    ) -> XCSEdge:
        """Creates a directed data dependency between nodes.

        Establishes that the output of one node flows into another,
        forming a directed edge in the computation graph.

        Args:
            from_id: Source node producing output data
            to_id: Destination node consuming the data
            field_mappings: Optional mapping from output fields to input fields

        Returns:
            The created edge object

        Raises:
            ValueError: If either node doesn't exist in the graph
        """
        if from_id not in self.nodes:
            raise ValueError(f"Source node '{from_id}' does not exist.")
        if to_id not in self.nodes:
            raise ValueError(f"Destination node '{to_id}' does not exist.")

        # Create or retrieve the edge
        edge_key = f"{from_id}_{to_id}"
        if edge_key not in self.edges:
            edge = XCSEdge(from_node=from_id, to_node=to_id)
            self.edges[edge_key] = edge
        else:
            edge = self.edges[edge_key]

        # Add field mappings if provided
        if field_mappings:
            for output_field, input_field in field_mappings.items():
                edge.add_field_mapping(output_field, input_field)

        # Maintain backward compatibility with node edge lists
        self.nodes[from_id].outbound_edges.append(to_id)
        self.nodes[to_id].inbound_edges.append(from_id)

        return edge

    def topological_sort(self) -> List[str]:
        """Orders nodes so dependencies come before dependents.

        Produces an execution ordering where each node appears after
        all nodes it depends on, ensuring valid sequential execution.

        Returns:
            List of node IDs in dependency-respecting order

        Raises:
            ValueError: If graph contains cycles (not a DAG)
        """
        # Track remaining dependencies for each node
        in_degree = {
            node_id: len(node.inbound_edges) for node_id, node in self.nodes.items()
        }
        queue = deque([node_id for node_id in self.nodes if in_degree[node_id] == 0])
        sorted_nodes = []

        # Process nodes in topological order
        while queue:
            current = queue.popleft()
            sorted_nodes.append(current)

            for neighbor in self.nodes[current].outbound_edges:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        # Verify complete ordering (no cycles)
        if len(sorted_nodes) != len(self.nodes):
            raise ValueError("Graph contains a cycle")

        return sorted_nodes

    def prepare_node_inputs(
        self, node_id: str, results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Prepares inputs for a node based on edge field mappings.

        Args:
            node_id: The node to prepare inputs for
            results: Dictionary mapping node IDs to their output results

        Returns:
            Dictionary of inputs prepared for the node's execution
        """
        # Build inputs dictionary from upstream results
        inputs = {}
        incoming_node_ids = self.nodes[node_id].inbound_edges

        for from_id in incoming_node_ids:
            edge_key = f"{from_id}_{node_id}"
            if edge_key not in self.edges or from_id not in results:
                continue

            edge = self.edges[edge_key]
            source_results = results[from_id]

            # Map fields according to edge mappings
            if edge.field_mappings:
                for output_field, input_field in edge.field_mappings.items():
                    if output_field in source_results:
                        inputs[input_field] = source_results[output_field]
            else:
                # Default behavior: merge all results
                inputs.update(source_results)

        return inputs

    def __str__(self) -> str:
        """Creates a human-readable graph representation.

        Generates a structured text description showing nodes and
        their connections, useful for debugging and visualization.

        Returns:
            Multi-line string describing the graph structure
        """
        nodes_str = [
            f"Node {node_id}: {node.name or 'unnamed'}"
            for node_id, node in self.nodes.items()
        ]
        edges_str = []
        for edge_key, edge in self.edges.items():
            field_str = ""
            if edge.field_mappings:
                mappings = ", ".join(
                    f"{k}->{v}" for k, v in edge.field_mappings.items()
                )
                field_str = f" ({mappings})"
            edges_str.append(f"{edge.from_node} -> {edge.to_node}{field_str}")

        return (
            f"XCSGraph with {len(self.nodes)} nodes, {len(self.edges)} edges:\n"
            + "\n".join(nodes_str)
            + "\n\nEdges:\n"
            + "\n".join(edges_str)
        )

def merge_xcs_graphs(base: XCSGraph, additional: XCSGraph, namespace: str) -> XCSGraph:
"""Combines two computation graphs with namespace isolation.

    Creates a new graph containing all nodes from both input graphs,
    with nodes from the additional graph prefixed to avoid collisions.
    Preserves all edge connections and field mappings, adjusting IDs as needed.

    Args:
        base: Primary graph to merge into
        additional: Secondary graph to incorporate with namespace prefixing
        namespace: Prefix for additional graph's node IDs for isolation

    Returns:
        New graph containing nodes and edges from both inputs

    Example:
        ```python
        # Merge specialized processing graph into main workflow
        main_graph = XCSGraph()  # Main computation pipeline
        process_graph = XCSGraph()  # Specialized processing subgraph

        # Combine while isolating process_graph nodes
        merged = merge_xcs_graphs(main_graph, process_graph, "process")
        ```
    """
    merged = XCSGraph()

    # Copy base graph nodes with original IDs
    for node_id, node in base.nodes.items():
        merged.add_node(
            operator=node.operator,
            node_id=node_id,
            name=node.name,
            metadata=node.metadata.copy(),
        )

    # Copy additional graph nodes with namespaced IDs to prevent collisions
    node_mapping = {}  # Maps original IDs to namespaced IDs
    for node_id, node in additional.nodes.items():
        namespaced_id = f"{namespace}_{node_id}"
        # Ensure uniqueness with random suffix if needed
        if namespaced_id in merged.nodes:
            namespaced_id = f"{namespace}_{node_id}_{uuid.uuid4().hex[:8]}"

        merged.add_node(
            operator=node.operator,
            node_id=namespaced_id,
            name=node.name,
            metadata=node.metadata.copy(),
        )
        node_mapping[node_id] = namespaced_id

    # Recreate edge connections from base graph (unchanged)
    for edge_key, edge in base.edges.items():
        # Copy edge with its field mappings
        merged.add_edge(
            from_id=edge.from_node,
            to_id=edge.to_node,
            field_mappings=edge.field_mappings.copy() if edge.field_mappings else None,
        )

    # Recreate edge connections from additional graph (with ID translation)
    for edge_key, edge in additional.edges.items():
        # Translate source and destination IDs
        from_id = node_mapping.get(edge.from_node, edge.from_node)
        to_id = node_mapping.get(edge.to_node, edge.to_node)

        # Copy edge with its field mappings
        merged.add_edge(
            from_id=from_id,
            to_id=to_id,
            field_mappings=edge.field_mappings.copy() if edge.field_mappings else None,
        )

    # Merge metadata (without overwriting)
    for key, value in base.metadata.items():
        merged.metadata[key] = value

    # Add additional metadata with namespace prefix to avoid collisions
    for key, value in additional.metadata.items():
        merged.metadata[f"{namespace}_{key}"] = value

    return merged

</code>

src\ember\xcs\graph\_\_init\_\_.py:
<code>
"""
Graph representation and analysis for XCS.

Provides data structures and utilities for representing, analyzing, and
manipulating computational graphs.
"""

# Dependency analysis

from ember.xcs.graph.dependency_analyzer import DependencyAnalyzer

# Graph building

from ember.xcs.graph.graph_builder import EnhancedTraceGraphBuilder, GraphBuilder

# Core graph representation

from ember.xcs.graph.xcs_graph import XCSGraph, XCSNode

**all** = [
# Core graph representation
"XCSGraph",
"XCSNode",
# Dependency analysis
"DependencyAnalyzer",
# Graph building
"GraphBuilder",
"EnhancedTraceGraphBuilder",
]

</code>

src\ember\xcs\jit\strategies\base_strategy.py:
<code>
"""Base class for JIT compilation strategies.

Defines the Strategy protocol that all JIT strategies must implement,
ensuring consistent interfaces for analyzing and compiling functions.
"""

import inspect
from typing import Any, Callable, Dict, Optional, Protocol, TypeVar

from ember.xcs.jit.cache import JITCache, get_cache

# Define type variable for functions

F = TypeVar("F", bound=Callable)

class Strategy(Protocol):
"""Protocol defining the interface for JIT compilation strategies.

    All JIT compilation strategies must implement this protocol to ensure
    consistent interfaces for analysis and compilation.
    """

    def analyze(self, func: Callable) -> Dict[str, Any]:
        """Analyze a function to determine if this strategy is suitable.

        Args:
            func: Function to analyze

        Returns:
            Dictionary with analysis results including:
            - score: Suitability score (higher is better)
            - rationale: Reason for the score
        """
        ...

    def compile(
        self, func: F, sample_input: Optional[Dict[str, Any]] = None, **kwargs: Any
    ) -> F:
        """Compile a function using this strategy.

        Args:
            func: Function to compile
            sample_input: Optional sample input for eager compilation
            **kwargs: Additional strategy-specific options

        Returns:
            Compiled function with the same signature
        """
        ...

class BaseStrategy:
"""Base class for JIT compilation strategies.

    Provides common functionality for all JIT strategies, including
    feature extraction, caching, and control methods.
    """

    def _extract_common_features(self, func: Callable[..., Any]) -> Dict[str, Any]:
        """Extract common features from a function for analysis.

        Args:
            func: Function to analyze

        Returns:
            Dictionary with common features
        """
        # Basic features
        features = {
            "is_class": inspect.isclass(func),
            "is_function": inspect.isfunction(func),
            "is_method": inspect.ismethod(func),
            "has_call": callable(func) and callable(func.__call__),
            "has_forward": hasattr(func, "forward") and callable(func.forward),
            "has_specification": hasattr(func, "specification"),
            "module": getattr(func, "__module__", "unknown"),
            "name": getattr(func, "__name__", "unnamed"),
        }

        # Try to get source code features
        try:
            source = inspect.getsource(func)
            features["has_source"] = True
            features["source_lines"] = len(source.splitlines())
            features["source_size"] = len(source)
        except (TypeError, OSError, IOError):
            features["has_source"] = False
            features["source_lines"] = 0
            features["source_size"] = 0

        return features

    def _get_cache(self, cache: Optional[JITCache] = None) -> JITCache:
        """Get the JIT cache to use.

        Args:
            cache: Optional explicit cache

        Returns:
            JIT cache instance
        """
        if cache is None:
            return get_cache()
        return cache

    def _add_control_methods(
        self, compiled_func: Callable, original_func: Callable, cache: JITCache
    ) -> None:
        """Add control methods to the compiled function.

        Args:
            compiled_func: Compiled function
            original_func: Original function
            cache: JIT cache
        """

        # Add disable/enable methods
        def disable_jit() -> None:
            compiled_func._jit_disabled = True

        def enable_jit() -> None:
            compiled_func._jit_disabled = False

        def get_stats() -> Dict[str, Any]:
            return cache.get_metrics(original_func)

        compiled_func.disable_jit = disable_jit
        compiled_func.enable_jit = enable_jit
        compiled_func.get_stats = get_stats
        compiled_func._original_function = original_func
        compiled_func._jit_strategy = self.__class__.__name__.replace(
            "Strategy", ""
        ).lower()

class JITFallbackMixin:
"""Mixin for JIT strategies that provides fallback functionality.

    This mixin adds methods for handling fallback cases when a JIT strategy
    cannot be applied or encounters errors during tracing/compilation.
    """

    def fallback_compile(self, func: F) -> F:
        """Provide a fallback compilation when the strategy cannot be applied.

        Args:
            func: The original function

        Returns:
            A function that executes the original function directly
        """
        return func

    def should_fallback(self, func: Callable) -> bool:
        """Determine whether compilation should fall back to the original function.

        Args:
            func: Function to analyze

        Returns:
            True if compilation should fall back, False otherwise
        """
        return False

</code>

src\ember\xcs\jit\strategies\enhanced.py:
<code>
"""Enhanced JIT compilation strategy.

Implements JIT compilation with improved parallelism detection and optimization.
This strategy combines aspects of both trace-based and structural analysis
approaches, making it particularly effective for complex operators with loop-based
patterns like ensembles.
"""

import functools
import logging
import time
from typing import Any, Callable, Dict, Optional

from ember.xcs.graph.graph_builder import EnhancedTraceGraphBuilder
from ember.xcs.jit.cache import JITCache
from ember.xcs.jit.strategies.base_strategy import BaseStrategy, JITFallbackMixin

logger = logging.getLogger(**name**)

class EnhancedStrategy(BaseStrategy, JITFallbackMixin):
"""Enhanced JIT compilation strategy.

    Compiles operators using sophisticated analysis that identifies parallelization
    opportunities even within sequential execution patterns. This strategy is
    optimized for complex operators like ensembles.
    """

    def __init__(self) -> None:
        """Initialize the enhanced strategy."""
        self.graph_builder = EnhancedTraceGraphBuilder()

    def analyze(self, func: Callable[..., Any]) -> Dict[str, Any]:
        """Analyze a function to determine if enhanced JIT is appropriate.

        Args:
            func: Function to analyze

        Returns:
            Dictionary with analysis results
        """
        import inspect

        features = self._extract_common_features(func)
        score = 0
        rationale = []

        # Check for ensemble patterns
        ensemble_indicators = ["ensemble", "aggregate", "combine", "accumulate"]
        if features["is_class"]:
            # Check for common ensemble operator naming patterns
            class_name = func.__name__.lower()
            if any(indicator in class_name for indicator in ensemble_indicators):
                score += 30
                rationale.append("Name suggests ensemble pattern")

            # Check for iteration methods/patterns
            if hasattr(func, "items") or hasattr(func, "__iter__"):
                score += 20
                rationale.append("Has iteration capability")

        # Check for code patterns indicating nested loops
        if features["has_source"] and features["source_lines"] > 5:
            try:
                source = inspect.getsource(func)
                loop_count = source.count("for ") + source.count("while ")
                if loop_count > 1:
                    score += 40
                    rationale.append(f"Contains multiple loops ({loop_count})")
                elif loop_count > 0:
                    score += 20
                    rationale.append("Contains loops")
            except Exception:
                pass

        # Enhanced JIT is a good general-purpose fallback
        score += 10
        rationale.append("Good general-purpose option")

        return {
            "score": score,
            "rationale": "; ".join(rationale),
            "features": features,
        }

    def compile(
        self,
        func: Callable[..., Any],
        sample_input: Optional[Dict[str, Any]] = None,
        force_trace: bool = False,
        recursive: bool = True,
        cache: Optional[JITCache] = None,
        preserve_stochasticity: bool = False,
        **options: Any,
    ) -> Callable[..., Any]:
        """Compile a function using enhanced JIT.

        Args:
            func: Function to compile
            sample_input: Optional sample input for eager compilation
            force_trace: Whether to force tracing on every call
            recursive: Whether to JIT compile nested calls
            cache: JIT cache to use
            preserve_stochasticity: When True, always executes the original function
                to maintain stochastic behavior (important for LLMs)
            **options: Additional options

        Returns:
            Compiled function
        """
        from ember.xcs.tracer.xcs_tracing import TracerContext

        # Use provided cache or get the default
        cache = self._get_cache(cache)

        @functools.wraps(func)
        def enhanced_function(*, inputs: Dict[str, Any]) -> Any:
            # Check if JIT is disabled, forcing trace, or preserving stochasticity
            if (getattr(enhanced_function, "_jit_disabled", False) or
                    force_trace or preserve_stochasticity):
                # Execute directly without JIT
                # For LLMs, this ensures we get stochastic outputs on each call
                execution_start = time.time()
                result = func(inputs=inputs)
                execution_duration = time.time() - execution_start
                cache.metrics.record_execution(execution_duration)
                return result

            # Try to get state signature
            state_signature = None
            if hasattr(func, "get_state_signature") and callable(
                func.get_state_signature
            ):
                state_signature = func.get_state_signature()

            # Try to get compiled graph
            graph = cache.get_with_state(func, state_signature)

            if graph is not None:
                # Execute cached graph
                try:
                    from ember.xcs.jit.execution_utils import execute_compiled_graph

                    return execute_compiled_graph(graph, inputs, cache, func=func)
                except Exception as e:
                    logger.warning(
                        f"Error executing graph: {e}. Falling back to direct execution."
                    )

            # Trace execution if compilation fails or is disabled
            tracing_start = time.time()
            with TracerContext() as tracer:
                # Record recursive flag in tracer context
                tracer.enable_recursion = recursive

                # Execute the function and record trace
                execution_start = time.time()
                result = func(inputs=inputs)
                execution_duration = time.time() - execution_start
                cache.metrics.record_execution(execution_duration)

                # Build graph from trace if non-trivial
                if tracer.records and len(tracer.records) > 1:
                    compilation_start = time.time()
                    graph = self.graph_builder.build_graph(tracer.records)
                    compilation_duration = time.time() - compilation_start
                    cache.metrics.record_compilation(compilation_duration)

                    # Cache compiled graph
                    state_signature = None
                    if hasattr(func, "get_state_signature") and callable(
                        func.get_state_signature
                    ):
                        state_signature = func.get_state_signature()

                    cache.set_with_state(func, graph, state_signature)

            # Record tracing time
            tracing_duration = time.time() - tracing_start
            cache.metrics.record_tracing(tracing_duration)

            return result

        # Add control methods
        self._add_control_methods(enhanced_function, func, cache)

        return enhanced_function

</code>

src\ember\xcs\jit\strategies\structural.py:
<code>
"""Structure-based JIT compilation strategy.

Implements JIT compilation by analyzing the structure of operators directly
without executing them. This strategy is particularly effective for container
operators with nested sub-operators, as it can detect parallelization opportunities
based on the operator structure.
"""

import functools
import inspect
import logging
import time
from typing import Any, Callable, Dict, Optional

from ember.xcs.graph.graph_builder import StructuralGraphBuilder
from ember.xcs.jit.cache import JITCache
from ember.xcs.jit.strategies.base_strategy import BaseStrategy, JITFallbackMixin

logger = logging.getLogger(**name**)

class StructuralStrategy(BaseStrategy, JITFallbackMixin):
"""Structure-based JIT compilation strategy.

    Compiles operators by analyzing their structure directly, without
    execution tracing. This approach is particularly effective for
    container operators with nested sub-operators.
    """

    def __init__(self) -> None:
        """Initialize the structural strategy."""
        self.graph_builder = StructuralGraphBuilder()

    def analyze(self, func: Callable[..., Any]) -> Dict[str, Any]:
        """Analyze a function to determine if structural JIT is appropriate.

        Args:
            func: Function to analyze

        Returns:
            Dictionary with analysis results
        """
        features = self._extract_common_features(func)
        score = 0
        rationale = []

        # Check if it's an operator class with forward method
        if features["is_class"]:
            # Check for operator characteristics
            if hasattr(func, "forward") and callable(getattr(func, "forward", None)):
                score += 30
                rationale.append("Has 'forward' method (likely an operator)")

            # Check for nested operators
            has_operator_fields = False
            for attr_name in dir(func):
                if attr_name.startswith("_"):
                    continue

                attr = getattr(func, attr_name, None)
                if inspect.isclass(attr) and hasattr(attr, "forward"):
                    has_operator_fields = True
                    break

            if has_operator_fields:
                score += 40
                rationale.append("Has nested operator fields (container pattern)")

        # Check for specific method signatures that indicate operator composition
        if callable(func) and callable(func.__call__):
            score += 20
            rationale.append("Has __call__ method")

        # Check for known operator-specific attributes
        if hasattr(func, "specification"):
            score += 10
            rationale.append("Has 'specification' attribute (operator pattern)")

        return {
            "score": score,
            "rationale": "; ".join(rationale),
            "features": features,
        }

    def compile(
        self,
        func: Callable[..., Any],
        sample_input: Optional[Dict[str, Any]] = None,
        force_trace: bool = False,
        recursive: bool = True,
        cache: Optional[JITCache] = None,
        preserve_stochasticity: bool = False,
        **options: Any,
    ) -> Callable[..., Any]:
        """Compile a function using structural JIT.

        Args:
            func: Function to compile
            sample_input: Optional sample input (not used in structural JIT)
            force_trace: Whether to force analysis on every call
            recursive: Whether to recursively analyze nested operators
            cache: JIT cache to use
            preserve_stochasticity: When True, always executes the original function
                to maintain stochastic behavior (important for LLMs)
            **options: Additional options

        Returns:
            Compiled function
        """
        # Use provided cache or get the default
        cache = self._get_cache(cache)

        @functools.wraps(func)
        def structural_function(*, inputs: Dict[str, Any]) -> Any:
            # Check if JIT is disabled, forcing trace, or preserving stochasticity
            if (getattr(structural_function, "_jit_disabled", False) or
                    force_trace or preserve_stochasticity):
                # Execute directly
                # For LLMs, this ensures we get stochastic outputs on each call
                execution_start = time.time()
                result = func(inputs=inputs)
                execution_duration = time.time() - execution_start
                cache.metrics.record_execution(execution_duration)
                return result

            # Try to get structure signature
            state_signature = None
            if hasattr(func, "get_structure_signature") and callable(
                func.get_structure_signature
            ):
                state_signature = func.get_structure_signature()

            # Try to get cached graph
            graph = cache.get_with_state(func, state_signature)

            if graph is not None:
                # Execute cached graph
                try:
                    from ember.xcs.jit.execution_utils import execute_compiled_graph

                    return execute_compiled_graph(graph, inputs, cache, func=func)
                except Exception as e:
                    logger.warning(
                        f"Error executing graph: {e}. Falling back to direct execution."
                    )

            # Analyze structure and build graph
            compilation_start = time.time()

            # Set recursion flag if available on the graph builder
            if hasattr(self.graph_builder, "set_recursive"):
                self.graph_builder.set_recursive(recursive)

            graph = self.graph_builder.build_graph(func)
            compilation_duration = time.time() - compilation_start
            cache.metrics.record_compilation(compilation_duration)

            # Cache compiled graph
            state_signature = None
            if hasattr(func, "get_structure_signature") and callable(
                func.get_structure_signature
            ):
                state_signature = func.get_structure_signature()

            cache.set_with_state(func, graph, state_signature)

            # Execute and return with fallback
            return self.execute_with_fallback(graph, func, inputs, cache)

        # Add control methods
        self._add_control_methods(structural_function, func, cache)

        return structural_function

    def execute_with_fallback(
        self,
        graph: Any,
        original_func: Callable,
        inputs: Dict[str, Any],
        cache: JITCache,
    ) -> Any:
        """Execute a compiled graph with fallback to direct execution.

        Args:
            graph: Compiled graph
            original_func: Original function
            inputs: Input values
            cache: JIT cache

        Returns:
            Execution result
        """
        try:
            # Try to execute the compiled graph
            from ember.xcs.jit.execution_utils import execute_compiled_graph

            return execute_compiled_graph(graph, inputs, cache, func=original_func)
        except Exception as e:
            # Log error and fall back to direct execution
            logger.warning(
                f"Error executing compiled graph: {e}. Falling back to direct execution."
            )

            # Execute directly and time it
            execution_start = time.time()
            result = original_func(inputs=inputs)
            execution_duration = time.time() - execution_start
            cache.metrics.record_execution(execution_duration)

            return result

</code>

src\ember\xcs\jit\strategies\trace.py:
<code>
"""Trace-based JIT compilation strategy.

Implements JIT compilation by tracing function execution to capture the
computation graph dynamically. This approach is most effective for operators
with data-dependent execution patterns.
"""

import functools
import inspect
import logging
import time
from typing import Any, Callable, Dict, Optional

from ember.xcs.jit.cache import JITCache
from ember.xcs.jit.strategies.base_strategy import BaseStrategy, JITFallbackMixin

logger = logging.getLogger(**name**)

class TraceStrategy(BaseStrategy, JITFallbackMixin):
"""Trace-based JIT compilation strategy.

    Compiles operators by executing them with tracing enabled and recording
    the execution graph. This approach works well for operators with simple
    execution patterns.
    """

    def analyze(self, func: Callable) -> Dict[str, Any]:
        """Analyze a function to determine if trace-based JIT is appropriate.

        Args:
            func: Function to analyze

        Returns:
            Dictionary with analysis results
        """

        # Extract basic features
        features = self._extract_common_features(func)
        score = 0
        rationale = []

        # Trace-based JIT is good for simple functions
        if features["source_lines"] < 20:
            score += 30
            rationale.append("Simple function (< 20 lines)")

        # Check for simple conditional patterns
        if features["has_source"]:
            # The source is not directly stored in features, but we can use inspect again
            try:
                source = inspect.getsource(func)
                if source.count("if ") < 3 and source.count("for ") < 2:
                    score += 20
                    rationale.append("Simple control flow")
            except Exception:
                pass

        # Trace is the most basic strategy, so it has a base score
        score += 5
        rationale.append("Basic fallback strategy")

        return {
            "score": score,
            "rationale": "; ".join(rationale),
            "features": features,
        }

    def compile(
        self,
        func: Callable[..., Any],
        sample_input: Optional[Dict[str, Any]] = None,
        force_trace: bool = False,
        recursive: bool = True,
        cache: Optional[JITCache] = None,
        preserve_stochasticity: bool = False,
        **options: Any,
    ) -> Callable[..., Any]:
        """Compile a function using trace-based JIT.

        Args:
            func: Function to compile
            sample_input: Optional sample input for eager compilation
            force_trace: Whether to force tracing on every call
            recursive: Whether to JIT compile nested calls
            cache: JIT cache to use
            preserve_stochasticity: When True, always executes the original function
                to maintain stochastic behavior (important for LLMs)
            **options: Additional options

        Returns:
            Compiled function
        """
        from ember.xcs.graph.xcs_graph import XCSGraph
        from ember.xcs.tracer.xcs_tracing import TracerContext

        # Get or create a cache
        cache = self._get_cache(cache)

        @functools.wraps(func)
        def traced_function(*, inputs: Dict[str, Any]) -> Any:
            # Check if JIT is disabled, forcing a trace, or preserving stochasticity
            if (getattr(traced_function, "_jit_disabled", False) or
                    force_trace or preserve_stochasticity):
                # Execute directly without JIT
                # This is critical for LLMs where we want unique outputs each time
                execution_start = time.time()
                result = func(inputs=inputs)
                execution_duration = time.time() - execution_start
                cache.metrics.record_execution(execution_duration)
                return result

            # Try to get cached graph
            graph = cache.get(func)

            if graph is not None:
                # Execute cached graph
                execution_start = time.time()
                try:
                    from ember.xcs.engine import execute_graph

                    results = execute_graph(graph, inputs)
                    execution_end = time.time()
                    execution_duration = execution_end - execution_start
                    cache.metrics.record_execution(execution_duration)

                    # Find the output node (last node in the graph)
                    output_node_id = graph.get_output_nodes()[0]
                    return results[output_node_id]
                except Exception as e:
                    logger.warning(
                        f"Error executing cached graph: {e}. Falling back to direct execution."
                    )

            # Trace execution
            tracing_start = time.time()
            graph = XCSGraph()

            with TracerContext() as tracer:
                # Enable recursive tracing if requested
                tracer.enable_recursion = recursive

                # Execute and record trace
                execution_start = time.time()
                result = func(inputs=inputs)
                execution_end = time.time()
                execution_duration = execution_end - execution_start
                cache.metrics.record_execution(execution_duration)

                # Build graph from trace if trace was recorded
                if tracer.records:
                    compilation_start = time.time()

                    # Add all nodes first
                    for i, record in enumerate(tracer.records):
                        # Create a deterministic function that replays the recorded output
                        def create_replay_function(rec):
                            def replay_func(**_):
                                # Simply replay the recorded outputs for this operation
                                return rec.outputs

                            return replay_func

                        # Add node with proper metadata for diagnostics and optimization
                        node_metadata = {
                            "original_operator": record.operator_name,
                            "instance_id": record.instance_id,
                            "execution_time": record.duration,
                            "timestamp": record.timestamp,
                        }

                        # Use the original node_id for traceability
                        node_id = record.node_id or f"traced_node_{i}"

                        # Add the node to the graph
                        graph.add_node(
                            operator=create_replay_function(record),
                            node_id=node_id,
                            name=record.operator_name,
                            metadata=node_metadata,
                        )

                    # Add dependencies between nodes
                    for i in range(1, len(tracer.records)):
                        prev_record = tracer.records[i - 1]
                        curr_record = tracer.records[i]
                        graph.add_edge(prev_record.node_id, curr_record.node_id)

                    compilation_end = time.time()
                    compilation_duration = compilation_end - compilation_start
                    cache.metrics.record_compilation(compilation_duration)

                    # Cache the graph
                    cache.set(func, graph)

            # Record tracing time
            tracing_end = time.time()
            tracing_duration = tracing_end - tracing_start
            cache.metrics.record_tracing(tracing_duration)

            return result

        # Add control methods
        self._add_control_methods(traced_function, func, cache)

        return traced_function

</code>

src\ember\xcs\jit\strategies\_\_init\_\_.py:
<code>
"""JIT compilation strategies.

Provides various strategies for JIT compilation, including trace-based,
structural, and enhanced approaches. Each strategy has strengths for
different operator patterns.
"""

from ember.xcs.jit.strategies.base_strategy import JITFallbackMixin, Strategy
from ember.xcs.jit.strategies.enhanced import EnhancedStrategy
from ember.xcs.jit.strategies.structural import StructuralStrategy
from ember.xcs.jit.strategies.trace import TraceStrategy

**all** = [
# Base protocol and utilities
"Strategy",
"JITFallbackMixin",
# Concrete strategy implementations
"TraceStrategy",
"StructuralStrategy",
"EnhancedStrategy",
]

</code>

src\ember\xcs\jit\cache.py:
<code>
"""JIT compilation caching system.

Manages compiled graphs and execution metrics for Just-In-Time compilation,
supporting invalidation, metrics collection, and state signature handling.
"""

import logging
import threading
import time
from typing import Any, Callable, Dict, Optional

logger = logging.getLogger(**name**)

# Global JIT cache instance for default use

\_GLOBAL_CACHE = None
\_CACHE_LOCK = threading.Lock()

class JITMetrics:
"""Metrics for JIT compilation and execution performance.

    Tracks compilation times, execution times, cache hits/misses, and other
    metrics to help with performance analysis and optimization.
    """

    def __init__(self) -> None:
        """Initialize empty metrics."""
        # Time metrics in seconds
        self.total_tracing_time = 0.0
        self.total_compilation_time = 0.0
        self.total_execution_time = 0.0

        # Operation counts
        self.cache_hits = 0
        self.cache_misses = 0
        self.compilation_count = 0
        self.execution_count = 0

        # Function-specific metrics
        self.function_metrics: Dict[int, Dict[str, Any]] = {}

    def record_tracing(
        self, duration: float, function_id: Optional[int] = None
    ) -> None:
        """Record a tracing operation's duration.

        Args:
            duration: Duration in seconds
            function_id: Optional ID for function-specific metrics
        """
        self.total_tracing_time += duration

        if function_id is not None:
            if function_id not in self.function_metrics:
                self.function_metrics[function_id] = {
                    "tracing_time": 0.0,
                    "compilation_time": 0.0,
                    "execution_time": 0.0,
                    "cache_hits": 0,
                    "cache_misses": 0,
                    "compilation_count": 0,
                    "execution_count": 0,
                }
            self.function_metrics[function_id]["tracing_time"] += duration

    def record_compilation(
        self, duration: float, function_id: Optional[int] = None
    ) -> None:
        """Record a compilation operation's duration.

        Args:
            duration: Duration in seconds
            function_id: Optional ID for function-specific metrics
        """
        self.total_compilation_time += duration
        self.compilation_count += 1

        if function_id is not None:
            if function_id not in self.function_metrics:
                self.function_metrics[function_id] = {
                    "tracing_time": 0.0,
                    "compilation_time": 0.0,
                    "execution_time": 0.0,
                    "cache_hits": 0,
                    "cache_misses": 0,
                    "compilation_count": 0,
                    "execution_count": 0,
                }
            self.function_metrics[function_id]["compilation_time"] += duration
            self.function_metrics[function_id]["compilation_count"] += 1

    def record_execution(
        self, duration: float, function_id: Optional[int] = None
    ) -> None:
        """Record an execution operation's duration.

        Args:
            duration: Duration in seconds
            function_id: Optional ID for function-specific metrics
        """
        self.total_execution_time += duration
        self.execution_count += 1

        if function_id is not None:
            if function_id not in self.function_metrics:
                self.function_metrics[function_id] = {
                    "tracing_time": 0.0,
                    "compilation_time": 0.0,
                    "execution_time": 0.0,
                    "cache_hits": 0,
                    "cache_misses": 0,
                    "compilation_count": 0,
                    "execution_count": 0,
                }
            self.function_metrics[function_id]["execution_time"] += duration
            self.function_metrics[function_id]["execution_count"] += 1

    def record_cache_hit(self, function_id: Optional[int] = None) -> None:
        """Record a cache hit.

        Args:
            function_id: Optional ID for function-specific metrics
        """
        self.cache_hits += 1
        if function_id is not None:
            if function_id not in self.function_metrics:
                self.function_metrics[function_id] = {
                    "tracing_time": 0.0,
                    "compilation_time": 0.0,
                    "execution_time": 0.0,
                    "cache_hits": 0,
                    "cache_misses": 0,
                    "compilation_count": 0,
                    "execution_count": 0,
                }
            self.function_metrics[function_id]["cache_hits"] += 1

    def record_cache_miss(self, function_id: Optional[int] = None) -> None:
        """Record a cache miss.

        Args:
            function_id: Optional ID for function-specific metrics
        """
        self.cache_misses += 1
        if function_id is not None:
            if function_id not in self.function_metrics:
                self.function_metrics[function_id] = {
                    "tracing_time": 0.0,
                    "compilation_time": 0.0,
                    "execution_time": 0.0,
                    "cache_hits": 0,
                    "cache_misses": 0,
                    "compilation_count": 0,
                    "execution_count": 0,
                }
            self.function_metrics[function_id]["cache_misses"] += 1

    def get_function_metrics(self, function_id: int) -> Dict[str, Any]:
        """Get metrics for a specific function.

        Args:
            function_id: ID of the function

        Returns:
            Dictionary of metrics or empty dict if not tracked
        """
        return self.function_metrics.get(function_id, {})

    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of all metrics.

        Returns:
            Dictionary with metric summaries
        """
        # Calculate derived metrics
        hit_rate = 0.0
        if (self.cache_hits + self.cache_misses) > 0:
            hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses)

        avg_compilation = 0.0
        if self.compilation_count > 0:
            avg_compilation = self.total_compilation_time / self.compilation_count

        avg_execution = 0.0
        if self.execution_count > 0:
            avg_execution = self.total_execution_time / self.execution_count

        return {
            "total_tracing_time_ms": self.total_tracing_time * 1000,
            "total_compilation_time_ms": self.total_compilation_time * 1000,
            "total_execution_time_ms": self.total_execution_time * 1000,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_rate": hit_rate,
            "compilation_count": self.compilation_count,
            "execution_count": self.execution_count,
            "avg_compilation_time_ms": avg_compilation * 1000,
            "avg_execution_time_ms": avg_execution * 1000,
            "function_count": len(self.function_metrics),
        }

class JITCache:
"""Cache for JIT-compiled execution graphs.

    Provides caching for compiled graphs with support for invalidation,
    operator state tracking, and metrics collection.
    """

    def __init__(self) -> None:
        """Initialize an empty cache."""
        self._cache: Dict[int, Dict[str, Any]] = {}
        self._lock = threading.Lock()
        self.metrics = JITMetrics()
        # Registry mapping compiled operators to their functions
        # This enables metrics lookups from operator instances
        self._operator_registry: Dict[int, int] = {}

    def get(self, key: Callable) -> Optional[Any]:
        """Get a cached graph for a function.

        Args:
            key: Function to look up

        Returns:
            Cached graph or None if not found
        """
        key_id = id(key)
        with self._lock:
            if key_id in self._cache and "graph" in self._cache[key_id]:
                self.metrics.record_cache_hit(key_id)
                return self._cache[key_id]["graph"]

        self.metrics.record_cache_miss(key_id)
        return None

    def set(self, key: Callable, value: Any) -> None:
        """Cache a graph for a function.

        Args:
            key: Function as cache key
            value: Graph to cache
        """
        key_id = id(key)
        with self._lock:
            if key_id not in self._cache:
                self._cache[key_id] = {}
            self._cache[key_id]["graph"] = value
            self._cache[key_id]["timestamp"] = time.time()

    def get_with_state(
        self, key: Callable, state_signature: Optional[str] = None
    ) -> Optional[Any]:
        """Get a cached graph that matches both function and state.

        Args:
            key: Function to look up
            state_signature: Optional state signature for state-dependent caching

        Returns:
            Cached graph or None if not found/matched
        """
        key_id = id(key)
        with self._lock:
            if key_id in self._cache and "graphs_by_state" in self._cache[key_id]:
                if state_signature in self._cache[key_id]["graphs_by_state"]:
                    self.metrics.record_cache_hit(key_id)
                    return self._cache[key_id]["graphs_by_state"][state_signature]

        self.metrics.record_cache_miss(key_id)
        return None

    def set_with_state(
        self, key: Callable, value: Any, state_signature: Optional[str] = None
    ) -> None:
        """Cache a graph for a function with associated state signature.

        Args:
            key: Function as cache key
            value: Graph to cache
            state_signature: Optional state signature for state-dependent caching
        """
        key_id = id(key)
        if state_signature is None:
            state_signature = "default"

        with self._lock:
            if key_id not in self._cache:
                self._cache[key_id] = {"graphs_by_state": {}}
            if "graphs_by_state" not in self._cache[key_id]:
                self._cache[key_id]["graphs_by_state"] = {}

            self._cache[key_id]["graphs_by_state"][state_signature] = value
            self._cache[key_id]["timestamp"] = time.time()

    def invalidate(self, key: Callable) -> None:
        """Invalidate all cached graphs for a function.

        Args:
            key: Function to invalidate
        """
        key_id = id(key)
        with self._lock:
            # Remove from main cache
            if key_id in self._cache:
                del self._cache[key_id]

            # If this is an operator, also clean up the registry entry
            if key_id in self._operator_registry:
                del self._operator_registry[key_id]

    def invalidate_all(self) -> None:
        """Invalidate the entire cache."""
        with self._lock:
            self._cache.clear()
            self._operator_registry.clear()

    def get_metrics(self, func: Optional[Callable] = None) -> Dict[str, Any]:
        """Get metrics for a function or overall cache metrics.

        Args:
            func: Optional function to get metrics for. For JIT-decorated operators,
                 automatically retrieves metrics from the internal compiled function.

        Returns:
            Dictionary with metric information
        """
        if func is None:
            return self.metrics.get_summary()

        func_id = id(func)

        # Check if this is a registered operator instance
        if func_id in self._operator_registry:
            # Use the registered compiled function ID
            compiled_func_id = self._operator_registry[func_id]
            metrics = self.metrics.get_function_metrics(compiled_func_id)

            # Add strategy information if available
            if hasattr(func, '_jit_strategy') and 'strategy' not in metrics:
                metrics['strategy'] = func._jit_strategy

            return metrics

        # For JIT-decorated operator instances (older ones without registration)
        if hasattr(func, '_compiled_func'):
            metrics = self.metrics.get_function_metrics(id(func._compiled_func))

            # Add strategy information if available
            if hasattr(func, '_jit_strategy') and 'strategy' not in metrics:
                metrics['strategy'] = func._jit_strategy

            return metrics

        # Standard lookup by function ID
        return self.metrics.get_function_metrics(func_id)

    def __len__(self) -> int:
        """Get number of cached functions.

        Returns:
            Number of cached functions
        """
        with self._lock:
            return len(self._cache)

def get_cache() -> JITCache:
"""Get the global JIT cache instance.

    Returns:
        Global cache instance
    """
    global _GLOBAL_CACHE
    with _CACHE_LOCK:
        if _GLOBAL_CACHE is None:
            _GLOBAL_CACHE = JITCache()
        return _GLOBAL_CACHE

def set_cache(cache: JITCache) -> None:
"""Set the global JIT cache instance.

    Args:
        cache: Cache instance to use globally
    """
    global _GLOBAL_CACHE
    with _CACHE_LOCK:
        _GLOBAL_CACHE = cache

</code>

src\ember\xcs\jit\core.py:
<code>
"""Core JIT compilation system for XCS.

Provides a unified Just-In-Time compilation mechanism that optimizes operators
and functions by analyzing their structure and execution patterns. The system
selects the most appropriate compilation strategy based on target characteristics.
"""

import inspect
import logging
from typing import Any, Callable, Dict, Optional, Type, TypeVar, Union

from ember.xcs.jit.cache import JITCache, get_cache
from ember.xcs.jit.modes import JITMode
from ember.xcs.jit.strategies import Strategy

F = TypeVar("F", bound=Callable)
logger = logging.getLogger(**name**)

class JITSettings:
"""Settings for JIT compilation behavior.

    Encapsulates all configuration options for the JIT system.
    """

    def __init__(
        self,
        mode: Union[JITMode, str] = JITMode.AUTO,
        force_trace: bool = False,
        sample_input: Optional[Dict[str, Any]] = None,
        custom_cache: Optional[JITCache] = None,
        recursive: bool = True,
        preserve_stochasticity: bool = False,
        **kwargs: Any,
    ) -> None:
        """Initialize JIT settings.

        Args:
            mode: JIT compilation mode to use
            force_trace: Whether to force retracing
            sample_input: Optional sample input for eager compilation
            custom_cache: Custom cache instance
            recursive: Whether to apply JIT recursively
            preserve_stochasticity: Whether to preserve stochastic behavior by
                always executing the original function instead of using cached results
            **kwargs: Additional strategy-specific options
        """
        # Normalize mode to enum
        if isinstance(mode, str):
            try:
                self.mode = JITMode(mode.lower())
            except ValueError:
                logger.warning(f"Unknown JIT mode '{mode}', falling back to AUTO")
                self.mode = JITMode.AUTO
        else:
            self.mode = mode

        self.force_trace = force_trace
        self.sample_input = sample_input
        self.custom_cache = custom_cache
        self.recursive = recursive
        self.preserve_stochasticity = preserve_stochasticity
        self.options = kwargs

class StrategySelector:
"""Selects the optimal JIT strategy for target functions.

    Implements heuristic policy for determining the most appropriate
    compilation strategy based on function or class characteristics.
    """

    def __init__(self) -> None:
        """Initializes strategy registry with implementations."""
        # Import strategies lazily to avoid circular dependencies
        from ember.xcs.jit.strategies.enhanced import EnhancedStrategy
        from ember.xcs.jit.strategies.structural import StructuralStrategy
        from ember.xcs.jit.strategies.trace import TraceStrategy

        # Map modes to strategy implementations
        self._strategies: Dict[JITMode, Strategy] = {
            JITMode.TRACE: TraceStrategy(),
            JITMode.STRUCTURAL: StructuralStrategy(),
            JITMode.ENHANCED: EnhancedStrategy(),
        }

    def select_strategy(
        self, func: Callable[..., Any], mode: JITMode = JITMode.AUTO
    ) -> Strategy:
        """Selects optimal strategy for the target function or class.

        Args:
            func: Target function or class to optimize
            mode: User-specified mode or AUTO for automatic selection

        Returns:
            Most appropriate strategy implementation
        """
        # Use explicit strategy when specified
        if mode != JITMode.AUTO:
            return self._strategies[mode]

        # Collect and score strategies for auto-selection
        analyses = [
            (mode, strategy, strategy.analyze(func))
            for mode, strategy in self._strategies.items()
        ]

        # Sort by score in descending order
        analyses.sort(key=lambda x: x[2].get("score", 0), reverse=True)

        # Log selection rationale for debugging
        for mode, _, analysis in analyses:
            logger.debug(
                f"Strategy {mode.value}: score={analysis.get('score', 0)}, "
                f"reason={analysis.get('rationale', 'No rationale provided')}"
            )

        # Return highest-scoring strategy
        return analyses[0][1]

# Global strategy selector

\_selector = StrategySelector()

def \_jit_function(
func: Callable[..., Any], strategy: Strategy, settings: JITSettings
) -> Callable[..., Any]:
"""Compiles a regular function using the chosen JIT strategy.

    Args:
        func: Function to compile
        strategy: Selected compilation strategy
        settings: JIT configuration settings

    Returns:
        Compiled function
    """
    return strategy.compile(
        func,
        sample_input=settings.sample_input,
        force_trace=settings.force_trace,
        recursive=settings.recursive,
        cache=settings.custom_cache or get_cache(),
        **settings.options,
    )

def \_create_operator_forward_proxy(strategy: Strategy, settings: JITSettings):
"""Creates a specialized proxy for operator's forward method.

    Instead of compiling the forward method directly, this creates a proxy function
    that correctly maintains the instance context when called.

    Args:
        strategy: JIT strategy to use
        settings: JIT configuration settings

    Returns:
        Function that creates a callable forward proxy for an operator instance
    """

    def create_forward_proxy(instance, forward_method):
        """Creates a bound method proxy that preserves instance context.

        Args:
            instance: Operator instance
            forward_method: The forward method to proxy

        Returns:
            Callable that acts like the operator's forward method
        """

        # Create a closure to capture the instance and method
        def forward_proxy(*, inputs):
            """Execute the forward method with tracing support.

            This proxy preserves the call interface of the original forward method
            while adding tracing transparency and JIT optimizations.
            """
            # Import here to avoid circular dependencies
            from ember.xcs.tracer.xcs_tracing import TracerContext

            # Check if we're in a tracing context
            tracer = TracerContext.get_current()

            # If we're in a tracing context, track the call
            if tracer and tracer.is_active:
                # Use the instance's name if available, otherwise use class name
                operator_name = getattr(instance, "name", instance.__class__.__name__)
                # Track the call with proper operator identity
                call_id = tracer.track_call(instance, inputs)

                try:
                    # Directly invoke the instance's forward method
                    result = forward_method(instance, inputs=inputs)
                    # Record the successful call completion
                    tracer.complete_call(call_id, result)
                    return result
                except Exception as e:
                    # Record the exception for observability
                    tracer.complete_call(call_id, {}, e)
                    # Re-raise the original exception to maintain behavior
                    raise
            else:
                # No tracing context - direct execution path
                return forward_method(instance, inputs=inputs)

        return forward_proxy

    return create_forward_proxy

def \_jit_operator_class(cls: Type, strategy: Strategy, settings: JITSettings) -> Type:
"""Creates a JIT-optimized version of an operator class.

    This function isolates the operator-specific JIT logic, creating a new
    class that inherits from the original but with optimized execution.

    Args:
        cls: Operator class to optimize
        strategy: Selected compilation strategy
        settings: JIT configuration settings

    Returns:
        JIT-optimized operator class
    """
    class_name = cls.__name__ + "_JIT"
    strategy_name = strategy.__class__.__name__.replace("Strategy", "")

    # Verify the class has a forward method
    if not hasattr(cls, "forward"):
        raise ValueError(f"Operator class {cls.__name__} must have a forward method")

    # Get the forward method - we'll use it directly in call
    original_forward = cls.forward

    # Create a forward proxy factory - used to handle binding self properly
    create_proxy = _create_operator_forward_proxy(strategy, settings)

    # We compile the full operation inside the __call__ method, not just forward
    def jit_init(self, *args, **kwargs):
        # Filter out 'inputs' parameter which belongs to __call__, not __init__
        init_kwargs = {k: v for k, v in kwargs.items() if k != "inputs"}
        # Initialize the class normally
        cls.__init__(self, *args, **init_kwargs)
        # Create a proxy and compile it - this happens per instance
        self._forward_proxy = create_proxy(self, original_forward)

        # Get cache reference - needed for both compilation and registration
        cache = settings.custom_cache or get_cache()

        # Compile the function
        self._compiled_func = strategy.compile(
            self._forward_proxy,
            sample_input=settings.sample_input,
            force_trace=settings.force_trace,
            recursive=settings.recursive,
            cache=cache,
            **settings.options,
        )

        # Store the strategy name for metrics reporting
        self._jit_strategy = strategy_name

        # Register this operator instance with the cache
        # This enables metrics lookups from operator to compiled_func
        cache._operator_registry[id(self)] = id(self._compiled_func)

    def jit_call(self, **kwargs):
        # Get required inputs - everything else is passed to the compiled function as-is
        inputs = kwargs.get("inputs", {})

        # Import here to avoid circular dependencies
        from ember.xcs.tracer.xcs_tracing import TracerContext

        # Get current tracing context to properly propagate tracing through call chain
        tracer = TracerContext.get_current()

        # Get the cache for recording metrics
        from ember.xcs.jit.cache import get_cache
        cache = get_cache()
        func_id = id(self._compiled_func)

        # If we're in a trace context, add operator information before execution
        if tracer and tracer.is_active:
            # Track the operator call in the trace context
            call_id = tracer.track_call(self, inputs)

            try:
                # Execute using compiled function
                result = self._compiled_func(inputs=inputs)

                # Record successful execution in metrics
                cache.metrics.record_cache_hit(func_id)

                # Complete the trace record with successful execution
                tracer.complete_call(call_id, result)
                return result
            except Exception as e:
                # Record the exception but don't swallow it
                tracer.complete_call(call_id, {}, e)
                raise
        else:
            # Not tracing - direct execution path
            # Record cache hit when reusing compiled function
            cache.metrics.record_cache_hit(func_id)
            return self._compiled_func(inputs=inputs)

    # Create the JIT-optimized class
    return type(
        class_name,
        (cls,),
        {
            "__init__": jit_init,
            "__call__": jit_call,
            "__doc__": cls.__doc__,
        },
    )

def jit(
func: Optional[Callable[..., Any]] = None,
\*,
mode: Union[str, JITMode] = JITMode.AUTO,
force_trace: bool = False,
sample_input: Optional[Dict[str, Any]] = None,
cache: Optional[JITCache] = None,
recursive: bool = True,
preserve_stochasticity: bool = False,
\*\*kwargs: Any,
) -> Any:
"""Optimizes functions and operators with Just-In-Time compilation.

    Core optimization decorator that analyzes and compiles functions or
    operator classes for efficient execution. Supports multiple compilation
    strategies with automatic selection based on target characteristics.

    Args:
        func: Target function or operator class
        mode: Compilation strategy to use (auto, trace, structural, enhanced)
        force_trace: Whether to force retracing on each call
        sample_input: Example input for eager compilation
        cache: Custom cache implementation
        recursive: Whether to recursively optimize nested functions
        preserve_stochasticity: If True, always executes the original function even
            when inputs match previous calls. This is important for LLMs where
            multiple calls with the same prompts should produce different outputs.
        **kwargs: Strategy-specific configuration options

    Returns:
        Optimized function or operator class

    Example:
        ```python
        # Simple usage
        @jit
        class MyOperator(Operator):
            def forward(self, *, inputs):
                return process(inputs)

        # Advanced configuration
        @jit(
            mode="structural",
            sample_input={"data": "example"},
            recursive=False
        )
        def process_data(*, inputs):
            return {"result": complex_calculation(inputs["data"])}

        # LLM usage, preserving stochasticity
        @jit(preserve_stochasticity=True)
        class LLMOperator(Operator):
            def forward(self, *, inputs):
                # Each call will execute fresh, even with identical inputs
                return self.llm.generate(inputs["prompt"])
        ```
    """
    # Support both @jit and @jit() decorator styles
    if func is None:
        return lambda f: jit(
            f,
            mode=mode,
            force_trace=force_trace,
            sample_input=sample_input,
            cache=cache,
            recursive=recursive,
            preserve_stochasticity=preserve_stochasticity,
            **kwargs,
        )

    # Prepare optimization configuration
    settings = JITSettings(
        mode=mode,
        force_trace=force_trace,
        sample_input=sample_input,
        custom_cache=cache,
        recursive=recursive,
        preserve_stochasticity=preserve_stochasticity,
        **kwargs,
    )

    # Get optimal compilation strategy
    strategy = _selector.select_strategy(func, settings.mode)

    # Apply appropriate optimization based on target type
    if inspect.isclass(func) and hasattr(func, "forward"):
        return _jit_operator_class(func, strategy, settings)
    return _jit_function(func, strategy, settings)

def get_jit_stats(func: Optional[Callable[..., Any]] = None) -> Dict[str, Any]:
"""Get statistics about JIT compilation and execution.

    Args:
        func: Optional function to get stats for. If None, returns overall stats.
            For JIT-decorated operator classes, automatically retrieves metrics
            from the internal compiled function.

    Returns:
        Dictionary with compilation and execution statistics
    """
    cache = get_cache()

    # Handle JIT-decorated operator class instances
    if func is not None and hasattr(func, '_compiled_func'):
        return cache.get_metrics(func._compiled_func)

    return cache.get_metrics(func)

def explain_jit_selection(func: Callable[..., Any]) -> Dict[str, Any]:
"""Explain why a particular JIT strategy would be selected.

    Useful for understanding and debugging the auto-selection process.

    Args:
        func: Function to analyze

    Returns:
        Dictionary with detailed analysis from each strategy
    """
    from ember.xcs.jit.strategies.enhanced import EnhancedStrategy
    from ember.xcs.jit.strategies.structural import StructuralStrategy
    from ember.xcs.jit.strategies.trace import TraceStrategy

    strategies = {
        "trace": TraceStrategy(),
        "structural": StructuralStrategy(),
        "enhanced": EnhancedStrategy(),
    }

    results = {}
    for name, strategy in strategies.items():
        results[name] = strategy.analyze(func)

    return results

</code>

src\ember\xcs\jit\execution_utils.py:
<code>
"""Execution utilities for JIT-compiled graphs.

Provides utilities for executing compiled graphs with various strategies.
This module bridges between the JIT compilation system and the engine's execution
capabilities, handling input/output conversion, error handling, and metrics.
"""

import logging
import time
from typing import Any, Callable, Dict, Optional

from ember.xcs.engine.unified_engine import ExecutionOptions, execute_graph
from ember.xcs.jit.cache import JITCache

logger = logging.getLogger(**name**)

def execute_compiled_graph(
graph: Any,
inputs: Dict[str, Any],
cache: JITCache,
func: Optional[Callable] = None,
options: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
"""Execute a compiled graph with provided inputs.

    Args:
        graph: Compiled graph to execute
        inputs: Input values to the graph
        cache: JIT cache for metrics tracking
        options: Optional execution options

    Returns:
        Dictionary with execution results
    """
    # Track execution time for metrics
    execution_start = time.time()

    try:
        # If graph has a specific execution mode attached, use it
        if hasattr(graph, "execution_mode") and hasattr(graph, "execution_options"):
            mode = getattr(graph, "execution_mode", "auto")
            mode_options = getattr(graph, "execution_options", {})

            # Create execution options
            exec_options = ExecutionOptions(scheduler_type=mode, **mode_options)
        else:
            # Use default options or provided options
            exec_options = ExecutionOptions(**(options or {}))

        # Execute the graph
        result_dict = execute_graph(graph, inputs, options=exec_options)

        # Determine root node or output node
        root_id = None
        if hasattr(graph, "root_id"):
            root_id = graph.root_id
        elif hasattr(graph, "_output_node_id"):
            root_id = graph._output_node_id
        elif hasattr(graph, "metadata") and "root_id" in graph.metadata:
            root_id = graph.metadata["root_id"]
        elif hasattr(graph, "metadata") and "output_node_id" in graph.metadata:
            root_id = graph.metadata["output_node_id"]

        # Get result from appropriate node
        if root_id and root_id in result_dict:
            result = result_dict.get(root_id, {})
        else:
            # If no specific output node, return all results
            result = result_dict
            # If the result is empty, call the original function directly
            if not result and func is not None:
                return func(inputs=inputs)

        # Ensure proper boundary crossing for outputs
        # If the original function has a specification, validate output
        if (
            func is not None
            and hasattr(func, "specification")
            and hasattr(func.specification, "validate_output")
        ):
            try:
                result = func.specification.validate_output(output=result)
            except Exception as e:
                import logging

                logging.warning(
                    f"Output validation failed in execute_compiled_graph: {e}"
                )

        # Record execution time
        execution_duration = time.time() - execution_start
        if func is not None:
            func_id = id(func)
            cache.metrics.record_execution(execution_duration, func_id)
        else:
            cache.metrics.record_execution(execution_duration)

        return result
    except Exception as e:
        # Log error and record execution time (failure case)
        execution_duration = time.time() - execution_start
        if func is not None:
            func_id = id(func)
            cache.metrics.record_execution(execution_duration, func_id)
        else:
            cache.metrics.record_execution(execution_duration)

        # Propagate exception with context
        logger.error(f"Error executing JIT graph: {e}")
        raise

</code>

src\ember\xcs\jit\modes.py:
<code>
"""JIT compilation modes.

Defines the set of available JIT compilation modes used throughout the system.
This module is separate to avoid circular imports between jit modules.
"""

from enum import Enum

class JITMode(str, Enum):
"""JIT compilation modes available in the system."""

    AUTO = "auto"  # Automatically select the best strategy
    TRACE = "trace"  # Traditional execution tracing
    STRUCTURAL = "structural"  # Structure-based analysis
    ENHANCED = "enhanced"  # Enhanced JIT with improved parallelism detection

</code>

src\ember\xcs\jit\_\_init\_\_.py:
<code>
"""Unified JIT compilation system for XCS.

Provides Just-In-Time compilation strategies for optimizing operator execution
through tracing, structural analysis, and enhanced dependency tracking.

The JIT system enables automatic caching of computation results, tracking of
performance metrics, and selection of optimal compilation strategies based on
operator characteristics. These features combine to create efficient execution
pipelines with minimal user configuration.
"""

from typing import Any, Callable, Dict, Optional

# JIT caching system

from ember.xcs.jit.cache import JITCache, get_cache

# Core JIT decorator - these need to be imported after JITMode to avoid circular imports

from ember.xcs.jit.core import explain_jit_selection, get_jit_stats, jit

# Import JIT modes

from ember.xcs.jit.modes import JITMode

**all** = [
# Core JIT functionality
"jit",
"JITMode",
"JITCache",
"get_jit_stats",
"explain_jit_selection",
"get_cache",
]

</code>

src\ember\xcs\schedulers\base_scheduler.py:
<code>
"""
Base scheduler protocol and implementation for XCS.

Defines the core interface for graph execution schedulers and provides base
implementations that other schedulers can extend.
"""

import logging
from typing import Any, Dict, List, Optional, Protocol

from ember.xcs.common.plans import XCSPlan
from ember.xcs.graph.xcs_graph import XCSGraph

class BaseScheduler(Protocol):
"""Protocol defining the interface for graph execution schedulers.

    All schedulers must implement this protocol to ensure consistent behavior
    and interoperability with the execution engine.
    """

    def prepare(self, graph: XCSGraph) -> None:
        """Prepare the scheduler for execution.

        This method is called before execution begins to initialize the
        scheduler with the graph structure.

        Args:
            graph: The graph to prepare for execution
        """
        ...

    def execute(
        self, graph: XCSGraph, inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute a graph with the given inputs.

        This is the main method for running a graph through the scheduler.

        Args:
            graph: The graph to execute
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """
        ...

    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from an incomplete execution.

        This is used when execution is stopped prematurely (e.g., due to
        an error or timeout) to retrieve partial results.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        ...

class UnifiedSchedulerBase:
"""Base implementation of scheduler with unified execution capabilities.

    This class serves as a foundation for all scheduler implementations,
    providing common execution capabilities through the execution coordinator.
    It enables schedulers to use both thread pool and async execution engines
    depending on the workload characteristics.

    This is separate from the BaseScheduler protocol to maintain backward
    compatibility with existing scheduler implementations while offering
    enhanced execution capabilities for new scheduler implementations.
    """

    def __init__(
        self,
        max_workers: Optional[int] = None,
        execution_engine: str = "auto",
        timeout_seconds: Optional[float] = None,
        error_handling: str = "fail_fast",
    ) -> None:
        """Initialize scheduler with execution parameters.

        Args:
            max_workers: Maximum concurrent operations
            execution_engine: Engine selection - one of:
                "auto" - automatically select based on workload
                "async" - use async/await for IO-bound operations
                "threaded" - use thread pool for CPU-bound operations
            timeout_seconds: Maximum execution time in seconds
            error_handling: Error handling strategy - one of:
                "fail_fast" - stop execution on first error
                "continue" - continue execution after errors when possible
        """
        self.max_workers = max_workers
        self.execution_engine = execution_engine
        self.timeout_seconds = timeout_seconds
        self.error_handling = error_handling
        self.logger = logging.getLogger(__name__)
        self._results: Dict[str, Dict[str, Any]] = {}

    def _get_execution_order(self, graph: XCSGraph) -> List[List[str]]:
        """Get the execution order for nodes in the graph.

        Must be implemented by concrete scheduler classes.

        Args:
            graph: Graph to analyze

        Returns:
            List of waves, where each wave is a list of node IDs
        """
        raise NotImplementedError("Subclasses must implement _get_execution_order")

    def _prepare_node_input(
        self,
        node_id: str,
        node: Any,
        results: Dict[str, Dict[str, Any]],
        inputs: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Prepare inputs for a node based on graph connections.

        Args:
            node_id: ID of node being prepared
            node: Node being prepared
            results: Results collected so far
            inputs: Original graph inputs

        Returns:
            Properly prepared inputs for the node
        """
        # Default implementation uses graph's prepare_node_inputs
        # Can be overridden by subclasses for custom behavior
        node_inputs = {}

        # If this is a source node with no dependencies, use graph inputs
        if not node.inbound_edges:
            node_inputs.update(inputs)
            return node_inputs

        # Otherwise, collect results from dependencies
        for source_id in node.inbound_edges:
            if source_id in results:
                # For now, just shallow merge results
                # More sophisticated implementations can use field_mappings
                node_inputs.update(results[source_id])

        return node_inputs

    def run_plan(
        self, *, plan: XCSPlan, inputs: Dict[str, Any], graph: XCSGraph
    ) -> Dict[str, Any]:
        """Execute plan with adaptive execution engine.

        Args:
            plan: Execution plan to run
            inputs: Input values for the plan
            graph: Computation graph

        Returns:
            Results from plan execution
        """
        # Import here to avoid circular imports
        from ember.xcs.utils.executor import ExecutionCoordinator

        # Create execution coordinator with optimal configuration
        coordinator = ExecutionCoordinator(
            max_workers=self.max_workers,
            timeout=self.timeout_seconds,
            error_handling=self.error_handling,
            execution_engine=self.execution_engine,
        )

        try:
            results = {}
            # Process execution waves in order
            for wave in self._get_execution_order(graph):
                # Prepare wave nodes with inputs
                wave_nodes = [
                    {
                        "id": node_id,
                        "operator": graph.nodes[node_id].operator,
                        "inputs": self._prepare_node_input(
                            node_id, graph.nodes[node_id], results, inputs
                        ),
                    }
                    for node_id in wave
                ]

                # Skip empty waves
                if not wave_nodes:
                    continue

                # Execute wave with optimal engine selection
                wave_results = coordinator.map(
                    lambda **kwargs: kwargs["operator"](inputs=kwargs["inputs"]),
                    wave_nodes,
                )

                # Store results by node ID
                for i, node_dict in enumerate(wave_nodes):
                    if i < len(wave_results):
                        node_id = node_dict["id"]
                        results[node_id] = wave_results[i]

            # Store the final results for potential retrieval via get_partial_results
            self._results = results
            return results
        finally:
            coordinator.close()

    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from execution that may have been interrupted.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        return self._results

</code>

src\ember\xcs\schedulers\base_scheduler_impl.py:
<code>
"""Base implementation for scheduler strategies.

Provides abstract base classes and mixin patterns for scheduler implementations,
allowing for composition of ordering and execution strategies.
"""

import abc
import logging
from typing import Any, Dict, List, Optional, Set

from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.schedulers.base_scheduler import BaseScheduler
from ember.xcs.utils.boundary import to_dict, to_ember_model

logger = logging.getLogger(**name**)

class OrderingStrategy(abc.ABC):
"""Strategy for determining execution order of graph nodes."""

    @abc.abstractmethod
    def get_execution_order(self, graph: XCSGraph) -> List[str]:
        """Determine the execution order for graph nodes.

        Args:
            graph: Graph to analyze

        Returns:
            List of node IDs in execution order
        """
        pass

class ExecutionStrategy(abc.ABC):
"""Strategy for executing graph nodes."""

    @abc.abstractmethod
    def execute_nodes(
        self, graph: XCSGraph, execution_order: List[str], inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute graph nodes in given order.

        Args:
            graph: Graph to execute
            execution_order: Node IDs in execution order
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """
        pass

    @abc.abstractmethod
    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from execution.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        pass

class TopologicalOrderingStrategy(OrderingStrategy):
"""Orders nodes topologically, with dependencies before dependents."""

    def get_execution_order(self, graph: XCSGraph) -> List[str]:
        """Get topological execution order for graph nodes.

        Args:
            graph: Graph to analyze

        Returns:
            List of node IDs in topological order
        """
        return graph.topological_sort()

class DepthOrderingStrategy(OrderingStrategy):
"""Orders nodes by depth in the dependency graph."""

    def get_execution_order(self, graph: XCSGraph) -> List[str]:
        """Get depth-based execution order for graph nodes.

        Assigns depths to nodes (distance from root) and orders them
        by increasing depth for layer-by-layer execution.

        Args:
            graph: Graph to analyze

        Returns:
            List of node IDs in depth-first order
        """
        # Compute node depths (distance from inputs)
        depths: Dict[str, int] = {}
        visited: Set[str] = set()

        # Find input nodes (no inbound edges)
        input_nodes = [
            node_id for node_id, node in graph.nodes.items() if not node.inbound_edges
        ]

        # Assign depth 0 to input nodes
        for node_id in input_nodes:
            depths[node_id] = 0

        # BFS to compute depths
        queue = input_nodes.copy()
        while queue:
            node_id = queue.pop(0)
            visited.add(node_id)
            node_depth = depths[node_id]

            # Update depths of outbound nodes
            for outbound_id in graph.nodes[node_id].outbound_edges:
                if outbound_id not in depths or depths[outbound_id] < node_depth + 1:
                    depths[outbound_id] = node_depth + 1

                # Add to queue if all inbound nodes have been visited
                outbound_node = graph.nodes[outbound_id]
                if outbound_id not in visited and all(
                    in_id in visited for in_id in outbound_node.inbound_edges
                ):
                    queue.append(outbound_id)

        # Sort nodes by depth
        return sorted(
            graph.nodes.keys(), key=lambda node_id: depths.get(node_id, float("inf"))
        )

class WaveOrderingStrategy(OrderingStrategy):
"""Orders nodes by execution waves for parallel processing."""

    def get_execution_order(self, graph: XCSGraph) -> List[str]:
        """Get wave-based execution order for graph nodes.

        Groups nodes into execution waves based on dependencies,
        where all nodes in a wave can be executed in parallel.

        Args:
            graph: Graph to analyze

        Returns:
            List of node IDs with wave annotations in metadata
        """
        # Track remaining dependencies for each node
        in_degree = {}
        for node_id, node in graph.nodes.items():
            in_degree[node_id] = len(node.inbound_edges)

        # Group nodes into waves for parallel execution
        waves = []
        wave_map = {}

        # Add nodes to waves based on dependency satisfaction
        while in_degree:
            # Find nodes with no remaining dependencies
            current_wave = [
                node_id for node_id, count in in_degree.items() if count == 0
            ]
            if not current_wave:
                # Cyclic dependency detected
                raise ValueError("Graph contains a cycle")

            # Add current wave to waves list
            waves.append(current_wave)

            # Map nodes to their wave number
            wave_num = len(waves) - 1
            for node_id in current_wave:
                wave_map[node_id] = wave_num

            # Remove processed nodes and update dependencies
            for node_id in current_wave:
                for outbound_id in graph.nodes[node_id].outbound_edges:
                    in_degree[outbound_id] -= 1
                del in_degree[node_id]

        # Add wave metadata to graph for later use in execution
        graph.metadata["waves"] = waves
        graph.metadata["wave_map"] = wave_map

        # Return flattened list while preserving wave information
        return [node_id for wave in waves for node_id in wave]

class SequentialExecutionStrategy(ExecutionStrategy):
"""Executes nodes sequentially in specified order."""

    def __init__(self) -> None:
        """Initialize sequential execution strategy."""
        self._results: Dict[str, Dict[str, Any]] = {}

    def execute_nodes(
        self, graph: XCSGraph, execution_order: List[str], inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute graph nodes sequentially.

        Args:
            graph: Graph to execute
            execution_order: Node IDs in execution order
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """
        self._results = {}  # Clear previous results

        # Initialize shared results with inputs
        shared_results = {}

        # Process nodes in order
        for node_id in execution_order:
            node = graph.nodes[node_id]

            # Prepare node inputs
            node_inputs = graph.prepare_node_inputs(node_id, self._results)
            # Add shared inputs for nodes with no inbound edges
            if not node.inbound_edges:
                node_inputs.update(inputs)

            # Ensure inputs have the right type for the target node

            # Get expected input type from node's operator specification if available
            target_operator = node.operator
            input_model = None
            if hasattr(target_operator, "specification") and hasattr(
                target_operator.specification, "input_model"
            ):
                input_model = target_operator.specification.input_model

            # Execute the operator with boundary crossing
            try:
                # Always cross boundary when entering and exiting operator
                if input_model:
                    # ENTER: Convert dict to model when crossing into operator
                    logger.debug(
                        f"Node {node_id}: Boundary crossing: dict → {input_model.__name__}"
                    )
                    typed_inputs = to_ember_model(node_inputs, input_model)

                    # Execute in operator domain (with proper types)
                    result = node.operator(inputs=typed_inputs)

                    # EXIT: Convert result back to dict when crossing back to execution engine
                    logger.debug(
                        f"Node {node_id}: Boundary crossing: {type(result).__name__} → dict"
                    )
                    self._results[node_id] = to_dict(result)
                else:
                    # No model type specified - simpler execution path
                    result = node.operator(inputs=node_inputs)
                    # Still ensure dict format on return for consistency
                    self._results[node_id] = to_dict(result)
            except TypeError as e:
                # Specific handling for boundary crossing errors
                logger.error(f"Boundary crossing error at node {node_id}: {e}")
                raise RuntimeError(
                    f"Type error at system boundary for node {node_id}: {e}"
                ) from e
            except Exception as e:
                logger.error(f"Error executing node {node_id}: {e}")
                # Store partial successful results
                self._results[node_id] = {"error": str(e)}
                # Propagate error
                raise RuntimeError(f"Error executing node {node_id}: {e}") from e

        return self._results

    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from execution.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        return self._results

class ParallelExecutionStrategy(ExecutionStrategy):
"""Executes nodes in parallel where possible."""

    def __init__(self, max_workers: Optional[int] = None) -> None:
        """Initialize parallel execution strategy.

        Args:
            max_workers: Maximum number of worker threads (None uses CPU count)
        """
        self.max_workers = max_workers
        self._results: Dict[str, Dict[str, Any]] = {}

    def _execute_and_convert(
        self, operator: Any, inputs: Any, boundary_debug_id: str = ""
    ) -> Dict[str, Any]:
        """Execute an operator and convert its result to a dictionary.

        Helper method for boundary crossing in parallel execution.

        Args:
            operator: Operator to execute
            inputs: Properly typed inputs for the operator
            boundary_debug_id: Optional identifier for debugging boundary crossing

        Returns:
            Dictionary representation of the operator result
        """
        # Execute the operator with its expected input types
        result = operator(inputs=inputs)

        # EXIT boundary: Always ensure dictionary format when returning to execution engine
        if not isinstance(result, dict):
            logger.debug(
                f"Boundary crossing EXIT {boundary_debug_id}: {type(result).__name__} → dict"
            )
            return to_dict(result)
        return result

    def execute_nodes(
        self, graph: XCSGraph, execution_order: List[str], inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute graph nodes with parallel processing where possible.

        Args:
            graph: Graph to execute
            execution_order: Node IDs in execution order
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """

        self._results = {}  # Clear previous results

        # Check if wave information is available for parallel execution
        waves = graph.metadata.get("waves")
        if waves:
            # Execute by waves using wave information
            return self._execute_by_waves(graph, waves, inputs)

        # Fallback: determine parallelizable nodes using dependencies
        return self._execute_with_dependencies(graph, execution_order, inputs)

    def _execute_by_waves(
        self, graph: XCSGraph, waves: List[List[str]], inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute nodes in waves, with each wave executed in parallel.

        Args:
            graph: Graph to execute
            waves: List of waves, where each wave is a list of node IDs
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """
        import concurrent.futures
        from concurrent.futures import ThreadPoolExecutor

        # Initialize results
        self._results = {}

        # Calculate appropriate worker count
        workers = self.max_workers
        if workers is None:
            import multiprocessing

            workers = max(1, multiprocessing.cpu_count() - 1)

        # Process each wave in order
        for wave in waves:
            with ThreadPoolExecutor(max_workers=workers) as executor:
                # Prepare futures for this wave
                futures = {}
                for node_id in wave:
                    node = graph.nodes[node_id]
                    # Prepare inputs for this node
                    node_inputs = graph.prepare_node_inputs(node_id, self._results)
                    # Add shared inputs for nodes with no inbound edges
                    if not node.inbound_edges:
                        node_inputs.update(inputs)

                    # Ensure inputs have the right type for the target node

                    # Get expected input type from node's operator specification if available
                    target_operator = node.operator
                    input_model = None
                    if hasattr(target_operator, "specification") and hasattr(
                        target_operator.specification, "input_model"
                    ):
                        input_model = target_operator.specification.input_model

                    # Always apply consistent boundary crossing
                    try:
                        if input_model:
                            # ENTER boundary: Convert dict to model when crossing into operator
                            logger.debug(
                                f"Node {node_id}: Boundary crossing ENTER: dict → {input_model.__name__}"
                            )
                            typed_inputs = to_ember_model(node_inputs, input_model)
                            # Submit the task with properly typed inputs through conversion helper
                            futures[
                                executor.submit(
                                    self._execute_and_convert,
                                    node.operator,
                                    typed_inputs,
                                    f"node={node_id}",
                                )
                            ] = node_id
                        else:
                            # No model type specified - simpler execution path, but still ensure dict return
                            futures[
                                executor.submit(
                                    self._execute_and_convert,
                                    node.operator,
                                    node_inputs,
                                    f"node={node_id}",
                                )
                            ] = node_id
                    except TypeError as e:
                        # Handle conversion error
                        logger.error(f"Boundary crossing error at node {node_id}: {e}")
                        raise RuntimeError(
                            f"Type error at system boundary for node {node_id}: {e}"
                        ) from e

                # Wait for all tasks in this wave to complete
                for future in concurrent.futures.as_completed(futures):
                    node_id = futures[future]
                    try:
                        result = future.result()
                        self._results[node_id] = result
                    except Exception as e:
                        logger.error(f"Error executing node {node_id}: {e}")
                        # Store partial successful results
                        self._results[node_id] = {"error": str(e)}
                        # Propagate error
                        raise RuntimeError(
                            f"Error executing node {node_id}: {e}"
                        ) from e

        return self._results

    def _execute_with_dependencies(
        self, graph: XCSGraph, execution_order: List[str], inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute nodes using dependency tracking for parallelism.

        Args:
            graph: Graph to execute
            execution_order: Node IDs in execution order
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """
        import concurrent.futures
        import threading
        from concurrent.futures import ThreadPoolExecutor

        # Initialize tracking structures
        self._results = {}
        pending_nodes = set(execution_order)
        completed_nodes = set()
        node_lock = threading.Lock()

        # Calculate appropriate worker count
        workers = self.max_workers
        if workers is None:
            import multiprocessing

            workers = max(1, multiprocessing.cpu_count() - 1)

        # Execute nodes with dependency tracking
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit initial nodes (those with no dependencies)
            futures = {}
            initial_nodes = [
                node_id
                for node_id in execution_order
                if not graph.nodes[node_id].inbound_edges
            ]

            for node_id in initial_nodes:
                # Prepare inputs
                node_inputs = graph.prepare_node_inputs(node_id, self._results)
                node_inputs.update(inputs)  # Add shared inputs

                # Submit the task
                futures[
                    executor.submit(
                        self._execute_node,
                        graph,
                        node_id,
                        node_inputs,
                        completed_nodes,
                        pending_nodes,
                        node_lock,
                        executor,
                        futures,
                        inputs,
                    )
                ] = node_id

            # Wait for all tasks to complete
            try:
                concurrent.futures.wait(futures.keys())
            except Exception as e:
                logger.error(f"Error during parallel execution: {e}")
                raise

        return self._results

    def _execute_node(
        self,
        graph: XCSGraph,
        node_id: str,
        node_inputs: Dict[str, Any],
        completed_nodes: Set[str],
        pending_nodes: Set[str],
        node_lock: Any,
        executor: Any,
        futures: Dict[Any, str],
        global_inputs: Dict[str, Any],
    ) -> None:
        """Execute a single node and schedule its dependents.

        Args:
            graph: The graph being executed
            node_id: ID of the node to execute
            node_inputs: Inputs for the node
            completed_nodes: Set of completed node IDs
            pending_nodes: Set of pending node IDs
            node_lock: Lock for synchronizing access to shared state
            executor: ThreadPoolExecutor for submitting new tasks
            futures: Dictionary mapping futures to node IDs
            global_inputs: Global inputs to the graph
        """
        node = graph.nodes[node_id]

        try:
            # Execute the node - node_inputs are already properly typed
            # at this point due to the boundary crossing in the caller
            result = node.operator(inputs=node_inputs)

            # EXIT: Convert result back to dictionary when crossing back to XCS
            logger.debug(
                f"Node {node_id}: Boundary crossing EXIT: {type(result).__name__} → dict"
            )
            dict_result = to_dict(result)

            # Update shared state
            with node_lock:
                self._results[node_id] = dict_result
                completed_nodes.add(node_id)
                pending_nodes.remove(node_id)

                # Check for nodes that can now be executed
                for outbound_id in node.outbound_edges:
                    if outbound_id in pending_nodes:
                        # Check if all dependencies are satisfied
                        outbound_node = graph.nodes[outbound_id]
                        if all(
                            dep_id in completed_nodes
                            for dep_id in outbound_node.inbound_edges
                        ):
                            # Prepare inputs for this node
                            outbound_inputs = graph.prepare_node_inputs(
                                outbound_id, self._results
                            )

                            # Ensure inputs have the right type for the target node

                            # Get expected input type from node's operator specification if available
                            target_operator = outbound_node.operator
                            input_model = None
                            if hasattr(target_operator, "specification") and hasattr(
                                target_operator.specification, "input_model"
                            ):
                                input_model = target_operator.specification.input_model

                            # Always apply consistent boundary crossing at the entry point
                            try:
                                if input_model:
                                    # ENTER boundary: Convert dict to model when crossing into operator
                                    logger.debug(
                                        f"Node {outbound_id}: Boundary crossing ENTER: dict → {input_model.__name__}"
                                    )
                                    typed_inputs = to_ember_model(
                                        outbound_inputs, input_model
                                    )
                                    # Submit the task with properly typed inputs
                                    futures[
                                        executor.submit(
                                            self._execute_node,
                                            graph,
                                            outbound_id,
                                            typed_inputs,
                                            completed_nodes,
                                            pending_nodes,
                                            node_lock,
                                            executor,
                                            futures,
                                            global_inputs,
                                        )
                                    ] = outbound_id
                                else:
                                    # No model specified, use as-is but ensure dict return
                                    futures[
                                        executor.submit(
                                            self._execute_node,
                                            graph,
                                            outbound_id,
                                            outbound_inputs,
                                            completed_nodes,
                                            pending_nodes,
                                            node_lock,
                                            executor,
                                            futures,
                                            global_inputs,
                                        )
                                    ] = outbound_id
                            except TypeError as e:
                                # Handle conversion error
                                logger.error(
                                    f"Boundary crossing error at node {outbound_id}: {e}"
                                )
                                with node_lock:
                                    self._results[outbound_id] = {
                                        "error": f"Type error at system boundary: {e}"
                                    }
                                raise

        except Exception as e:
            logger.error(f"Error executing node {node_id}: {e}")
            # Store error information as a proper dict (will be converted to correct model type by prepare_node_inputs)
            with node_lock:
                self._results[node_id] = {"error": str(e)}
            raise

    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from execution.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        return self._results

class NoopExecutionStrategy(ExecutionStrategy):
"""Non-executing strategy for testing and validation."""

    def __init__(self) -> None:
        """Initialize no-op execution strategy."""
        self._results: Dict[str, Dict[str, Any]] = {}

    def execute_nodes(
        self, graph: XCSGraph, execution_order: List[str], inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Return inputs without executing nodes (for simulation).

        Args:
            graph: Graph to not execute
            execution_order: Node IDs in execution order
            inputs: Input values for the graph

        Returns:
            Dictionary mapping node IDs to placeholder results
        """
        self._results = {}

        # Create placeholder results for each node
        for node_id in execution_order:
            self._results[node_id] = {"_simulated": True}

        return self._results

    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from execution.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        return self._results

class BaseSchedulerImpl(BaseScheduler):
"""Base implementation for all schedulers using strategy pattern.

    Combines ordering and execution strategies to implement the full
    scheduler interface. Concrete schedulers can extend this class and
    provide specific strategy combinations.
    """

    def __init__(
        self, ordering_strategy: OrderingStrategy, execution_strategy: ExecutionStrategy
    ) -> None:
        """Initialize with specific strategies.

        Args:
            ordering_strategy: Strategy for determining execution order
            execution_strategy: Strategy for executing nodes
        """
        self._ordering_strategy = ordering_strategy
        self._execution_strategy = execution_strategy
        self._execution_order: List[str] = []

    def prepare(self, graph: XCSGraph) -> None:
        """Prepare the scheduler for graph execution.

        Args:
            graph: The graph to be executed
        """
        # Determine execution order using ordering strategy
        self._execution_order = self._ordering_strategy.get_execution_order(graph)

    def execute(
        self, graph: XCSGraph, inputs: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Execute all nodes in the graph using execution strategy.

        Args:
            graph: The graph to execute
            inputs: Initial inputs to the graph

        Returns:
            Dictionary mapping node IDs to their output results
        """
        return self._execution_strategy.execute_nodes(
            graph, self._execution_order, inputs
        )

    def get_partial_results(self) -> Dict[str, Dict[str, Any]]:
        """Get partial results from execution that may have been interrupted.

        Returns:
            Dictionary mapping node IDs to their output results
        """
        return self._execution_strategy.get_partial_results()

</code>

src\ember\xcs\schedulers\factory.py:
<code>
"""
Scheduler factory for XCS.

Provides a factory function for creating scheduler instances based on
scheduler type and options. This centralizes scheduler creation logic
and allows for dynamic selection of the most appropriate scheduler.
"""

import logging
from typing import Any

from ember.xcs.schedulers.base_scheduler import BaseScheduler

logger = logging.getLogger(**name**)

def create_scheduler(scheduler_type: str = "auto", \*\*kwargs: Any) -> BaseScheduler:
"""Create a scheduler instance based on the specified type.

    Factory function that returns an appropriate scheduler for the given type
    and parameters.

    Args:
        scheduler_type: Type of scheduler to create. One of:
            - "auto": Automatically select the best scheduler
            - "sequential": Simple sequential execution
            - "parallel": Parallel execution with wave scheduling
            - "topological": Topological sort-based execution
            - "wave": Wave-based parallel execution
            - "noop": No-op scheduler for testing
        **kwargs: Additional scheduler-specific options
            - max_workers: Maximum number of parallel workers
            - timeout_seconds: Execution timeout
            - continue_on_error: Whether to continue after errors

    Returns:
        Scheduler instance
    """
    # Import implementations here to avoid circular imports
    from ember.xcs.schedulers.unified_scheduler import (
        NoOpScheduler,
        ParallelScheduler,
        SequentialScheduler,
        TopologicalScheduler,
        WaveScheduler,
    )

    # Extract common options
    max_workers = kwargs.get("max_workers")

    # Create appropriate scheduler based on type
    if scheduler_type == "noop":
        return NoOpScheduler()
    elif scheduler_type == "sequential":
        return SequentialScheduler()
    elif scheduler_type == "topological":
        return TopologicalScheduler()
    elif scheduler_type == "wave":
        return WaveScheduler(max_workers=max_workers)
    elif scheduler_type == "parallel":
        return ParallelScheduler(max_workers=max_workers)
    elif scheduler_type == "auto":
        # Auto-select based on properties
        if max_workers is not None and max_workers > 1:
            logger.debug(
                "Automatically selecting wave scheduler for parallel execution"
            )
            return WaveScheduler(max_workers=max_workers)
        else:
            logger.debug(
                "Automatically selecting topological scheduler for sequential execution"
            )
            return TopologicalScheduler()
    else:
        logger.warning(
            f"Unknown scheduler type: {scheduler_type}, falling back to topological"
        )
        return TopologicalScheduler()

</code>

src\ember\xcs\schedulers\unified_scheduler.py:
<code>
"""Unified scheduler implementations for XCS graph execution.

Provides concrete scheduler implementations using the strategy pattern,
with predefined combinations of ordering and execution strategies.
"""

from typing import Optional

from ember.xcs.schedulers.base_scheduler_impl import (
BaseSchedulerImpl,
NoopExecutionStrategy,
ParallelExecutionStrategy,
SequentialExecutionStrategy,
TopologicalOrderingStrategy,
WaveOrderingStrategy,
)

class NoOpScheduler(BaseSchedulerImpl):
"""Scheduler that doesn't actually execute operations.

    Useful for debugging, testing, and checking graph structure.
    """

    def __init__(self) -> None:
        """Initialize no-op scheduler."""
        super().__init__(
            ordering_strategy=TopologicalOrderingStrategy(),
            execution_strategy=NoopExecutionStrategy(),
        )

class SequentialScheduler(BaseSchedulerImpl):
"""Scheduler that executes operations sequentially in topological order.

    Simple, safe execution for any graph, with no parallelism.
    """

    def __init__(self) -> None:
        """Initialize sequential scheduler."""
        super().__init__(
            ordering_strategy=TopologicalOrderingStrategy(),
            execution_strategy=SequentialExecutionStrategy(),
        )

class TopologicalScheduler(BaseSchedulerImpl):
"""Scheduler that executes operations in topological order.

    Ensures correct dependency ordering while offering potential for parallelism.
    """

    def __init__(self) -> None:
        """Initialize topological scheduler."""
        super().__init__(
            ordering_strategy=TopologicalOrderingStrategy(),
            execution_strategy=SequentialExecutionStrategy(),
        )

class ParallelScheduler(BaseSchedulerImpl):
"""Scheduler that executes operations in parallel where possible.

    Uses wave-based execution for maximum parallelism based on dependencies.
    """

    def __init__(self, max_workers: Optional[int] = None) -> None:
        """Initialize parallel scheduler.

        Args:
            max_workers: Maximum number of worker threads
        """
        super().__init__(
            ordering_strategy=WaveOrderingStrategy(),
            execution_strategy=ParallelExecutionStrategy(max_workers=max_workers),
        )
        self.max_workers = max_workers

class WaveScheduler(BaseSchedulerImpl):
"""Scheduler that groups operations into execution waves.

    Provides structured parallelism with wave-based synchronization.
    """

    def __init__(self, max_workers: Optional[int] = None) -> None:
        """Initialize wave scheduler.

        Args:
            max_workers: Maximum number of worker threads
        """
        super().__init__(
            ordering_strategy=WaveOrderingStrategy(),
            execution_strategy=ParallelExecutionStrategy(max_workers=max_workers),
        )
        self.max_workers = max_workers

# Legacy aliases

TraceScheduler = SequentialScheduler
TopologicalSchedulerWithParallelDispatch = ParallelScheduler

</code>

src\ember\xcs\schedulers\_\_init\_\_.py:
<code>
"""
Schedulers for XCS graph execution.

Provides scheduler implementations for executing computational graphs with
different parallel execution strategies, using a unified interface.
"""

from ember.xcs.schedulers.base_scheduler import BaseScheduler
from ember.xcs.schedulers.factory import create_scheduler
from ember.xcs.schedulers.unified_scheduler import (
NoOpScheduler,
ParallelScheduler,
SequentialScheduler,
TopologicalScheduler,
WaveScheduler,
)

**all** = [
# Core scheduler interface
"BaseScheduler",
"create_scheduler",
# Scheduler implementations
"NoOpScheduler",
"ParallelScheduler",
"SequentialScheduler",
"TopologicalScheduler",
"WaveScheduler",
]

</code>

src\ember\xcs\tracer\autograph.py:
<code>
"""Dependency analysis and graph construction for XCS computational graphs.

Identifies data flow, state dependencies, and execution constraints between operator
invocations. Transforms execution traces into optimized computation graphs through:

1. Identity-based reference tracking
2. Value signature analysis
3. State mutation detection
4. Execution order constraints

Provides deterministic graph construction with parallelization opportunity detection.

Example:
```python
from ember.xcs.tracer.autograph import AutoGraphBuilder
from ember.xcs.tracer.xcs_tracing import TraceRecord
from ember.xcs.engine import execute_graph

    # Execution traces from previous operator calls
    records = [
        TraceRecord(
            operator_name="TextProcessor",
            node_id="1",
            inputs={"text": "input text"},
            outputs={"tokens": ["input", "text"]}
        ),
        TraceRecord(
            operator_name="Embedding",
            node_id="2",
            inputs={"tokens": ["input", "text"]},
            outputs={"vectors": [[0.1, 0.2], [0.3, 0.4]]}
        ),
        TraceRecord(
            operator_name="Classifier",
            node_id="3",
            inputs={"vectors": [[0.1, 0.2], [0.3, 0.4]]},
            outputs={"class": "category_a", "confidence": 0.92}
        )
    ]

    # Construct optimized graph with parallelization metadata
    builder = AutoGraphBuilder()
    graph = builder.build_graph(records)

    # Execute with new inputs
    results = execute_graph(
        graph,
        inputs={"text": "different input"}
    )
    # Results contain final classification output
    ```

"""

from **future** import annotations

import hashlib
import logging
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any, Callable, Dict, List, Optional, Set, Union

from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.tracer.xcs_tracing import TraceRecord

logger = logging.getLogger(**name**)

class DependencyType(Enum):
"""Dependency types between operator invocations."""

    DATA_FLOW = auto()  # Value passed directly between operators
    EXECUTION_ORDER = auto()  # Sequential execution requirement from control flow
    STATE_MUTATION = auto()  # Operators sharing stateful instance
    INFERRED = auto()  # Dependency determined through heuristic analysis

@dataclass
class DataReference:
"""Reference to data passed between operators.

    Tracks object identity and content signatures for dependency detection.
    Provides dual matching strategies: direct object identity for reference types
    and content-based signatures for value types.

    Attributes:
        obj_id: Object identity hash or None for primitive values
        path: Data location path (e.g., "outputs.result")
        signature: Content-based signature for value comparison
        producer: Node ID that produced this data
    """

    obj_id: Optional[int]
    path: str
    signature: str
    producer: Optional[str] = None

@dataclass
class DependencyNode:
"""Represents a single operator invocation in the dependency graph.

    Tracks data flow between operators, capturing inputs consumed and outputs
    produced. Functions as the primary data structure for dependency analysis.

    Attributes:
        trace_record: Source execution trace
        node_id: Unique node identifier
        inputs: Input data references keyed by name
        outputs: Output data references keyed by name
        dependencies: Incoming dependencies with dependency types
        outbound_edges: IDs of nodes that consume this node's outputs
    """

    trace_record: TraceRecord
    node_id: str
    inputs: Dict[str, DataReference] = field(default_factory=dict)
    outputs: Dict[str, DataReference] = field(default_factory=dict)
    dependencies: Dict[str, DependencyType] = field(default_factory=dict)
    outbound_edges: Set[str] = field(default_factory=set)

class DependencyAnalyzer:
"""Analyzes dependencies between operator executions.

    Performs multi-phase dependency analysis to identify relationships between
    operators. Employs several detection strategies in priority order:
    1. Object identity matching (most reliable)
    2. Content signature matching (for value types)
    3. Execution ordering constraints (for stateful operations)
    4. State mutation dependencies (for shared operator instances)

    The implementation prioritizes deterministic analysis over heuristics.
    """

    def __init__(self) -> None:
        """Initialize the dependency analyzer."""
        self.nodes: Dict[str, DependencyNode] = {}
        self.data_registry: Dict[Union[int, str], str] = {}
        self.operator_to_nodes: Dict[int, List[str]] = {}

    def analyze(self, records: List[TraceRecord]) -> Dict[str, DependencyNode]:
        """Performs multi-phase dependency analysis on trace records.

        Args:
            records: Execution trace records to analyze

        Returns:
            Mapping from node IDs to fully constructed dependency nodes

        Process:
        1. Creates dependency nodes for each trace record
        2. Extracts data references from inputs/outputs with path tracking
        3. Matches data flows using identity and signature comparison
        4. Applies execution ordering constraints for sequential operations
        5. Detects stateful operations requiring ordered execution
        6. Validates and optimizes the graph by removing redundant edges
        """
        if not records:
            return {}

        # Reset state for this analysis
        self.nodes.clear()
        self.data_registry.clear()
        self.operator_to_nodes.clear()

        # Create nodes and register operators
        for record in records:
            node_id = record.node_id
            node = DependencyNode(trace_record=record, node_id=node_id)
            self.nodes[node_id] = node

            # Track which nodes use the same operator instance
            op_id = id(record.operator) if record.operator else None
            if op_id:
                self.operator_to_nodes.setdefault(op_id, []).append(node_id)

        # Phase 1: Extract data references
        for _, node in self.nodes.items():
            self._extract_data_references(node)

        # Phase 2: Match data flow based on identity and signatures
        self._analyze_data_flow()

        # Phase 3: Apply execution ordering constraints
        self._apply_execution_ordering(records)

        # Phase 4: Detect stateful operation patterns
        self._detect_stateful_operations()

        # Phase 5: Validate and optimize dependency graph
        self._validate_and_optimize()

        return self.nodes

    def _extract_data_references(self, node: DependencyNode) -> None:
        """Extract data references from inputs and outputs.

        Creates DataReference objects for all input and output values,
        handling both dictionary and non-dictionary data structures uniformly.

        Args:
            node: Node to extract references for
        """
        # Extract input references
        self._extract_references_from_dict(
            data_dict=node.trace_record.inputs,
            references_dict=node.inputs,
            path_prefix="inputs",
        )

        # Extract output references - handle all types uniformly
        self._extract_references_from_dict(
            data_dict=node.trace_record.outputs,
            references_dict=node.outputs,
            path_prefix="outputs",
        )

        # Register outputs in the data registry for dependency tracking
        for _, ref in node.outputs.items():
            # Register by object ID for identity-based matching (most reliable)
            if ref.obj_id:
                self.data_registry[ref.obj_id] = node.node_id

            # Register by signature for content-based matching (fallback)
            if ref.signature:
                self.data_registry[ref.signature] = node.node_id

            # Set producer reference
            ref.producer = node.node_id

    def _extract_data_from_value(self, value: Any) -> Dict[str, Any]:
        """Extract dictionary data from various value types.

        Handles different types of input values and converts them to a
        standard dictionary format for dependency analysis.

        Args:
            value: Input value to extract data from

        Returns:
            Dictionary representation of the value

        Raises:
            ValueError: If value cannot be converted to a dictionary
        """
        # Handle None
        if value is None:
            return {}

        # Handle native dictionaries
        if isinstance(value, dict):
            return value

        # Handle Pydantic models (v2 and v1)
        if hasattr(value, "model_dump"):
            # Pydantic v2
            return value.model_dump()
        elif hasattr(value, "dict") and callable(value.dict):
            # Pydantic v1
            return value.dict()

        # Handle other mapping types
        if hasattr(value, "items") and callable(value.items):
            try:
                return dict(value.items())
            except (TypeError, ValueError):
                pass

        # Value is not a dictionary-like object
        # Just return a simple identifier key
        return {"value": value}

    def _extract_references_from_dict(
        self,
        data_dict: Dict[str, Any],
        references_dict: Dict[str, DataReference],
        path_prefix: str,
    ) -> None:
        """Extract data references from a dictionary.

        Args:
            data_dict: Dictionary to extract references from
            references_dict: Dictionary to store extracted references
            path_prefix: Path prefix for generated references
        """
        if not data_dict:
            return

        # Convert to standard dictionary format
        try:
            extracted_data = self._extract_data_from_value(data_dict)
        except ValueError:
            # If conversion fails, treat as a single value
            ref = self._create_reference(data_dict, path_prefix)
            references_dict["value"] = ref
            return

        # Process all keys in the dictionary
        for key, value in extracted_data.items():
            path = f"{path_prefix}.{key}"

            # Create reference for this value
            ref = self._create_reference(value, path)
            references_dict[key] = ref

            # Recursively process nested structures
            nested_dict = self._extract_data_from_value(value)
            if nested_dict and nested_dict != {"value": value}:
                # Only process if we got a meaningful dictionary
                nested_references: Dict[str, DataReference] = {}
                self._extract_references_from_dict(
                    data_dict=nested_dict,
                    references_dict=nested_references,
                    path_prefix=path,
                )
                # Add nested references with prefixed keys
                for nested_key, nested_ref in nested_references.items():
                    references_dict[f"{key}.{nested_key}"] = nested_ref

    def _create_reference(self, value: Any, path: str) -> DataReference:
        """Create a data reference for a value.

        Args:
            value: Value to create reference for
            path: Path to this value in the data structure

        Returns:
            Data reference for the value
        """
        # Use object identity for reference types, handle None specially
        obj_id = None
        if value is not None and not isinstance(value, (int, float, bool, str)):
            obj_id = id(value)

        # Create content signature
        signature = self._create_signature(value)

        return DataReference(obj_id=obj_id, path=path, signature=signature)

    def _create_signature(self, value: Any) -> str:
        """Create a content signature for a value.

        Optimized for speed and collision resistance.

        Args:
            value: Value to create signature for

        Returns:
            Content signature string
        """
        if value is None:
            return "none"

        # Handle primitive types directly
        if isinstance(value, (int, float, bool)):
            return f"{type(value).__name__}:{value}"

        if isinstance(value, str):
            # For strings, use a hash function to avoid excessive memory use
            if len(value) > 100:
                return f"str:{hashlib.md5(value.encode('utf-8')).hexdigest()}"
            return f"str:{value}"

        # Generate signature for complex types
        try:
            # Try a stable string representation first
            if hasattr(value, "__repr__"):
                repr_val = repr(value)
                # Avoid huge values
                if len(repr_val) > 1000:
                    repr_val = repr_val[:1000]
                # Create hash of the string representation
                md5_hash = hashlib.md5(repr_val.encode("utf-8")).hexdigest()
                return f"{type(value).__name__}:{md5_hash}"

            # Fall back to type and id
            return f"{type(value).__name__}:{id(value)}"
        except Exception:
            # Ultimate fallback
            return f"obj:{id(value)}"

    def _analyze_data_flow(self) -> None:
        """Analyze data flow between nodes based on references.

        Identifies where outputs from one node are used as inputs to another.
        Uses both identity-based matching and signature-based matching with
        priority given to identity for better reliability.
        """
        # Process nodes in order
        for consumer_id, consumer in self.nodes.items():
            # Check each input for a matching output
            for _, input_ref in consumer.inputs.items():
                producer_id = None

                # Try to find producer by object identity first
                if input_ref.obj_id and input_ref.obj_id in self.data_registry:
                    producer_id = self.data_registry[input_ref.obj_id]

                # Fall back to signature matching if no identity match
                elif input_ref.signature and input_ref.signature in self.data_registry:
                    producer_id = self.data_registry[input_ref.signature]

                # If we found a producer (that isn't the consumer itself)
                if producer_id and producer_id != consumer_id:
                    # A valid data dependency exists
                    consumer.dependencies[producer_id] = DependencyType.DATA_FLOW

                    # Register outbound edge on the producer
                    producer = self.nodes[producer_id]
                    producer.outbound_edges.add(consumer_id)

    def _apply_execution_ordering(self, records: List[TraceRecord]) -> None:
        """Apply execution ordering constraints.

        Ensures dependencies reflect the original execution order where necessary.

        Args:
            records: Original trace records in execution order
        """
        # Sort records by timestamp
        sorted_records = sorted(records, key=lambda r: r.timestamp)
        ordered_node_ids = [r.node_id for r in sorted_records]

        # Track nodes that must maintain execution order
        order_dependent_nodes = set()

        # Find nodes that need execution order preservation
        for i in range(len(ordered_node_ids) - 1):
            current_id = ordered_node_ids[i]
            next_id = ordered_node_ids[i + 1]

            current_node = self.nodes[current_id]
            next_node = self.nodes[next_id]

            # Check if nodes share operator instances
            current_op = current_node.trace_record.operator
            next_op = next_node.trace_record.operator

            # Nodes using the same stateful operator likely need ordering
            if current_op is not None and current_op is next_op:
                order_dependent_nodes.add(current_id)
                order_dependent_nodes.add(next_id)

        # Apply ordering dependencies for order-dependent nodes
        for i in range(len(ordered_node_ids) - 1):
            current_id = ordered_node_ids[i]

            # Skip nodes not marked as order-dependent
            if current_id not in order_dependent_nodes:
                continue

            # Find the next order-dependent node
            for j in range(i + 1, len(ordered_node_ids)):
                next_id = ordered_node_ids[j]
                if next_id in order_dependent_nodes:
                    # Check if dependency already exists
                    curr_node = self.nodes[current_id]
                    next_node = self.nodes[next_id]
                    has_dependency = (
                        next_id in curr_node.outbound_edges
                        or current_id in next_node.dependencies
                    )

                    # Add execution order dependency if not already connected
                    if not has_dependency:
                        # Add dependency with execution order type
                        next_node.dependencies[
                            current_id
                        ] = DependencyType.EXECUTION_ORDER
                        self.nodes[current_id].outbound_edges.add(next_id)
                    break

    def _detect_stateful_operations(self) -> None:
        """Detect stateful operation patterns.

        Identifies where operations on the same operator instance might have
        dependencies due to shared state.
        """
        # Process operator instances with multiple invocations
        for _, node_ids in self.operator_to_nodes.items():
            if len(node_ids) < 2:
                continue

            # Find temporal order of these nodes
            temporal_order = sorted(
                node_ids, key=lambda n: self.nodes[n].trace_record.timestamp
            )

            # Connect nodes in temporal order if not already connected
            for i in range(len(temporal_order) - 1):
                current_id = temporal_order[i]
                next_id = temporal_order[i + 1]

                current_node = self.nodes[current_id]
                next_node = self.nodes[next_id]

                # Check if already connected via data flow
                if (
                    next_id in current_node.outbound_edges
                    or current_id in next_node.dependencies
                ):
                    continue

                # Check for non-trivial state in the operator instance
                current_rec = self.nodes[current_id].trace_record
                has_state = self._check_for_operator_state(current_rec.operator)

                # If operator has state, add state mutation dependency
                if has_state:
                    next_node.dependencies[current_id] = DependencyType.STATE_MUTATION
                    current_node.outbound_edges.add(next_id)

    def _check_for_operator_state(self, operator: Any) -> bool:
        """Check if an operator has non-trivial state.

        Args:
            operator: Operator instance to check

        Returns:
            True if operator likely has mutable state
        """
        if operator is None:
            return False

        # Check for instance variables beyond standard ones
        if hasattr(operator, "__dict__"):
            # Ignore standard attributes and methods
            state_attrs = [
                attr
                for attr in operator.__dict__
                if not attr.startswith("_") and not callable(getattr(operator, attr))
            ]
            return len(state_attrs) > 0

        return False

    def _validate_and_optimize(self) -> None:
        """Validate and optimize the dependency graph.

        Checks for cycles and ensures the graph is optimized for execution.
        """
        # Detect and break cycles
        if self._detect_and_break_cycles():
            logger.warning("Cycles detected and broken in dependency graph")

        # Optimize the graph (e.g., transitive reduction)
        self._perform_transitive_reduction()

    def _detect_and_break_cycles(self) -> bool:
        """Detect and break cycles in the dependency graph.

        Returns:
            True if cycles were detected and broken
        """
        # Build adjacency list
        graph: Dict[str, Set[str]] = {
            node_id: set(node.dependencies.keys())
            for node_id, node in self.nodes.items()
        }

        # Track visited nodes for cycle detection
        visited = set()
        temp_visited = set()
        cycles_broken = False

        def visit(node_id: str) -> bool:
            """DFS visit with cycle detection.

            Args:
                node_id: Node to visit

            Returns:
                True if a cycle was detected and broken
            """
            nonlocal cycles_broken

            # Skip already processed nodes
            if node_id in visited:
                return False

            # Check for cycle
            if node_id in temp_visited:
                # Found a cycle - break it by removing this edge
                # We need to identify which edge to remove
                for dependent_id in temp_visited:
                    if (
                        dependent_id in self.nodes
                        and node_id in self.nodes[dependent_id].dependencies
                    ):
                        # Remove lowest priority dependency
                        dep_type = self.nodes[dependent_id].dependencies[node_id]

                        # Prefer to break inferred or execution order dependencies first
                        if dep_type in (
                            DependencyType.INFERRED,
                            DependencyType.EXECUTION_ORDER,
                        ):
                            self.nodes[dependent_id].dependencies.pop(node_id)
                            if dependent_id in self.nodes[node_id].outbound_edges:
                                self.nodes[node_id].outbound_edges.remove(dependent_id)
                            cycles_broken = True
                            return True

                # If no preferred edge found, remove this one
                if node_id in self.nodes[list(temp_visited)[-1]].dependencies:
                    self.nodes[list(temp_visited)[-1]].dependencies.pop(node_id)
                    cycles_broken = True
                    return True

            # Mark as being visited
            temp_visited.add(node_id)

            # Visit neighbors
            for neighbor in graph.get(node_id, set()):
                if visit(neighbor):
                    return True

            # Mark as fully visited
            temp_visited.remove(node_id)
            visited.add(node_id)
            return False

        # Visit all nodes
        for node_id in list(self.nodes.keys()):
            if node_id not in visited:
                if visit(node_id):
                    # Reset visited sets on cycle detection
                    visited.clear()
                    temp_visited.clear()

        return cycles_broken

    def _perform_transitive_reduction(self) -> None:
        """Perform transitive reduction on the dependency graph.

        Removes redundant edges that don't affect the partial order.
        """
        # Build reachability table using Floyd-Warshall algorithm
        n = len(self.nodes)
        node_ids = list(self.nodes.keys())
        node_to_index = {node_id: i for i, node_id in enumerate(node_ids)}

        # Initialize reachability matrix
        reachable = [[False] * n for _ in range(n)]

        # Set direct connections
        for i, node_id in enumerate(node_ids):
            node = self.nodes[node_id]
            for dep_id in node.dependencies:
                if dep_id in node_to_index:
                    j = node_to_index[dep_id]
                    reachable[i][j] = True

        # Compute transitive closure
        for k in range(n):
            for i in range(n):
                for j in range(n):
                    reachable[i][j] = reachable[i][j] or (
                        reachable[i][k] and reachable[k][j]
                    )

        # Perform reduction
        for i, node_id in enumerate(node_ids):
            node = self.nodes[node_id]

            # Track dependencies to remove
            to_remove = []

            # Check each direct dependency
            for dep_id in node.dependencies:
                if dep_id not in node_to_index:
                    continue

                j = node_to_index[dep_id]

                # Check if this dependency is redundant
                # A->C is redundant if A->B and B->C for some B
                is_redundant = False
                for k in range(n):
                    if k != i and k != j and reachable[i][k] and reachable[k][j]:
                        is_redundant = True
                        break

                if is_redundant:
                    to_remove.append(dep_id)

            # Remove redundant dependencies
            for dep_id in to_remove:
                if dep_id in node.dependencies:
                    node.dependencies.pop(dep_id)

                # Also update outbound edges
                if node_id in self.nodes[dep_id].outbound_edges:
                    self.nodes[dep_id].outbound_edges.remove(node_id)

class AutoGraphBuilder:
"""Builds optimized XCS computation graphs from execution traces.

    Transforms trace records into executable graphs with proper dependency
    preservation. Performs graph optimization through:

    1. Accurate dependency identification
    2. Parallelization opportunity detection
    3. Transitive reduction for minimal edge count
    4. Execution wave identification for parallel scheduling
    5. Operator binding with fallback mechanisms

    The implementation focuses on correctness and deterministic behavior
    first, with performance optimizations applied where safe.
    """

    def __init__(self) -> None:
        """Initializes an AutoGraphBuilder instance."""
        # Initialize dependencies storage
        self.nodes: Dict[str, DependencyNode] = {}

    def _extract_field_mappings(
        self, dep_node: DependencyNode, source_node_id: str, target_node_id: str
    ) -> Dict[str, str]:
        """Extract field-level mappings between nodes from dependency analysis.

        Uses the references in dependency nodes to determine which output fields
        from the source node are used as input fields in the target node.

        Args:
            dep_node: Dependency node containing data references
            source_node_id: ID of the node producing output
            target_node_id: ID of the node consuming input

        Returns:
            Dictionary mapping output field names to input field names
        """
        field_mappings = {}

        # Find the producer node for each input reference
        for input_field, input_ref in dep_node.inputs.items():
            # Check if this input came from the source node
            producer_id = input_ref.producer
            if producer_id == source_node_id:
                # Find matching output reference by path or signature
                for output_field, output_ref in self.nodes[
                    source_node_id
                ].outputs.items():
                    if (input_ref.obj_id and input_ref.obj_id == output_ref.obj_id) or (
                        input_ref.signature
                        and input_ref.signature == output_ref.signature
                    ):
                        # Map output field to input field
                        field_mappings[output_field] = input_field
                        break

        return field_mappings

    def build_graph(self, records: List[TraceRecord] = None, **kwargs) -> XCSGraph:
        """Builds an executable XCS graph from trace records.

        Constructs a graph in multiple phases:
        1. Analyzes dependencies between operator executions
        2. Creates graph nodes with callable operator references
        3. Adds edges based on identified dependencies
        4. Adds optimization metadata for execution scheduling

        Args:
            records: Trace records from operator executions
            **kwargs: Alternative keyword passing for records

        Returns:
            Executable XCS graph with optimization metadata

        Raises:
            ValueError: If invalid or conflicting records are provided
        """
        # Support both positional and keyword args for different calling conventions
        if records is None and "records" in kwargs:
            records = kwargs["records"]

        # Handle empty case
        if not records:
            return XCSGraph()

        # Create new graph
        graph = XCSGraph()

        # Map from trace record node_id to graph node_id
        node_id_map: Dict[str, str] = {}

        # Analyze dependencies
        analyzer = DependencyAnalyzer()
        dep_nodes = analyzer.analyze(records)
        # Store for field mapping extraction
        self.nodes = dep_nodes

        # First pass: Add nodes to the graph
        for i, record in enumerate(records):
            # Create a stable, predictable node ID
            graph_node_id = f"{record.operator_name}_{i}"
            node_id_map[record.node_id] = graph_node_id
            record.graph_node_id = graph_node_id

            # Add node to graph with appropriate operator
            graph.add_node(
                operator=self._create_operator_callable(trace_record=record),
                node_id=graph_node_id,
                name=record.operator_name,
            )

        # Second pass: Add edges based on dependencies
        for node_id, dep_node in dep_nodes.items():
            # Skip nodes not in the map (should never happen)
            if node_id not in node_id_map:
                continue

            graph_node_id = node_id_map[node_id]

            # Add edges for all dependencies
            for dep_id, dep_type in dep_node.dependencies.items():
                # Skip if dependent node not in map
                if dep_id not in node_id_map:
                    continue

                dep_graph_id = node_id_map[dep_id]

                # Extract field-level dependencies for precise mapping
                field_mappings = self._extract_field_mappings(dep_node, dep_id, node_id)

                # Add edge with dependency type and field mappings
                graph.add_edge(
                    from_id=dep_graph_id,
                    to_id=graph_node_id,
                    field_mappings=field_mappings,
                )

                # Optionally add edge metadata
                if "dependencies" not in graph.metadata:
                    graph.metadata["dependencies"] = {}

                edge_key = f"{dep_graph_id}->{graph_node_id}"
                graph.metadata["dependencies"][edge_key] = dep_type.name

        # Add optimization metadata
        self._add_execution_metadata(graph, dep_nodes, node_id_map)

        return graph

    def _add_execution_metadata(
        self,
        graph: XCSGraph,
        dep_nodes: Dict[str, DependencyNode],
        node_id_map: Dict[str, str],
    ) -> None:
        """Adds optimization metadata to the graph for efficient execution.

        Enhances the graph with execution hints for the scheduler:
        - Identifies parallelizable node groups
        - Detects aggregator nodes that combine parallel outputs
        - Organizes nodes into execution waves for wave-based scheduling
        - Identifies leaf nodes as potential outputs

        Args:
            graph: Target XCS graph to enhance with metadata
            dep_nodes: Analyzed dependency nodes
            node_id_map: Mapping from analysis node IDs to graph node IDs
        """
        # Add parallelization hints
        graph.metadata["parallelizable_nodes"] = []
        graph.metadata["parallel_groups"] = {}

        # Identify independent nodes (siblings without dependencies between them)
        parent_to_children: Dict[str, List[str]] = {}

        # Group nodes by parent
        for node_id, dep_node in dep_nodes.items():
            # Skip if not in the map
            if node_id not in node_id_map:
                continue

            graph_node_id = node_id_map[node_id]

            # Check dependencies to find parent
            dependencies = list(dep_node.dependencies.keys())

            if not dependencies:
                # Root node (no parent)
                parent_to_children.setdefault("root", []).append(graph_node_id)
            else:
                # Use first dependency as parent
                parent_id = dependencies[0]
                if parent_id in node_id_map:
                    parent_graph_id = node_id_map[parent_id]
                    parent_to_children.setdefault(parent_graph_id, []).append(
                        graph_node_id
                    )

        # Identify parallel groups - siblings with no dependencies between them
        group_id = 0
        for _, children in parent_to_children.items():
            if len(children) < 2:
                continue

            # Check if these children can run in parallel
            can_parallelize = True

            # Verify no child depends on another child
            for i, child1 in enumerate(children):
                for j, child2 in enumerate(children):
                    if i != j:
                        # Find original node IDs - extract from id maps
                        orig_id_map1 = [
                            (n, g) for n, g in node_id_map.items() if g == child1
                        ]
                        orig_id_map2 = [
                            (n, g) for n, g in node_id_map.items() if g == child2
                        ]

                        orig_id1 = orig_id_map1[0][0] if orig_id_map1 else None
                        orig_id2 = orig_id_map2[0][0] if orig_id_map2 else None

                        # Check for any dependencies between the nodes
                        has_dependency = False
                        if orig_id1 and orig_id2:
                            # Check if node2 depends on node1
                            if orig_id1 in dep_nodes:
                                node1 = dep_nodes[orig_id1]
                                if orig_id2 in node1.dependencies:
                                    has_dependency = True

                            # Check if node1 depends on node2
                            if not has_dependency and orig_id2 in dep_nodes:
                                node2 = dep_nodes[orig_id2]
                                if orig_id1 in node2.dependencies:
                                    has_dependency = True

                        if has_dependency:
                            can_parallelize = False
                            break

                if not can_parallelize:
                    break

            # If can parallelize, create a group
            if can_parallelize:
                group_name = f"parallel_group_{group_id}"
                group_id += 1

                # Add group to metadata
                graph.metadata["parallel_groups"][group_name] = children

                # Mark nodes as parallelizable
                graph.metadata["parallelizable_nodes"].extend(children)

        # Identify output node(s)
        leaf_nodes = []
        for node_id, node in graph.nodes.items():
            if not node.outbound_edges:
                leaf_nodes.append(node_id)

        if leaf_nodes:
            # Use the last leaf node as the primary output
            # Set both legacy and new-style metadata for compatibility
            graph.metadata["output_node"] = leaf_nodes[-1]
            graph.metadata["output_node_id"] = leaf_nodes[-1]
            # Store all leaf nodes for completeness
            graph.metadata["leaf_nodes"] = leaf_nodes

        # Add aggregator nodes identification
        graph.metadata["aggregator_nodes"] = []

        # Find aggregator nodes (nodes with multiple inputs)
        for node_id in graph.nodes:
            orig_id = next((n for n, g in node_id_map.items() if g == node_id), None)
            if orig_id and orig_id in dep_nodes:
                dep_node = dep_nodes[orig_id]
                if len(dep_node.dependencies) > 1:
                    # This node aggregates results from multiple sources
                    graph.metadata["aggregator_nodes"].append(node_id)

                    # Get mapped dependency IDs for this node
                    dep_graph_ids = []
                    for dep_id in dep_node.dependencies:
                        if dep_id in node_id_map:
                            dep_graph_ids.append(node_id_map[dep_id])

                    # Check each parallel group for membership
                    for group_name, members in graph.metadata[
                        "parallel_groups"
                    ].items():
                        # Check if multiple dependencies are from this group
                        common_members = [m for m in members if m in dep_graph_ids]
                        if len(common_members) > 1:
                            # Add information about what this node aggregates
                            if "aggregates_groups" not in graph.metadata:
                                graph.metadata["aggregates_groups"] = {}
                            graph.metadata["aggregates_groups"][node_id] = group_name

        # Add dependency waves for wave-based execution
        topo_order = graph.topological_sort()

        # Calculate in-degree for each node (number of dependencies)
        in_degree = {}
        for node_id in topo_order:
            in_degree[node_id] = len(graph.nodes[node_id].inbound_edges)
        waves = []
        remaining = set(topo_order)

        while remaining:
            wave = [node_id for node_id in remaining if in_degree[node_id] == 0]
            if not wave:
                break

            waves.append(wave)
            remaining.difference_update(wave)

            for node_id in wave:
                for out_edge in graph.nodes[node_id].outbound_edges:
                    if out_edge in in_degree:
                        in_degree[out_edge] -= 1

        graph.metadata["execution_waves"] = waves

    @staticmethod
    def _create_operator_callable(
        *, trace_record: TraceRecord
    ) -> Callable[[Dict[str, Any]], Any]:
        """Creates a callable that invokes the original operator with new inputs.

        Enables true JIT compilation rather than trace replay by:
        1. Retrieving operator reference through weak reference
        2. Executing the operator with provided inputs
        3. Falling back to recorded outputs if operator unavailable

        This approach balances memory safety with execution fidelity.

        Args:
            trace_record: Trace record with operator reference

        Returns:
            Function that executes the operator or replays traced outputs
        """
        import weakref

        # Store weak reference to avoid reference cycles
        operator_ref = (
            weakref.ref(trace_record.operator) if trace_record.operator else None
        )

        # Capture type information for reconstruction
        input_type_paths = trace_record.input_type_paths
        output_type_paths = trace_record.output_type_paths

        def operation_fn(*, inputs: Dict[str, Any]) -> Any:
            # Get the actual operator from the weak reference
            operator = operator_ref() if operator_ref else None

            if operator is None:
                # Operator no longer exists - reconstruct outputs with correct types
                if isinstance(trace_record.outputs, dict) and output_type_paths:
                    return AutoGraphBuilder._reconstruct_with_types(
                        trace_record.outputs, output_type_paths
                    )
                return trace_record.outputs

            # Execute with proper boundary crossing
            try:
                # Apply proper boundary crossing for inputs
                # Convert inputs to proper EmberModel types using the operator's specification
                typed_inputs = inputs
                if hasattr(operator, "specification") and hasattr(
                    operator.specification, "validate_inputs"
                ):
                    try:
                        typed_inputs = operator.specification.validate_inputs(
                            inputs=inputs
                        )
                    except Exception as e:
                        # Log but continue with original inputs if validation fails
                        import logging

                        logging.warning(
                            f"Input validation failed during JIT execution: {e}"
                        )

                # Execute operator with validated inputs
                raw_output = (
                    operator(inputs=typed_inputs)
                    if callable(operator)
                    else operator.forward(inputs=typed_inputs)
                )

                # Apply proper boundary crossing for outputs
                # Ensure the output is a properly validated EmberModel
                if hasattr(operator, "specification") and hasattr(
                    operator.specification, "validate_output"
                ):
                    try:
                        return operator.specification.validate_output(output=raw_output)
                    except Exception as e:
                        # Log but continue with original output if validation fails
                        import logging

                        logging.warning(
                            f"Output validation failed during JIT execution: {e}"
                        )
                        return raw_output

                return raw_output
            except Exception as e:
                # Fallback to reconstructed outputs on error
                import logging

                logging.exception(f"Error during JIT execution: {e}")

                if isinstance(trace_record.outputs, dict) and output_type_paths:
                    return AutoGraphBuilder._reconstruct_with_types(
                        trace_record.outputs, output_type_paths
                    )
                return trace_record.outputs

        return operation_fn

    @staticmethod
    def _reconstruct_with_types(
        data: Dict[str, Any], type_paths: Dict[str, str]
    ) -> Dict[str, Any]:
        """Reconstruct dictionary values with their proper types.

        Args:
            data: Dictionary with values to potentially reconstruct
            type_paths: Mapping of keys to type paths for reconstruction

        Returns:
            Dictionary with values reconstructed to proper types where possible
        """
        result = {}

        for key, value in data.items():
            if key in type_paths and isinstance(value, dict):
                type_path = type_paths[key]
                try:
                    # Import class
                    last_dot = type_path.rfind(".")
                    if last_dot > 0:
                        module_name = type_path[:last_dot]
                        class_name = type_path[last_dot + 1 :]

                        module = __import__(module_name, fromlist=[class_name])
                        cls = getattr(module, class_name)

                        # Reconstruct proper type
                        if hasattr(cls, "from_dict") and callable(cls.from_dict):
                            result[key] = cls.from_dict(value)
                            continue
                except (ImportError, AttributeError):
                    pass

            # Fallback to original value
            result[key] = value

        return result

def autograph(records=None):
"""Creates an XCS graph from execution trace records.

    A convenience function that constructs an AutoGraphBuilder and builds
    a graph from the provided trace records.

    Args:
        records: List of trace records from operator executions

    Returns:
        An XCS graph with nodes and edges based on the trace records
    """
    builder = AutoGraphBuilder()
    return builder.build_graph(records=records)

</code>

src\ember\xcs\tracer\structural_jit.py:
<code>
"""
Structural JIT: Graph-Based Auto-Optimization for Ember Operators

Providing a just-in-time (JIT) compilation system for Ember operators
that analyzes operator structure directly rather than relying on execution tracing.
Converting operator compositions into optimized XCS graphs and executing
them with the appropriate scheduling strategy.

Key capabilities:

1. Structural analysis using Python's pytree protocol
2. Automatic graph construction without execution tracing
3. Parallel execution of independent operations
4. Adaptive scheduling based on graph structure

The implementation uses immutable data structures and side-effect-free functions
with a modular design:

- Components are focused on specific aspects of the JIT process
- New execution strategies can be added without modifying existing code
- Strategy implementations are interchangeable
- High-level modules depend on abstractions rather than specific implementations

Example:
```python
@structural_jit
class MyCompositeOperator(Operator):
def **init**(self):
self.op1 = SubOperator1()
self.op2 = SubOperator2()

        def forward(self, *, inputs):
            # Multi-step computation
            intermediate = self.op1(inputs=inputs)
            result = self.op2(inputs=intermediate)
            return result

    # Using the optimized operator
    op = MyCompositeOperator()
    result = op(inputs={"text": "example"})
    # result == {"output": "processed example"}
    ```

"""

from **future** import annotations

import functools
import inspect
import logging
import time
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import (
Any,
Callable,
Dict,
List,
Optional,
Protocol,
Tuple,
Type,
TypeVar,
Union,
cast,
runtime_checkable,
)

from ember.xcs.engine.xcs_engine import (
IScheduler,
TopologicalSchedulerWithParallelDispatch,
compile_graph,
)
from ember.xcs.engine.xcs_noop_scheduler import XCSNoOpScheduler
from ember.xcs.graph.xcs_graph import XCSGraph
from ember.xcs.tracer.tracer_decorator import JITCache

# Logger for this module

logger = logging.getLogger(**name**)

# Type variables

T = TypeVar("T") # Generic return type
OpT = TypeVar("OpT", bound="Operator") # Operator type

# Cache for compiled graphs

\_structural_jit_cache = JITCache[XCSGraph]()

# -----------------------------------------------------------------------------

# Protocols & Type Definitions

# -----------------------------------------------------------------------------

@runtime_checkable
class Operator(Protocol):
"""Protocol defining the expected interface for Operators."""

    def __call__(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the operator with provided inputs."""
        pass

@runtime_checkable
class PytreeCompatible(Protocol):
"""Protocol for objects compatible with the pytree protocol."""

    def __pytree_flatten__(self) -> Tuple[List[Any], Dict[str, Any]]:
        """Flatten object into a list of dynamic values and static metadata."""
        pass

    @classmethod
    def __pytree_unflatten__(cls, metadata: Dict[str, Any], values: List[Any]) -> Any:
        """Reconstruct object from flattened values and metadata."""
        pass

@runtime_checkable
class StructureDependency(Protocol):
"""Protocol for operators to declare structural dependencies.

    Operators implementing this protocol can explicitly define their
    structural dependencies, improving the precision of structural JIT
    and enabling state-aware caching.
    """

    def get_structural_dependencies(self) -> Dict[str, List[str]]:
        """Return mapping of operator attribute names to their dependencies.

        Returns:
            Dict mapping attribute names to lists of attribute names they depend on.
            Example: {"output_field": ["input_field1", "input_field2"]}
        """
        pass

    def get_structure_signature(self) -> str:
        """Return a signature representing the current structure state.

        When this signature changes, cached structure graphs should be invalidated.
        This could be a hash of structure variables or a version number that
        the operator increments when structure changes.

        Returns:
            A string signature representing the current structure state.
        """
        pass

# -----------------------------------------------------------------------------

# Execution Strategy Definition

# -----------------------------------------------------------------------------

@dataclass
class ExecutionConfig:
"""Configuration for graph execution.

    Defines parameters for scheduler selection and execution behavior.

    Attributes:
        strategy: Execution approach to use
        parallel_threshold: Minimum nodes to trigger parallelism in auto mode
        max_workers: Maximum concurrent worker threads for parallel execution
    """

    strategy: str = "auto"
    parallel_threshold: int = 5
    max_workers: Optional[int] = None

def get_scheduler(graph: XCSGraph, config: ExecutionConfig) -> IScheduler:
"""Create the appropriate scheduler based on strategy and graph.

    Analyzes graph characteristics and config settings to select
    the optimal scheduler implementation.

    Args:
        graph: Graph to be executed
        config: Execution configuration parameters

    Returns:
        Scheduler instance optimized for the graph

    Raises:
        ValueError: If strategy is invalid
    """
    # Handle pre-defined strategies first
    if config.strategy == "sequential":
        return XCSNoOpScheduler()

    if config.strategy == "parallel":
        return TopologicalSchedulerWithParallelDispatch(max_workers=config.max_workers)

    if config.strategy == "auto":
        # Auto mode - analyze graph for parallelization potential
        if len(graph.nodes) < config.parallel_threshold:
            return XCSNoOpScheduler()

        # Count potentially parallelizable nodes
        parallel_nodes = _count_parallelizable_nodes(graph)
        return (
            TopologicalSchedulerWithParallelDispatch(max_workers=config.max_workers)
            if parallel_nodes >= 2
            else XCSNoOpScheduler()
        )

    # Invalid strategy
    raise ValueError(
        f"Unknown execution strategy: {config.strategy}. "
        "Expected 'auto', 'parallel', or 'sequential'."
    )

def \_count_parallelizable_nodes(graph: XCSGraph) -> int:
"""Count nodes that could execute in parallel.

    Analyzes graph structure to identify potential parallelism.

    Args:
        graph: Graph to analyze

    Returns:
        Estimated count of parallelizable nodes
    """
    # Count nodes with no dependencies (root nodes)
    root_nodes = sum(1 for node in graph.nodes.values() if not node.inbound_edges)
    if root_nodes > 1:
        return root_nodes

    # Count nodes with only one dependency (could execute in parallel after the root)
    return sum(1 for node in graph.nodes.values() if len(node.inbound_edges) == 1)

# -----------------------------------------------------------------------------

# Operator Structure Analysis

# -----------------------------------------------------------------------------

@dataclass(frozen=True)
class OperatorStructureNode:
"""
Immutable representation of an operator in the structure graph.

    This class captures the essential information about an operator and its
    relationships to other operators in the composition structure.

    Attributes:
        operator: The actual operator instance
        node_id: Unique identifier for this node
        attribute_path: Dot-notation path to this operator from the root
        parent_id: ID of the parent node, or None for the root
        metadata: Dictionary for storing node-specific metadata
    """

    operator: Operator
    node_id: str
    attribute_path: str
    parent_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class OperatorStructureGraph:
"""
Graph representation of an operator's composition structure.

    Captures the hierarchical structure of operator composition by analyzing
    the operator's attribute hierarchy through the pytree protocol.

    Attributes:
        nodes: Dictionary mapping node IDs to OperatorStructureNode instances
        root_id: ID of the root node in the graph
    """

    nodes: Dict[str, OperatorStructureNode] = field(default_factory=dict)
    root_id: Optional[str] = None

def \_analyze_operator_structure(operator: Operator) -> OperatorStructureGraph:
"""Analyze operator composition structure.

    Identifies nested operators with their parent-child relationships.
    If the operator implements the StructureDependency protocol, uses
    the explicitly declared dependencies for more precise analysis.

    Args:
        operator: Root operator

    Returns:
        Operator structure graph
    """
    graph = OperatorStructureGraph()
    visited = set()
    logger = logging.getLogger("ember.xcs.tracer.structural_jit")

    # First check for explicit structural dependencies
    if isinstance(operator, StructureDependency) and hasattr(
        operator, "get_structural_dependencies"
    ):
        try:
            explicit_deps = operator.get_structural_dependencies()
            class_name = operator.__class__.__name__
            logger.debug(f"Using explicit structural dependencies for {class_name}")

            # Create the root node
            root_node_id = f"node_{id(operator)}"
            graph.nodes[root_node_id] = OperatorStructureNode(
                operator=operator, node_id=root_node_id, attribute_path="root"
            )
            graph.root_id = root_node_id

            # Add dependencies from the explicit declaration
            for attr_name, _ in explicit_deps.items():
                # Skip if we can't get the attribute
                if not hasattr(operator, attr_name):
                    continue

                # Add dependent operator if it exists
                attr_value = getattr(operator, attr_name)
                if isinstance(attr_value, Operator):
                    attr_node_id = f"node_{id(attr_value)}"
                    graph.nodes[attr_node_id] = OperatorStructureNode(
                        operator=attr_value,
                        node_id=attr_node_id,
                        attribute_path=f"root.{attr_name}",
                        parent_id=root_node_id,
                    )

            # If we successfully used explicit dependencies, return now
            if len(graph.nodes) > 1:  # More than just the root node
                return graph

            # Otherwise fall back to heuristic analysis
            logger.debug(
                "Explicit dependencies produced incomplete graph, "
                "falling back to heuristic analysis"
            )
        except Exception as e:
            logger.warning(
                f"Error using explicit structural dependencies: {e}. "
                f"Falling back to heuristic analysis."
            )

    # Reset graph for heuristic analysis
    graph = OperatorStructureGraph()
    visited = set()

    def visit(obj: Any, path: str, parent_id: Optional[str] = None) -> Optional[str]:
        """Recursively process object and its attributes.

        Args:
            obj: Current object
            path: Attribute path from root
            parent_id: Parent node ID

        Returns:
            Node ID if operator was added
        """
        # Skip primitives and None
        if obj is None or isinstance(obj, (str, int, float, bool, bytes)):
            return None

        # Skip cycles
        obj_id = id(obj)
        if obj_id in visited:
            return None

        # Mark as visited to prevent cycles
        visited.add(obj_id)

        # Add node if it's an operator
        node_id = None
        if isinstance(obj, Operator):
            node_id = f"node_{obj_id}"
            # Create the node
            node = OperatorStructureNode(
                operator=obj, node_id=node_id, attribute_path=path, parent_id=parent_id
            )

            # Capture type information from specification if available
            if hasattr(obj, "specification"):
                spec = obj.specification
                # Capture input model type for dict-to-model conversion
                if hasattr(spec, "input_model") and spec.input_model:
                    node.metadata["input_model"] = spec.input_model

                # Capture output model type for model-to-dict conversion
                if hasattr(spec, "structured_output") and spec.structured_output:
                    node.metadata["structured_output"] = spec.structured_output

            graph.nodes[node_id] = node

            # First node becomes root
            if graph.root_id is None:
                graph.root_id = node_id

        # Process attributes regardless of whether it's an operator
        # This is critical for nested operators!
        if hasattr(obj, "__dict__"):
            for attr_name, value in _get_attributes(obj):
                if attr_name.startswith("_"):
                    continue

                attr_path = f"{path}.{attr_name}"
                visit(value, attr_path, node_id or parent_id)

        # Process collections
        if isinstance(obj, dict):
            for key, value in obj.items():
                visit(value, f"{path}[{key}]", node_id or parent_id)
        elif isinstance(obj, (list, tuple)):
            for i, value in enumerate(obj):
                visit(value, f"{path}[{i}]", node_id or parent_id)

        return node_id

    # Start traversal from root
    visit(operator, "root")
    return graph

def \_get_attributes(obj: Any) -> List[Tuple[str, Any]]:
"""Get accessible attributes of an object.

    Extracts attributes that could potentially contain operators.

    Args:
        obj: Object to examine

    Returns:
        List of (name, value) tuples
    """
    # Start with instance variables
    attributes = list(getattr(obj, "__dict__", {}).items())

    # Add class variables for class objects
    if inspect.isclass(obj):
        for name in dir(obj):
            if not name.startswith("_"):
                try:
                    value = getattr(obj, name)
                    if not callable(value):
                        attributes.append((name, value))
                except Exception:
                    pass

    return attributes

# -----------------------------------------------------------------------------

# XCS Graph Building

# -----------------------------------------------------------------------------

def \_build_xcs_graph_from_structure(
operator: Operator,
structure: OperatorStructureGraph,
sample_input: Optional[Dict[str, Any]] = None,
) -> XCSGraph:
"""Build execution graph from operator structure.

    Creates a graph with nodes and edges based on the analyzed
    operator composition structure. Sets explicit output node metadata
    for deterministic result extraction.

    Args:
        operator: Root operator
        structure: Analyzed structure graph
        sample_input: Optional input for data flow analysis

    Returns:
        Executable XCS graph
    """
    graph = XCSGraph()

    # Add all operators as nodes with their metadata
    for node_id, node in structure.nodes.items():
        # Extract operator object
        operator_obj = node.operator

        # Add the node to the graph
        xcs_node = graph.add_node(operator=operator_obj, node_id=node_id)

        # Determine the actual metadata target
        metadata_target = None
        if not isinstance(xcs_node, str) and hasattr(xcs_node, "metadata"):
            metadata_target = xcs_node
        elif (
            isinstance(xcs_node, str)
            and node_id in graph.nodes
            and hasattr(graph.nodes[node_id], "metadata")
        ):
            metadata_target = graph.nodes[node_id]

        # Apply metadata if we have a valid target
        if metadata_target is not None:
            # Capture input_model from operator's specification
            if hasattr(operator_obj, "specification"):
                spec = operator_obj.specification
                if hasattr(spec, "input_model") and spec.input_model:
                    metadata_target.metadata["input_model"] = spec.input_model

            # Preserve any existing metadata
            if isinstance(node.metadata, dict):
                metadata_target.metadata.update(node.metadata)

    # Connect parent-child relationships
    for node_id, node in structure.nodes.items():
        if node.parent_id:
            graph.add_edge(from_id=node.parent_id, to_id=node_id)

    # Try to determine and set the output node for deterministic result extraction
    leaf_nodes = [
        node_id
        for node_id, node in structure.nodes.items()
        if node_id not in [edge.from_node for edge in graph.edges.values()]
    ]

    if leaf_nodes:
        # Use the root node if it's a leaf (single node graph)
        if structure.root_id in leaf_nodes:
            output_node_id = structure.root_id
        else:
            # Otherwise use the last leaf node in the list
            output_node_id = leaf_nodes[-1]

        # Set explicit output node ID metadata
        graph.metadata["output_node_id"] = output_node_id
        # Legacy metadata for backward compatibility
        graph.metadata["output_node"] = output_node_id

    return graph

# -----------------------------------------------------------------------------

# Execution & Caching

# -----------------------------------------------------------------------------

def \_execute_with_engine(
graph: XCSGraph,
inputs: Dict[str, Any],
config: ExecutionConfig,
) -> Dict[str, Any]:
"""Execute a graph using the XCS engine.

    Core execution method for structural JIT that handles graph execution
    with appropriate scheduling based on graph characteristics.

    Args:
        graph: Graph to execute
        inputs: Input data
        config: Execution configuration

    Returns:
        Execution results

    Raises:
        OperatorExecutionError: For errors in operator implementation
        Exception: For errors in graph execution machinery
    """
    logger = logging.getLogger("ember.xcs.tracer.structural_jit")

    # Get appropriate scheduler based on strategy and graph
    scheduler = get_scheduler(graph, config)
    scheduler_name = scheduler.__class__.__name__
    logger.debug(
        f"Executing graph with {len(graph.nodes)} nodes using {scheduler_name}"
    )

    try:
        # Compile and execute graph
        plan = compile_graph(graph=graph)
        results = scheduler.run_plan(
            plan=plan,
            global_input=inputs,
            graph=graph,
        )

        # Find appropriate output from results
        result = _extract_result(graph, results, logger)
        return result

    except Exception as e:
        # Handle execution errors
        from ember.core.exceptions import OperatorExecutionError

        # Propagate operator errors without recovery attempts
        if isinstance(e, (OperatorExecutionError, ValueError, TypeError, RuntimeError)):
            raise

        # For machinery errors, try to recover with cached result if available
        if hasattr(graph, "original_result") and graph.original_result is not None:
            logger.debug(f"Recovering from JIT error: {str(e)}")
            return graph.original_result

        # Cannot recover - re-raise the original exception
        raise

def \_extract_result(
graph: XCSGraph, results: Dict[str, Any], logger: logging.Logger
) -> Dict[str, Any]:
"""Extract the appropriate result from graph execution output.

    Uses a deterministic prioritized approach to identify the output value.
    Applies explicit metadata markers first, falling back to structural analysis
    when explicit markers aren't available.

    Args:
        graph: The executed graph
        results: Execution results for all nodes
        logger: Logger for debug messages

    Returns:
        The extracted result
    """
    # Priority 1: Explicit metadata markers
    if "output_node_id" in graph.metadata:
        node_id = graph.metadata["output_node_id"]
        if node_id in results:
            logger.debug(f"Using explicit output_node_id: {node_id}")
            return results[node_id]
        logger.warning(f"Output node '{node_id}' not found in results")

    if "original_operator" in results:
        logger.debug("Using original_operator node")
        return results["original_operator"]

    if "output_node" in graph.metadata and graph.metadata["output_node"] in results:
        node_id = graph.metadata["output_node"]
        logger.debug(f"Using legacy output_node: {node_id}")
        return results[node_id]

    # Priority 2: Structural inference
    # Simple case: single node graph
    if len(graph.nodes) == 1:
        node_id = next(iter(graph.nodes.keys()))
        if node_id in results:
            logger.debug(f"Using only node: {node_id}")
            return results[node_id]

    # Get leaf nodes (terminal outputs)
    leaf_nodes = [
        node_id for node_id, node in graph.nodes.items() if not node.outbound_edges
    ]

    # Single leaf node is unambiguous
    if len(leaf_nodes) == 1 and leaf_nodes[0] in results:
        logger.debug(f"Using single leaf node: {leaf_nodes[0]}")
        return results[leaf_nodes[0]]

    # Multiple identical leaf results
    if leaf_nodes:
        available_results = [
            (node, results[node]) for node in leaf_nodes if node in results
        ]
        if available_results and all(
            r[1] == available_results[0][1] for r in available_results
        ):
            logger.debug(
                f"Using identical result from {len(available_results)} leaf nodes"
            )
            return available_results[0][1]

    # Priority 3: Recovery mechanisms
    if hasattr(graph, "original_result") and graph.original_result is not None:
        logger.debug("Using cached original result")
        return graph.original_result

    # Last resort: return all results
    logger.debug("Could not determine specific output node, returning all results")
    return results

# -----------------------------------------------------------------------------

# JIT Decorator Implementation

# -----------------------------------------------------------------------------

def structural_jit(
func: Optional[Type[OpT]] = None,
\*,
execution_strategy: str = "auto",
parallel_threshold: int = 5,
max_workers: Optional[int] = None,
cache_graph: bool = True,
) -> Union[Callable[[Type[OpT]], Type[OpT]], Type[OpT]]:
"""Structure-based JIT optimization for operators.

    Analyzes operator composition structure to build optimized execution graphs
    without runtime tracing. Automatically identifies parallelization opportunities.

    Args:
        func: Operator class to decorate
        execution_strategy: Execution approach:
            - "auto": Select based on graph analysis
            - "parallel": Force parallel execution
            - "sequential": Force sequential execution
        parallel_threshold: Minimum nodes for parallelization in auto mode
        max_workers: Maximum concurrent workers for parallel execution
        cache_graph: Whether to cache graphs for repeated execution

    Returns:
        Decorated operator class with optimized execution

    Example:
        ```python
        @structural_jit
        class MyOperator(Operator):
            def __init__(self):
                self.op1 = SubOperator1()
                self.op2 = SubOperator2()

            def forward(self, *, inputs):
                intermediate = self.op1(inputs=inputs)
                return self.op2(inputs=intermediate)
        ```
    """

    def decorator(cls: Type[OpT]) -> Type[OpT]:
        """Inner decorator applied to operator class."""
        # Verify interface compatibility
        if not callable(cls) or not callable(cls.__call__):
            raise TypeError("@structural_jit requires a class with __call__ method")

        # Create execution config once
        execution_config = ExecutionConfig(
            strategy=execution_strategy,
            parallel_threshold=parallel_threshold,
            max_workers=max_workers,
        )

        # Save original methods
        original_init = cls.__init__
        original_call = cls.__call__

        @functools.wraps(original_init)
        def init_wrapper(self: OpT, *args: Any, **kwargs: Any) -> None:
            """Wrapped initialization with structure analysis."""
            # Initialize operator
            original_init(self, *args, **kwargs)

            # JIT properties
            self._jit_enabled = True
            self._jit_config = execution_config
            self._jit_cache_graph = cache_graph

            # Analyze structure during initialization
            self._jit_structure_graph = _analyze_operator_structure(self)
            self._jit_xcs_graph = None

        @functools.wraps(original_call)
        def call_wrapper(self: OpT, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
            """Wrapped execution with graph-based optimization."""
            # Handle disabled JIT
            if getattr(self, "_jit_enabled", True) is False:
                return original_call(self, inputs=inputs)

            # Prevent infinite recursion
            if getattr(self, "_jit_in_execution", False):
                return original_call(self, inputs=inputs)

            try:
                # Set recursion guard
                self._jit_in_execution = True

                # Get state signature if available
                state_signature = None
                if isinstance(self, StructureDependency) and hasattr(
                    self, "get_structure_signature"
                ):
                    try:
                        state_signature = self.get_structure_signature()
                    except Exception as e:
                        logger.warning(f"Error getting structure signature: {e}")

                # Try to get cached graph with state validation
                graph = None
                if self._jit_cache_graph:
                    graph = _structural_jit_cache.get_with_state(self, state_signature)

                # Use cached graph if available
                if graph is not None:
                    # Measure execution time for metrics
                    execution_start = time.time()
                    result = _execute_with_engine(
                        graph=graph,
                        inputs=inputs,
                        config=self._jit_config,
                    )
                    execution_duration = time.time() - execution_start
                    _structural_jit_cache.metrics.record_execution(execution_duration)
                    return result

                # First call - build the graph
                # Get original results
                original_result = original_call(self, inputs=inputs)

                # Measure compilation time for metrics
                compilation_start = time.time()

                # Build and configure graph
                structure = self._jit_structure_graph
                if structure is None:
                    # Just in case structure wasn't analyzed during init
                    structure = _analyze_operator_structure(self)
                    self._jit_structure_graph = structure

                graph = _build_xcs_graph_from_structure(
                    operator=self,
                    structure=structure,
                    sample_input=inputs,
                )

                # Save original result and add original operator node
                graph.original_result = original_result
                graph.add_node(
                    operator=original_call.__get__(self),
                    node_id="original_operator",
                )

                # Record compilation time
                compilation_duration = time.time() - compilation_start
                _structural_jit_cache.metrics.record_compilation(compilation_duration)

                # Cache the graph with state signature
                if self._jit_cache_graph:
                    _structural_jit_cache.set(self, graph, state_signature)

                # Update instance variable for backward compatibility
                self._jit_xcs_graph = graph

                return original_result
            finally:
                self._jit_in_execution = False

        # Replace methods with wrapped versions
        cls.__init__ = cast(Callable, init_wrapper)
        cls.__call__ = cast(Callable, call_wrapper)

        # Add control utilities
        cls.disable_jit = lambda self: setattr(self, "_jit_enabled", False)
        cls.enable_jit = lambda self: setattr(self, "_jit_enabled", True)
        cls.clear_graph_cache = lambda self: (
            _structural_jit_cache.invalidate(self),
            setattr(self, "_jit_xcs_graph", None),
        )
        cls.get_jit_metrics = lambda self: _structural_jit_cache.get_metrics()

        return cls

    # Handle both @structural_jit and @structural_jit(...) syntax
    return decorator(func) if func is not None else decorator

# -----------------------------------------------------------------------------

# Context Manager for Testing

# -----------------------------------------------------------------------------

@contextmanager
def disable_structural_jit() -> None:
"""
Context manager that temporarily disables structural JIT for testing.

    This utility is primarily intended for testing and debugging scenarios
    where you need to compare behavior with and without JIT optimization.

    Example:
        with disable_structural_jit():
            # JIT-decorated operators will run without optimization
            result = my_operator(inputs=test_input)
    """
    # Save all decorated operators we find
    operators = []

    # Find all objects in memory that have _jit_enabled attribute
    import gc

    for obj in gc.get_objects():
        if hasattr(obj, "_jit_enabled"):
            operators.append(obj)
            obj._jit_enabled = False

    try:
        yield
    finally:
        # Restore JIT state
        for op in operators:
            op._jit_enabled = True

</code>

src\ember\xcs\tracer\tracer_decorator.py:
<code>
"""
JIT Compilation and Execution Tracing for XCS Operators

This module provides a just-in-time (JIT) compilation system for Ember operators
through execution tracing. The @jit decorator transforms operator classes by
instrumenting them to record their execution patterns and automatically compile
optimized execution plans.

Key features:

1. Transparent operator instrumentation via the @jit decorator
2. Automatic execution graph construction from traced operator calls
3. Compile-once, execute-many optimization for repeated operations
4. Support for pre-compilation with sample inputs
5. Configurable tracing and caching behaviors

Implementation follows functional programming principles where possible,
separating concerns between tracing, compilation, and execution. The design
adheres to the Open/Closed Principle by extending operator behavior without
modifying their core implementation.

Example:
@jit
class MyOperator(Operator):
def **call**(self, \*, inputs): # Complex, multi-step computation
return result

    # First call triggers tracing and compilation
    op = MyOperator()
    result1 = op(inputs={"text": "example"})

    # Subsequent calls reuse the compiled execution plan
    result2 = op(inputs={"text": "another example"})

"""

from **future** import annotations

import dataclasses
import functools
import logging
import time
from typing import Any, Callable, Dict, Generic, Optional, Set, Type, TypeVar, Union, cast

# Import the base classes carefully to avoid circular imports

from ember.xcs.tracer.xcs_tracing import TracerContext

# We need to use a string for the bound to avoid circular imports

# Type variable for Operator subclasses

OperatorType = TypeVar("OperatorType", bound="Operator")

# Type alias for the decorator function's return type

OperatorDecorator = Callable[[Type[OperatorType]], Type[OperatorType]]

# Use a Protocol for Operator to avoid circular imports

from typing import Protocol, runtime_checkable

@runtime_checkable
class Operator(Protocol):
"""Protocol defining the expected interface for Operators."""

    def __call__(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the operator with provided inputs."""
        ...

@runtime_checkable
class StateDependency(Protocol):
"""Protocol for operators to declare state dependencies."""

    def get_state_signature(self) -> str:
        """Return a signature representing the current state.

        When this signature changes, cached JIT compilations should be invalidated.
        This could be a hash of state variables or a version number that
        the operator increments when state changes.
        """
        ...

    def get_state_dependencies(self) -> Set[object]:
        """Return set of objects this operator's behavior depends on.

        This is used to identify other objects that might affect this
        operator's behavior, allowing for more sophisticated cache
        invalidation strategies.
        """
        ...

import weakref

# Forward import execution components to avoid circular imports

from ember.xcs.graph.xcs_graph import XCSGraph

# Type variable for cache value type

T = TypeVar("T")

@dataclasses.dataclass
class JITMetrics:
"""Performance metrics for JIT compilation and execution.

    Tracks timing and cache statistics for analyzing JIT system performance.
    All times are in seconds.
    """

    # Timing metrics
    compilation_time: float = 0.0
    execution_time: float = 0.0
    tracing_time: float = 0.0

    # Cache metrics
    cache_hits: int = 0
    cache_misses: int = 0

    @property
    def cache_hit_ratio(self) -> float:
        """Ratio of cache hits to total cache lookups."""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0

    def record_compilation(self, duration: float) -> None:
        """Record time spent compiling a graph."""
        self.compilation_time += duration

    def record_execution(self, duration: float) -> None:
        """Record time spent executing a graph."""
        self.execution_time += duration

    def record_tracing(self, duration: float) -> None:
        """Record time spent tracing execution."""
        self.tracing_time += duration

    def record_cache_hit(self) -> None:
        """Record a cache hit."""
        self.cache_hits += 1

    def record_cache_miss(self) -> None:
        """Record a cache miss."""
        self.cache_misses += 1

    def reset(self) -> None:
        """Reset all metrics to zero."""
        self.compilation_time = 0.0
        self.execution_time = 0.0
        self.tracing_time = 0.0
        self.cache_hits = 0
        self.cache_misses = 0

    def __str__(self) -> str:
        """Human-readable performance summary."""
        return (
            f"JIT Performance Metrics:\n"
            f"  Compilation: {self.compilation_time:.6f}s\n"
            f"  Execution: {self.execution_time:.6f}s\n"
            f"  Tracing: {self.tracing_time:.6f}s\n"
            f"  Cache hit ratio: {self.cache_hit_ratio:.2%} ({self.cache_hits}/{self.cache_hits + self.cache_misses})"
        )

class JITCache(Generic[T]):
"""Thread-safe cache for JIT-compiled artifacts with proper lifecycle management."""

    def __init__(self) -> None:
        self._cache = weakref.WeakKeyDictionary()
        self._state_signatures = weakref.WeakKeyDictionary()
        self.metrics = JITMetrics()

    def get(self, key: object) -> Optional[T]:
        """Retrieve a cached item by key object (not id)."""
        return self._cache.get(key)

    def get_with_state(
        self, key: object, state_signature: Optional[str] = None
    ) -> Optional[T]:
        """Retrieve cached item, checking state signature if available."""
        if key not in self._cache:
            self.metrics.record_cache_miss()
            return None

        # If state signature provided, validate it matches
        if state_signature is not None:
            cached_signature = self._state_signatures.get(key)
            if cached_signature != state_signature:
                # State changed, invalidate cache entry
                self.invalidate(key)
                self.metrics.record_cache_miss()
                return None

        self.metrics.record_cache_hit()
        return self._cache.get(key)

    def set(self, key: object, value: T, state_signature: Optional[str] = None) -> None:
        """Store an item in the cache using the object itself as key."""
        self._cache[key] = value
        if state_signature is not None:
            self._state_signatures[key] = state_signature

    def invalidate(self, key: Optional[object] = None) -> None:
        """Invalidate specific entry or entire cache."""
        if key is not None:
            self._cache.pop(key, None)
            self._state_signatures.pop(key, None)
        else:
            self._cache.clear()
            self._state_signatures.clear()

    def __len__(self) -> int:
        """Return number of items in the cache."""
        return len(self._cache)

    def get_metrics(self, op=None) -> Union[JITMetrics, Dict[str, Any]]:
        """Get a copy of the current metrics or operator-specific metrics.

        Args:
            op: Optional operator instance. If provided, returns metrics specific
                to that operator's compiled function.

        Returns:
            Either a JITMetrics instance or a dictionary of operator-specific metrics.
        """
        if op is None:
            return dataclasses.replace(self.metrics)

        # Return operator-specific metrics as a dictionary
        if hasattr(self.metrics, 'function_metrics'):
            func_id = id(getattr(op, '_compiled_func', None))
            metrics_dict = self.metrics.function_metrics.get(func_id, {}).copy()

            # Include strategy information if available
            if hasattr(op, '_jit_strategy'):
                metrics_dict['strategy'] = op._jit_strategy

            return metrics_dict

        return {}

    def reset_metrics(self) -> None:
        """Reset metrics to initial state."""
        self.metrics.reset()

# Cache to store compiled execution graphs for each operator class instance

\_jit_cache = JITCache[XCSGraph]()

def jit(
func=None,
\*,
sample_input: Optional[Dict[str, Any]] = None,
force_trace: bool = False,
recursive: bool = True,
):
"""Just-In-Time compilation decorator for Ember Operators.

    The @jit decorator transforms Operator classes to automatically trace their execution
    and compile optimized execution plans. This brings significant performance benefits
    for complex operations and operator pipelines by analyzing the execution pattern
    once and reusing the optimized plan for subsequent calls.

    The implementation follows a lazily evaluated, memoization pattern:
    1. First execution triggers tracing to capture the full execution graph
    2. The traced operations are compiled into an optimized execution plan
    3. Subsequent calls reuse this plan without re-tracing (unless force_trace=True)

    Pre-compilation via sample_input is available for performance-critical paths where
    even the first execution needs to be fast. This implements an "eager" JIT pattern
    where compilation happens at initialization time rather than first execution time.

    Design principles:
    - Separation of concerns: Tracing, compilation, and execution are distinct phases
    - Minimal overhead: Non-tracing execution paths have negligible performance impact
    - Transparency: Decorated operators maintain their original interface contract
    - Configurability: Multiple options allow fine-tuning for different use cases

    Args:
        func: The function or class to be JIT-compiled. This is automatically passed when
             using the @jit syntax directly. If using @jit(...) with parameters, this will be None.
        sample_input: Optional pre-defined input for eager compilation during initialization.
                    This enables "compile-time" optimization rather than runtime JIT compilation.
                    Recommended for performance-critical initialization paths.
        force_trace: When True, disables caching and traces every invocation.
                    This is valuable for debugging and for operators whose execution
                    pattern varies significantly based on input values.
                    Performance impact: Significant, as caching benefits are disabled.
        recursive: Controls whether nested operator calls are also traced and compiled.
                 Currently limited to direct child operators observed during tracing.
                 Default is True, enabling full pipeline optimization.

    Returns:
        A decorated function/class or a decorator function that transforms the target
        Operator subclass by instrumenting its initialization and call methods for tracing.

    Raises:
        TypeError: If applied to a class that doesn't inherit from Operator.
                  The decorator strictly enforces type safety to prevent
                  incorrect usage on unsupported class types.

    Example:
        # Direct decoration (no parameters)
        @jit
        class SimpleOperator(Operator):
            def __call__(self, *, inputs):
                return process(inputs)

        # Parameterized decoration
        @jit(sample_input={"text": "example"})
        class ProcessorOperator(Operator):
            def __call__(self, *, inputs):
                # Complex multi-step process
                return {"result": processed_output}
    """

    def decorator(cls: Type[OperatorType]) -> Type[OperatorType]:
        """Internal decorator function applied to the Operator class.

        Args:
            cls: The Operator subclass to be instrumented.

        Returns:
            The decorated Operator class with tracing capabilities.

        Raises:
            TypeError: If cls is not an Operator subclass.
        """
        # More robust type checking that allows duck typing
        try:
            if not issubclass(cls, Operator):
                # Check for duck typing - if it has a __call__ method with the right signature
                if not (callable(cls) and callable(cls.__call__)):
                    raise TypeError(
                        "@jit decorator can only be applied to an Operator-like class with a __call__ method."
                    )
        except TypeError:
            # This handles the case where cls is not a class at all
            raise TypeError(
                "@jit decorator can only be applied to a class, not a function or other object."
            )

        original_call = cls.__call__
        original_init = cls.__init__

        @functools.wraps(original_init)
        def traced_init(self: OperatorType, *args: Any, **kwargs: Any) -> None:
            """Wrapped __init__ method that initializes the operator and pre-traces with sample input."""
            # Call the original __init__
            original_init(self, *args, **kwargs)

            # If sample_input is provided, perform pre-tracing during initialization
            if sample_input is not None:
                # Create a tracer context and trace the operator's execution
                with TracerContext() as tracer:
                    original_call(self=self, inputs=sample_input)

                if tracer.records:
                    # Import here to avoid circular imports
                    from ember.xcs.tracer.autograph import AutoGraphBuilder

                    # Build and cache the graph
                    graph_builder = AutoGraphBuilder()
                    graph = graph_builder.build_graph(tracer.records)

                    # Get state signature if available
                    state_signature = None
                    if hasattr(self, "get_state_signature") and callable(
                        self.get_state_signature
                    ):
                        state_signature = self.get_state_signature()

                    # Cache with the object itself as key and optional state signature
                    _jit_cache.set(self, graph, state_signature)

        @functools.wraps(original_call)
        def traced_call(self: OperatorType, *, inputs: Dict[str, Any]) -> Any:
            """Wrapped __call__ method with state-aware caching.

            Args:
                inputs: The input parameters for the operator.

            Returns:
                The output from the operator execution.
            """
            # Setup logging
            logger = logging.getLogger("ember.xcs.tracer.jit")

            # Get current tracer context
            tracer: Optional[TracerContext] = TracerContext.get_current()

            # For debugging and test purposes
            force_trace_local = getattr(self, "_force_trace", force_trace)

            # Check for state dependency protocol
            state_signature = None
            if hasattr(self, "get_state_signature") and callable(
                self.get_state_signature
            ):
                state_signature = self.get_state_signature()

            # Try to get cached graph with state validation
            graph = None
            if not force_trace_local:
                graph = _jit_cache.get_with_state(self, state_signature)

            # Phase 1: Try optimized execution with cached graph
            # -------------------------------------------------
            if graph is not None:
                try:
                    # Import execution components
                    from ember.xcs.engine.xcs_engine import (
                        TopologicalSchedulerWithParallelDispatch,
                        execute_graph,
                    )

                    logger.debug(
                        f"Using optimized graph for {self.__class__.__name__} (nodes: {len(graph.nodes)})"
                    )

                    # Execute the graph with the parallel scheduler and measure performance
                    execution_start = time.time()
                    results = execute_graph(
                        graph=graph,
                        global_input=inputs,
                        scheduler=TopologicalSchedulerWithParallelDispatch(),
                    )
                    execution_duration = time.time() - execution_start
                    _jit_cache.metrics.record_execution(execution_duration)

                    # Strict deterministic result extraction - no fallbacks
                    if "output_node_id" in graph.metadata:
                        output_node_id = graph.metadata["output_node_id"]
                        if output_node_id in results:
                            return results[output_node_id]
                        else:
                            raise ValueError(
                                f"Output node '{output_node_id}' specified in graph metadata but not found in results. "
                                f"Available nodes: {list(results.keys())}"
                            )
                    elif "output_node" in graph.metadata:
                        # Legacy compatibility
                        output_node = graph.metadata["output_node"]
                        if output_node in results:
                            logger.debug(
                                f"Using legacy output_node from graph metadata: {output_node}"
                            )
                            return results[output_node]

                    # If we got here with no explicit output node, try to determine the output
                    # This is a fallback for backward compatibility
                    logger.warning(
                        "Graph missing required 'output_node_id' metadata. "
                        "Using fallback strategies for backward compatibility."
                    )

                    # Strategy 1: Look for leaf nodes (nodes without outbound edges)
                    leaf_nodes = [
                        node_id
                        for node_id, node in graph.nodes.items()
                        if not node.outbound_edges
                    ]

                    if len(leaf_nodes) == 1:
                        # Single leaf node - clear choice for output
                        if leaf_nodes[0] in results:
                            return results[leaf_nodes[0]]

                    # Strategy 2: If there are multiple leaf nodes but all have identical results,
                    # arbitrarily choose one
                    if len(leaf_nodes) > 1:
                        first_result = results.get(leaf_nodes[0])
                        if all(
                            results.get(node) == first_result for node in leaf_nodes
                        ):
                            return first_result

                    # If all else fails, raise an error
                    raise ValueError(
                        "Could not determine output node. This indicates a bug in graph construction. "
                        f"Available nodes: {list(results.keys())}"
                    )

                except Exception as e:
                    # If graph execution fails, log the error and fall back to direct execution
                    logger.warning(
                        f"Error executing graph: {e}. Falling back to direct execution."
                    )

            # Phase 2: Tracing and direct execution
            # -------------------------------------------------
            # Initialize call tracking if in a tracer context
            call_id = None
            tracing_start = time.time()
            if tracer is not None:
                call_id = tracer.track_call(self, inputs)

            try:
                # Execute the original call directly
                execution_start = time.time()
                output = original_call(self=self, inputs=inputs)
                execution_duration = time.time() - execution_start
                _jit_cache.metrics.record_execution(execution_duration)

                # Complete the trace if we're tracing
                if tracer is not None and call_id is not None:
                    record = tracer.complete_call(call_id, output)

                    # Build and cache graph if appropriate
                    build_graph = (
                        force_trace_local
                        or not any(
                            r.node_id == str(id(self)) for r in tracer.records[:-1]
                        )
                        or len(tracer.records)
                        >= 3  # Minimum threshold for useful graph
                    )

                    if build_graph:
                        # Import here to avoid circular imports
                        from ember.xcs.tracer.autograph import AutoGraphBuilder

                        # Build a graph from the accumulated trace records
                        logger.debug(
                            f"Building graph from {len(tracer.records)} trace records"
                        )

                        # Measure compilation time
                        compilation_start = time.time()
                        graph_builder = AutoGraphBuilder()
                        graph = graph_builder.build_graph(tracer.records)
                        compilation_duration = time.time() - compilation_start
                        _jit_cache.metrics.record_compilation(compilation_duration)

                        # Only cache the graph if it has multiple nodes (otherwise no benefit)
                        if len(graph.nodes) > 1:
                            logger.debug(f"Caching graph with {len(graph.nodes)} nodes")
                            # Cache the graph with state signature if available
                            _jit_cache.set(self, graph, state_signature)
                        else:
                            logger.debug(
                                f"Not caching trivial graph with {len(graph.nodes)} nodes"
                            )

                # Record total tracing time if we traced
                if tracer is not None:
                    tracing_duration = time.time() - tracing_start
                    _jit_cache.metrics.record_tracing(tracing_duration)

                return output

            except Exception as e:
                # Complete the trace with the exception if we're tracing
                if tracer is not None and call_id is not None:
                    # Pass empty dict for outputs since execution failed
                    tracer.complete_call(call_id, {}, exception=e)

                # Re-raise the exception
                raise

        # Replace the original methods with our traced versions
        cls.__init__ = cast(Callable, traced_init)
        cls.__call__ = cast(Callable, traced_call)
        return cls

    # Handle both @jit and @jit(...) patterns
    if func is not None:
        # Called as @jit without parentheses
        return decorator(func)
    else:
        # Called with parameters as @jit(...)
        return decorator

# Removed \_build_graph_from_trace function since we're not implementing the enhanced

# JIT capability in this PR. This would be included in a future full implementation.

</code>

src\ember\xcs\tracer\unified_jit.py:
<code>
"""Unified JIT compilation interface for Ember operators.

Provides access to both trace-based and structure-based JIT strategies through
a single decorator. Handles mode selection and dispatches to specialized
implementations.

Example:
Default trace-based JIT:
`python
    @jit
    class SimpleOperator(Operator):
        def forward(self, *, inputs):
            return process(inputs)
    `

    JIT with explicit strategy:
    ```python
    @jit(mode="structural")
    class StructuredOperator(Operator):
        def forward(self, *, inputs):
            return process(inputs)
    ```

    Strategy-specific configuration:
    ```python
    @jit.trace(sample_input=sample_data)
    class TracedOperator(Operator):
        def forward(self, *, inputs):
            return complex_process(inputs)

    @jit.structural(execution_strategy="parallel")
    class ParallelOperator(Operator):
        def __init__(self):
            self.op1 = SubOperator()
            self.op2 = SubOperator()

        def forward(self, *, inputs):
            result1 = self.op1(inputs=inputs)
            result2 = self.op2(inputs=inputs)
            return combine(result1, result2)
    ```

"""

from **future** import annotations

from typing import Callable, Type, TypeVar, overload

from ember.xcs.tracer.structural_jit import structural_jit

# Import specialized JIT implementations

from ember.xcs.tracer.tracer_decorator import jit as trace_jit

# Type variable for operator classes

T = TypeVar("T")

@overload
def jit(func: Type[T]) -> Type[T]:
...

@overload
def jit(\*, mode: str = "trace") -> Callable[[Type[T]], Type[T]]:
...

def jit(func=None, \*, mode: str = "trace"):
"""Just-in-time compiler for operator optimization.

    Transforms operator classes for automatic graph-based execution.
    Selects between tracing and structural analysis strategies.

    Args:
        func: Operator class to decorate
        mode: JIT strategy:
            - "trace": Records and optimizes execution paths
            - "structural": Analyzes composition structure for parallelism

    Returns:
        Decorated operator class

    Raises:
        ValueError: If unknown mode specified
    """
    MODES = {"trace": trace_jit, "structural": structural_jit}

    def decorator(cls: Type[T]) -> Type[T]:
        if mode not in MODES:
            raise ValueError(
                f"Unknown JIT mode: {mode}. Valid options: {', '.join(MODES.keys())}"
            )
        return MODES[mode](cls)

    # Handle both @jit and @jit(...) syntax
    return decorator(func) if func is not None else decorator

# Direct access to specialized JIT implementations

jit.trace = trace_jit
jit.structural = structural_jit

# Legacy aliases for backward compatibility

trace_based_jit = trace_jit
structure_based_jit = structural_jit

</code>

src\ember\xcs\tracer\xcs_tracing.py:
<code>
"""
Tracing Module for XCS.

This module provides a context manager for tracing operator executions and recording
trace records.
"""

from **future** import annotations

import threading
import time
from contextlib import ContextDecorator
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Type

@dataclass(frozen=False) # Allow modifications to instances
class TraceRecord:
"""Record of a single operator invocation with complete lifecycle information.

    Attributes:
        operator_name (str): Name of the operator.
        node_id (str): Unique identifier for this specific invocation.
        instance_id (str): Identifies the operator instance (from id(operator)).
        inputs (Dict[str, Any]): The inputs passed to the operator.
        outputs (Any): The outputs returned by the operator.
        start_time (float): The time at which the operator started execution.
        end_time (float): The time at which the operator finished execution.
        graph_node_id (Optional[str]): ID used in the graph representation, for autograph internals.
        operator (Any): The operator instance that was called.
        exception (Optional[Exception]): Exception raised during execution, if any.
        input_type_paths (Dict[str, str]): Type paths for EmberModel inputs.
        output_type_paths (Dict[str, str]): Type paths for EmberModel outputs.
    """

    operator_name: str
    node_id: str
    inputs: Dict[str, Any]
    outputs: Any
    instance_id: str = field(default="")
    start_time: float = field(default_factory=time.time)
    end_time: float = field(default_factory=time.time)
    graph_node_id: Optional[str] = None
    operator: Any = None
    exception: Optional[Exception] = None
    input_type_paths: Dict[str, str] = field(default_factory=dict)
    output_type_paths: Dict[str, str] = field(default_factory=dict)

    def __post_init__(self):
        """Extract and store type information from inputs/outputs."""
        # Extract from inputs
        from ember.core.types.ember_model import EmberModel

        if isinstance(self.inputs, dict):
            for key, value in self.inputs.items():
                if isinstance(value, EmberModel):
                    model_type = type(value)
                    self.input_type_paths[
                        key
                    ] = f"{model_type.__module__}.{model_type.__qualname__}"

        # Extract from outputs
        if isinstance(self.outputs, dict):
            for key, value in self.outputs.items():
                if isinstance(value, EmberModel):
                    model_type = type(value)
                    self.output_type_paths[
                        key
                    ] = f"{model_type.__module__}.{model_type.__qualname__}"

    @property
    def timestamp(self) -> float:
        """Backward compatibility for legacy code using timestamp."""
        return self.end_time

    @property
    def duration(self) -> float:
        """Execution duration in seconds."""
        return self.end_time - self.start_time

    @property
    def succeeded(self) -> bool:
        """Whether the call completed successfully."""
        return self.exception is None

class TracerContext(ContextDecorator):
"""Context manager for tracing operator executions.

    When active, operator invocations may record their execution details to the active
    context. The active context is stored in thread-local storage to support safe concurrent use.

    Attributes:
        records (List[TraceRecord]): List of recorded operator invocation traces.
        active_calls: Dictionary mapping call IDs to tracked operator calls in progress
        is_active: Whether this context is currently active
    """

    _local = threading.local()

    def __init__(self) -> None:
        """Initializes a new TracerContext with an empty trace record list."""
        self.records: List[TraceRecord] = []
        self.is_active: bool = False
        self.active_calls: Dict[str, Dict[str, Any]] = {}

    def __enter__(self) -> TracerContext:
        """Enters the tracing context, setting it as the current active context.

        Returns:
            TracerContext: The active tracing context.
        """
        self._set_current(self)
        self.is_active = True
        return self

    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_value: Optional[BaseException],
        traceback: Optional[Any],
    ) -> Optional[bool]:
        """Exits the tracing context, clearing the active context.

        Args:
            exc_type (Optional[Type[BaseException]]): Exception type, if any.
            exc_value (Optional[BaseException]): Exception value, if any.
            traceback (Optional[Any]): Traceback, if any.

        Returns:
            Optional[bool]: None.
        """
        self.is_active = False
        self._clear_current()
        return None

    def add_record(self, *, record: TraceRecord) -> None:
        """Adds a trace record to the current context.

        Args:
            record (TraceRecord): The trace record to add.
        """
        self.records.append(record)

    def track_call(self, operator: Any, inputs: Dict[str, Any]) -> str:
        """Begin tracking an operator call.

        Args:
            operator: The operator instance being called
            inputs: Input parameters to the operator

        Returns:
            call_id: Unique identifier for this invocation
        """
        import uuid

        call_id = str(uuid.uuid4())
        instance_id = str(id(operator))

        # Store in active calls dictionary
        self.active_calls[call_id] = {
            "instance_id": instance_id,
            "operator": operator,
            "operator_name": getattr(operator, "name", operator.__class__.__name__),
            "inputs": inputs,
            "start_time": time.time(),
        }

        return call_id

    def complete_call(
        self,
        call_id: str,
        outputs: Dict[str, Any],
        exception: Optional[Exception] = None,
    ) -> TraceRecord:
        """Complete a tracked call, with optional exception.

        Args:
            call_id: The call ID returned from track_call
            outputs: The outputs from the operator execution
            exception: Exception raised during execution, if any

        Returns:
            The completed TraceRecord
        """
        if call_id not in self.active_calls:
            raise ValueError(f"Unknown call_id: {call_id}")

        call_data = self.active_calls.pop(call_id)

        # Create and store the complete record
        record = TraceRecord(
            instance_id=call_data["instance_id"],
            node_id=call_id,
            operator_name=call_data["operator_name"],
            operator=call_data["operator"],
            inputs=call_data["inputs"],
            outputs=outputs,
            start_time=call_data["start_time"],
            end_time=time.time(),
            exception=exception,
        )

        self.records.append(record)
        return record

    def get_call(self, call_id: str) -> Optional[Dict[str, Any]]:
        """Get a tracked call by ID.

        Args:
            call_id: The call ID to look up

        Returns:
            The active call data, or None if not found
        """
        return self.active_calls.get(call_id)

    @classmethod
    def get_current(cls) -> Optional[TracerContext]:
        """Retrieves the current active tracing context.

        Returns:
            Optional[TracerContext]: The active TracerContext, or None if none is active.
        """
        return getattr(cls._local, "current", None)

    def _set_current(self, ctx: TracerContext) -> None:
        """Sets the current tracing context in thread-local storage.

        Args:
            ctx (TracerContext): The tracer context to set as current.
        """
        type(self)._local.current = ctx

    def _clear_current(self) -> None:
        """Clears the current tracing context from thread-local storage."""
        type(self)._local.current = None

def get_tracing_context() -> Optional[TracerContext]:
"""Get the current active tracing context.

    This is a helper function that simply delegates to TracerContext.get_current()
    for convenience.

    Returns:
        The current active tracing context, or None if no context is active.
    """
    # Make sure there's always a context available
    context = TracerContext.get_current()
    if context is None:
        context = TracerContext()
        context._set_current(context)
    return context

# Dictionary to store original implementations when patching

\_ORIGINAL_METHODS: Dict[int, Any] = {}

def patch_operator(operator: Any, new_method: Any) -> None:
"""Replace an operator's **call** method with a new implementation.

    This function is primarily used for testing and debugging purposes
    to intercept operator calls.

    Args:
        operator: The operator to patch
        new_method: The new __call__ method implementation
    """
    # Store the original method
    operator_id = id(operator)
    _ORIGINAL_METHODS[operator_id] = operator.__call__

    # Apply the patch
    operator.__call__ = new_method

def restore_operator(operator: Any) -> None:
"""Restore an operator's original **call** method after patching.

    Args:
        operator: The operator to restore
    """
    operator_id = id(operator)
    if operator_id in _ORIGINAL_METHODS:
        # Restore the original method
        operator.__call__ = _ORIGINAL_METHODS[operator_id]
        # Clean up the reference
        del _ORIGINAL_METHODS[operator_id]

</code>

src\ember\xcs\tracer_context_types.py:
<code>
"""Types for XCS execution tracing.

Defines the type system for tracking and propagating trace information through
the XCS execution pipeline. Provides structured metadata for debugging, profiling,
and analysis.
"""

from typing import Dict, Generic, TypeVar

from typing_extensions import NotRequired, TypedDict

class TraceMetadata(TypedDict, total=False):
"""Schema for execution trace metadata.

    Attributes:
        source_file: Path to the source file where the trace originated
        source_line: Line number within source_file
        trace_id: Unique identifier for this trace instance
        parent_trace_id: Reference to parent trace for hierarchical tracing
        timestamp: Creation time (Unix timestamp)
        execution_time: Duration in seconds
        memory_usage: Peak memory usage in bytes
        custom_attributes: Dictionary for domain-specific metadata
    """

    source_file: NotRequired[str]
    source_line: NotRequired[int]
    trace_id: NotRequired[str]
    parent_trace_id: NotRequired[str]
    timestamp: NotRequired[float]
    execution_time: NotRequired[float]
    memory_usage: NotRequired[int]
    custom_attributes: NotRequired[Dict[str, object]]

T = TypeVar("T", bound=TraceMetadata)

class TraceContextData(Generic[T]):
"""Container for trace context data with type guarantees.

    Args:
        extra_info: Metadata dictionary conforming to TraceMetadata schema
    """

    def __init__(self, extra_info: T) -> None:
        self.extra_info = extra_info

</code>

src\ember\xcs\tracer\_\_init\_\_.py:
<code>
"""
Tracing and Graph Building Infrastructure for XCS.

Provides tracing context management and data collection for building
computational graphs from execution traces. This module is a core
component of the XCS system that enables automatic graph construction.
"""

from ember.xcs.tracer.\_context_types import TraceContextData
from ember.xcs.tracer.autograph import AutoGraphBuilder, autograph
from ember.xcs.tracer.xcs_tracing import TracerContext, TraceRecord

**all** = [
# Core tracing system
"TraceRecord",
"TraceContextData",
"TracerContext",
# Graph building
"AutoGraphBuilder",
"autograph",
]

</code>

src\ember\xcs\exceptions.py:
<code>
"""
Exception hierarchy for the XCS module.

This module defines a structured hierarchy of exceptions for the XCS system, enabling
more precise error handling and better diagnostics.

NOTE: This module re-exports exception classes from ember.core.exceptions
with API compatibility for backward compatibility.
"""

import logging
from typing import Any, Dict, Optional

from ember.core.exceptions import CompilationError as CoreCompilationError
from ember.core.exceptions import DataFlowError as CoreDataFlowError
from ember.core.exceptions import ExecutionError as CoreExecutionError
from ember.core.exceptions import ParallelExecutionError as CoreParallelExecutionError
from ember.core.exceptions import SchedulerError as CoreSchedulerError
from ember.core.exceptions import TraceError as CoreTraceError
from ember.core.exceptions import TransformError as CoreTransformError
from ember.core.exceptions import XCSError as CoreXCSError

# API Compatibility wrapper

class XCSError(CoreXCSError):
"""Base class for all XCS-related exceptions."""

    def __init__(self, message: str = "An error occurred in the XCS system"):
        super().__init__(message=message)
        # For backward compatibility - diagnostic_context is now self.context
        self.diagnostic_context = self.context

    def add_context(self, **kwargs: Any) -> None:
        """Adding diagnostic context to the exception.

        Storing additional metadata with the exception for improved
        traceability and debugging.

        Args:
            **kwargs: Key-value pairs to add to the diagnostic context.
        """
        super().add_context(**kwargs)
        # Keep diagnostic_context in sync with self.context
        self.diagnostic_context = self.context

    def get_context_data(self) -> Dict[str, Any]:
        """Retrieving the diagnostic context data.

        Returns:
            Dictionary containing all diagnostic context for this exception.
        """
        return self.get_context()

class TraceError(CoreTraceError, XCSError):
"""Raised when an error occurs during tracing operations."""

    def __init__(
        self,
        message: str = "Error during execution tracing",
        operation_id: Optional[str] = None,
    ):
        super().__init__(message=message)
        if operation_id:
            self.add_context(operation_id=operation_id)

class CompilationError(CoreCompilationError, XCSError):
"""Raised when an error occurs during graph compilation."""

    def __init__(
        self,
        message: str = "Error during graph compilation",
        graph_id: Optional[str] = None,
    ):
        super().__init__(message=message)
        if graph_id:
            self.add_context(graph_id=graph_id)

class ExecutionError(CoreExecutionError, XCSError):
"""Raised when an error occurs during graph execution."""

    def __init__(
        self,
        node_id: Optional[str] = None,
        message: str = "Error during graph execution",
        cause: Optional[Exception] = None,
        **context_data: Any,
    ):
        self.node_id = node_id
        node_msg = f" in node '{node_id}'" if node_id else ""
        full_message = f"{message}{node_msg}"

        super().__init__(message=full_message, cause=cause)

        # Add standard diagnostic context
        if node_id:
            self.add_context(node_id=node_id)
        # Add any additional context provided
        if context_data:
            self.add_context(**context_data)

class TransformError(CoreTransformError, XCSError):
"""Raised when an error occurs with XCS transforms."""

    def __init__(
        self,
        transform_name: Optional[str] = None,
        message: str = "Error in XCS transform",
        cause: Optional[Exception] = None,
        details: Optional[Dict[str, Any]] = None,
        **context_data: Any,
    ):
        self.transform_name = transform_name
        transform_msg = f" in transform '{transform_name}'" if transform_name else ""
        full_message = f"{message}{transform_msg}"

        super().__init__(message=full_message, cause=cause)

        # Add standard diagnostic context
        if transform_name:
            self.add_context(transform_name=transform_name)
        if details:
            self.add_context(**details)
        # Add any additional context provided
        if context_data:
            self.add_context(**context_data)

    def log_with_context(
        self, logger: logging.Logger, level: int = logging.ERROR
    ) -> None:
        """Logging the error with its full diagnostic context.

        Creating a structured log entry that includes all diagnostic context
        for enhanced error tracing and analysis.

        Args:
            logger: Logger to use for recording the error
            level: Logging level (default: ERROR)
        """
        super().log_with_context(logger, level)

class ParallelExecutionError(CoreParallelExecutionError, ExecutionError):
"""Raised when parallel execution fails."""

    def __init__(
        self,
        node_id: Optional[str] = None,
        message: str = "Error during parallel execution",
        cause: Optional[Exception] = None,
        worker_id: Optional[str] = None,
        **context_data: Any,
    ):
        # Add worker-specific context for parallel execution errors
        super_context = dict(context_data)
        if worker_id:
            super_context["worker_id"] = worker_id

        super().__init__(node_id, message, cause, **super_context)

class DataFlowError(CoreDataFlowError, XCSError):
"""Raised when there is an error in data flow analysis or processing."""

    def __init__(
        self,
        message: str = "Error in data flow",
        graph_id: Optional[str] = None,
        source_node: Optional[str] = None,
        target_node: Optional[str] = None,
    ):
        super().__init__(message=message)

        # Add data flow specific context
        context = {}
        if graph_id:
            context["graph_id"] = graph_id
        if source_node:
            context["source_node"] = source_node
        if target_node:
            context["target_node"] = target_node

        if context:
            self.add_context(**context)

class SchedulerError(CoreSchedulerError, XCSError):
"""Raised when there is an error in the XCS execution scheduler."""

    def __init__(
        self,
        message: str = "Error in XCS scheduler",
        graph_id: Optional[str] = None,
        scheduler_type: Optional[str] = None,
    ):
        super().__init__(message=message)

        # Add scheduler specific context
        context = {}
        if graph_id:
            context["graph_id"] = graph_id
        if scheduler_type:
            context["scheduler_type"] = scheduler_type

        if context:
            self.add_context(**context)

**all** = [
"XCSError",
"TraceError",
"CompilationError",
"ExecutionError",
"TransformError",
"ParallelExecutionError",
"DataFlowError",
"SchedulerError",
]

</code>

src\ember\xcs\README.md:
<code>

# Ember XCS: Unified Execution Framework

XCS (Accelerated Compound Systems) provides a high-performance distributed execution framework for computational graphs. It implements a directed acyclic graph (DAG) architecture for operator composition, intelligent scheduling, and just-in-time compilation.

## Architecture

XCS follows a clean, unified architecture with stratified layers:

1. **Protocol Layer**: Core interfaces defining component contracts
2. **Strategy Layer**: Pluggable strategy implementations for each component
3. **Implementation Layer**: Concrete implementations with consistent interfaces
4. **Facade Layer**: Simplified public API abstracting implementation details

## Key Components

### JIT Compilation

The JIT system combines multiple compilation strategies under a consistent interface:

```python
from ember.xcs import jit

# Simple usage with automatic strategy selection
@jit
class MyOperator(Operator):
    def forward(self, *, inputs):
        return {"result": process(inputs["data"])}

# Parameterized usage with explicit strategy
@jit(mode="enhanced", sample_input={"query": "example"})
class CompositeOperator(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()

    def forward(self, *, inputs):
        intermediate = self.op1(inputs=inputs)
        return self.op2(inputs=intermediate)
````

### Dependency Analysis

Provides unified dependency tracking and analysis for all graph operations:

- Efficient transitive closure calculation
- Topological sorting with cycle detection
- Execution wave calculation for parallel scheduling

### Execution Scheduling

Unified scheduler implementations share a common interface and strategy pattern:

```python
from ember.xcs import jit, execution_options, create_scheduler

# Using the JIT decorator with explicit execution options
@jit
class MyOperator(Operator):
    def forward(self, *, inputs):
        return {"result": process(inputs["data"])}

# Create an instance of the operator
op = MyOperator()

# Control execution with context manager
with execution_options(scheduler="wave", max_workers=4):
    results = op(inputs={"query": "example"})

# Or create a custom scheduler directly
scheduler = create_scheduler("parallel", max_workers=8)
```

### Function Transformations

High-level operations for batching and parallelization:

```python
from ember.xcs import vmap, pmap, pjit, compose

# Vectorizing a function for batch processing
batch_process = vmap(process_item)
batch_results = batch_process(inputs={"data": ["item1", "item2", "item3"]})

# Parallelizing execution across multiple workers
parallel_process = pmap(process_item, num_workers=4)
parallel_results = parallel_process(inputs={"data": large_dataset})

# Combining transformations
vectorized_parallel = compose(
    vmap(batch_size=32),
    pmap(num_workers=4)
)
optimized_fn = vectorized_parallel(process_item)

# Using combined JIT+parallel transformation
optimized_fn = pjit(process_item, mode="enhanced")
```

## Architectural Components

XCS is organized into the following key packages:

- **jit/**: JIT compilation system with pluggable strategies

  - **strategies/**: Different JIT compilation approaches (trace, structural, enhanced)
  - **core.py**: Main JIT decorator implementation
  - **cache.py**: Caching mechanism for compiled functions

- **schedulers/**: Unified execution scheduler system

  - **base_scheduler.py**: Core scheduler interface
  - **unified_scheduler.py**: Concrete scheduler implementations
  - **factory.py**: Factory for creating appropriate schedulers

- **graph/**: Graph representation and dependency analysis

  - **xcs_graph.py**: Core graph data structure
  - **dependency_analyzer.py**: Dependency tracking and analysis
  - **graph_builder.py**: Graph construction from traces

- **engine/**: Unified execution engine

  - **unified_engine.py**: Core execution functionality
  - **execution_options.py**: Execution configuration

- **transforms/**: Function transformations

  - **transform_base.py**: Shared foundation for all transforms
  - **vmap.py**: Vectorization implementation
  - **pmap.py**: Parallelization implementation
  - **mesh.py**: Device mesh-based sharding

- **common/**: Shared data structures

  - **plans.py**: Execution plan representations

- **tracer/**: Tracing infrastructure
  - **xcs_tracing.py**: Core tracing functionality
  - **autograph.py**: Automatic graph building

## Extension Points

XCS is designed for extensibility via clearly defined protocols:

- Create custom schedulers by implementing `BaseScheduler` or extending `BaseSchedulerImpl`
- Add new JIT strategies by implementing the `Strategy` protocol
- Implement custom graph transformations by extending `BaseTransformation`
- Define custom execution policies using the execution options system

For more examples, see the `examples/` directory.
</code>

src\ember\xcs\_\_init\_\_.py:
<code>
"""XCS: Ember Execution Framework.

Provides a high-performance computational engine for building, optimizing,
and executing complex operator pipelines. Core features include automatic
parallelization, just-in-time compilation, and intelligent scheduling.
"""

from ember.xcs.api.types import ExecutionResult as APIExecutionResult
from ember.xcs.api.types import JITOptions, TransformOptions, XCSExecutionOptions

# Public API - execution results and plans

from ember.xcs.common.plans import ExecutionResult, XCSPlan, XCSTask
from ember.xcs.engine.execution_options import ExecutionOptions

# Implementation components - execution

# Public API - execution control

from ember.xcs.engine.unified_engine import (
ExecutionMetrics,
GraphExecutor,
execute_graph,
execution_options,
)
from ember.xcs.graph.dependency_analyzer import DependencyAnalyzer
from ember.xcs.graph.graph_builder import EnhancedTraceGraphBuilder, GraphBuilder

# Implementation components - graph system

from ember.xcs.graph.xcs_graph import XCSGraph, XCSNode

# Public API - core optimization system

from ember.xcs.jit import JITCache, JITMode, explain_jit_selection, get_jit_stats, jit

# Implementation components - schedulers

from ember.xcs.schedulers.base_scheduler import BaseScheduler
from ember.xcs.schedulers.factory import create_scheduler
from ember.xcs.schedulers.unified_scheduler import (
NoOpScheduler,
ParallelScheduler,
SequentialScheduler,
TopologicalScheduler,
WaveScheduler,
)
from ember.xcs.tracer.\_context_types import TraceContextData

# Public API - graph construction

from ember.xcs.tracer.autograph import AutoGraphBuilder, autograph

# Implementation components - tracing

from ember.xcs.tracer.xcs_tracing import TracerContext, TraceRecord
from ember.xcs.transforms.mesh import DeviceMesh, PartitionSpec, mesh_sharded
from ember.xcs.transforms.pmap import pjit, pmap # Parallelization

# Implementation components - transformations

from ember.xcs.transforms.transform_base import (
BaseTransformation,
BatchingOptions,
ParallelOptions,
TransformError,
compose,
)

# Public API - transformations for parallel execution

from ember.xcs.transforms.vmap import vmap # Vectorization

# Explicitly define public interface

**all** = [

# Core user-facing API - optimization

"jit",
"JITMode",
"get_jit_stats",
"explain_jit_selection",

# Core user-facing API - execution

"execute_graph",
"execution_options",
"ExecutionOptions",
"create_scheduler",
"ExecutionResult",

# Core user-facing API - transformations

"vmap",
"pmap",
"pjit",
"DeviceMesh",
"PartitionSpec",
"mesh_sharded",
"compose",
"TransformError",

# Core user-facing API - graph building

"autograph",
"TracerContext",

# Core user-facing API - configuration

"JITOptions",
"XCSExecutionOptions",
"APIExecutionResult",
"TransformOptions",

# Implementation details - generally not needed by users

"XCSGraph",
"XCSNode",
"DependencyAnalyzer",
"GraphBuilder",
"EnhancedTraceGraphBuilder",
"TraceRecord",
"TraceContextData",
"AutoGraphBuilder",
"XCSPlan",
"XCSTask",
"BaseScheduler",
"NoOpScheduler",
"ParallelScheduler",
"SequentialScheduler",
"TopologicalScheduler",
"WaveScheduler",
"GraphExecutor",
"ExecutionMetrics",
"JITCache",
"BaseTransformation",
"BatchingOptions",
"ParallelOptions",
]

</code>

src\ember\example_simplified_imports.py:
<code>
"""
Example demonstrating the simplified import structure.

This example shows how to use the new top-level imports for operators and NON components.
"""

from ember.non import JudgeSynthesis, Sequential, UniformEnsemble

# Create an ensemble with 3 identical models

ensemble = UniformEnsemble(num_units=3, model_name="openai:gpt-4o", temperature=1.0)

# Create a judge to synthesize the outputs

judge = JudgeSynthesis(model_name="anthropic:claude-3-opus")

# Combine them sequentially

pipeline = Sequential(operators=[ensemble, judge])

# This can now be executed with:

# result = pipeline(inputs={"query": "What is the future of AI?"})

</code>

src\ember\non.py:
<code>
"""
Network of Operators (NON) Pattern
=================================

The NON module provides composable building blocks for LLM application patterns.
These high-level operators encapsulate common patterns for ensemble generation,
aggregation, verification, and sequential processing.

Core components:

- UniformEnsemble: Generates multiple model responses using identical LLM configurations
- MostCommon: Implements majority-vote aggregation
- JudgeSynthesis: Uses a judge model to synthesize multiple responses
- Verifier: Performs factual and logical verification of responses
- Sequential: Chains operators together in a pipeline

Basic usage:

```python
import ember
from ember.non import UniformEnsemble, JudgeSynthesis, Sequential

# Create an ensemble with 3 identical models
ensemble = UniformEnsemble(
    num_units=3,
    model_name="openai:gpt-4o",
    temperature=1.0
)

# Create a judge to synthesize the outputs
judge = JudgeSynthesis(model_name="anthropic:claude-3-opus")

# Combine them sequentially
pipeline = Sequential(operators=[ensemble, judge])

# Execute the pipeline
result = pipeline(inputs={"query": "What is the future of AI?"})
```

For more advanced usage, see the documentation.
"""

# This pattern ensures we prioritize types that don't have circular dependencies

# first, before importing implementation code that might create import cycles

# First, import all the type definitions which are less likely to have circular references

try: # Import type-related items first to avoid circular imports
from ember.core.registry.operator.core.ensemble import (
EnsembleOperatorInputs as EnsembleInputs,
)
from ember.core.registry.operator.core.ensemble import EnsembleOperatorOutputs
from ember.core.registry.operator.core.most_common import (
MostCommonAnswerSelectorOperatorInputs as MostCommonInputs,
)
from ember.core.registry.operator.core.most_common import (
MostCommonAnswerSelectorOutputs,
)
from ember.core.registry.operator.core.synthesis_judge import (
JudgeSynthesisInputs,
JudgeSynthesisOutputs,
)
from ember.core.registry.operator.core.verifier import (
VerifierOperatorInputs as VerifierInputs,
)
from ember.core.registry.operator.core.verifier import (
VerifierOperatorOutputs as VerifierOutputs,
)
except ImportError: # For tests, define stub classes in case imports fail
from typing import List

    from ember.core.types.ember_model import EmberModel

    # Stub classes for when imports fail during test collection
    class EnsembleInputs(EmberModel):
        """Input type for ensemble operations."""

        query: str

    class EnsembleOperatorOutputs(EmberModel):
        """Output type for ensemble operations."""

        responses: List[str]

    class MostCommonInputs(EmberModel):
        """Input type for most common answer selection."""

        query: str
        responses: List[str]

    class MostCommonAnswerSelectorOutputs(EmberModel):
        """Output type for most common answer selection."""

        final_answer: str

    class JudgeSynthesisInputs(EmberModel):
        """Input type for judge synthesis operations."""

        query: str
        responses: List[str]

    class JudgeSynthesisOutputs(EmberModel):
        """Output type for judge synthesis operations."""

        synthesized_response: str
        reasoning: str

    class VerifierInputs(EmberModel):
        """Input type for verification operations."""

        query: str
        candidate_answer: str

    class VerifierOutputs(EmberModel):
        """Output type for verification operations."""

        verdict: str
        explanation: str
        revised_answer: str

# Then, import the implementation modules which might have dependencies on the types

try:
from ember.core.non import ( # Core operators; Additional input types for VariedEnsemble
JudgeSynthesis,
MostCommon,
Sequential,
UniformEnsemble,
VariedEnsemble,
VariedEnsembleInputs,
VariedEnsembleOutputs,
Verifier,
)
except ImportError: # Stub implementations for tests
from typing import List

    class UniformEnsemble:
        """Stub UniformEnsemble for tests."""

        def __init__(self, num_units=3, model_name=None, temperature=1.0):
            self.num_units = num_units
            self.model_name = model_name
            self.temperature = temperature

        def __call__(self, *, inputs):
            return {"responses": ["stub response"] * self.num_units}

    class MostCommon:
        """Stub MostCommon for tests."""

        def __init__(self):
            pass

        def __call__(self, *, inputs):
            return {"final_answer": "stub answer"}

    class JudgeSynthesis:
        """Stub JudgeSynthesis for tests."""

        def __init__(self, model_name=None):
            self.model_name = model_name

        def __call__(self, *, inputs):
            return {
                "synthesized_response": "stub synthesis",
                "reasoning": "stub reasoning",
            }

    class Verifier:
        """Stub Verifier for tests."""

        def __init__(self, model_name=None):
            self.model_name = model_name

        def __call__(self, *, inputs):
            return {
                "verdict": "valid",
                "explanation": "stub explanation",
                "revised_answer": "stub revision",
            }

    class Sequential:
        """Stub Sequential for tests."""

        def __init__(self, operators=None):
            self.operators = operators or []

        def __call__(self, *, inputs):
            return {"result": "stub sequential result"}

    class VariedEnsemble:
        """Stub VariedEnsemble for tests."""

        def __init__(self, models=None):
            self.models = models or []

        def __call__(self, *, inputs):
            return {"responses": ["stub varied response"] * len(self.models)}

    class VariedEnsembleInputs(BaseModel):
        """Stub VariedEnsembleInputs for tests."""

        query: str

    class VariedEnsembleOutputs(BaseModel):
        """Stub VariedEnsembleOutputs for tests."""

        responses: List[str]

**all** = [

# Core operators

"UniformEnsemble",
"MostCommon",
"JudgeSynthesis",
"Verifier",
"Sequential",
"VariedEnsemble",

# Input/output types

"EnsembleInputs",
"EnsembleOperatorOutputs",
"MostCommonInputs",
"MostCommonAnswerSelectorOutputs",
"JudgeSynthesisInputs",
"JudgeSynthesisOutputs",
"VerifierInputs",
"VerifierOutputs",
"VariedEnsembleInputs",
"VariedEnsembleOutputs",
]

</code>

src\ember\plugin_system.py:
<code>
"""Plugin Registration System for Ember Model Providers.

Providers (e.g., OpenAI, Anthropic, etc.) can register themselves using the
@provider decorator. This mechanism decouples provider implementations from the core Ember code.
"""

from typing import Any, Callable, Dict, Type

# Global registry mapping provider names to their corresponding provider classes.

registered_providers: Dict[str, Type[Any]] = {}

def provider(name: str) -> Callable[[Type[Any]], Type[Any]]:
"""Decorator to register a model provider class with Ember.

    This decorator registers the provider class in a global registry under the specified name.
    It enables provider implementations to be decoupled from core Ember logic.

    Example:
        @provider(name="OpenAI")
        class OpenAIModel(BaseProviderModel):
            ...

    Args:
        name (str): The unique provider name used for registration.

    Returns:
        Callable[[Type[Any]], Type[Any]]: A decorator that registers the provider class.
    """

    def decorator(provider_class: Type[Any]) -> Type[Any]:
        registered_providers[name] = provider_class
        return provider_class

    return decorator

</code>

src\ember\_\_init\_\_.py:
<code>
"""
Ember: Compositional Framework for Compound AI Systems
=====================================================

Ember is a powerful, extensible Python framework for building and orchestrating
Compound AI Systems and "Networks of Networks" (NONs).

Core Features:

- Eager Execution by Default
- Parallel Graph Execution
- Composable Operators
- Extensible Registry System
- Enhanced JIT System
- Built-in Evaluation
- Powerful Data Handling
- Intuitive Model Access

For more information, visit https://pyember.org

Examples: # Import primary API modules
import ember

    # Initialize model registry and service
    from ember.api.models import initialize_registry, create_model_service

    registry = initialize_registry(auto_discover=True)
    model_service = create_model_service(registry=registry)

    # Call a model
    response = model_service.invoke_model(
        model_id="openai:gpt-4",
        prompt="What's the capital of France?",
        temperature=0.7
    )

    # Load datasets directly
    from ember.api.data import data
    mmlu_data = data("mmlu")

    # Or use the dataset builder pattern
    from ember.api.data import DatasetBuilder
    dataset = DatasetBuilder().split("test").sample(100).build("mmlu")

    # Create Networks of Networks (NONs)
    from ember.api import non
    ensemble = non.UniformEnsemble(
        num_units=3,
        model_name="openai:gpt-4o"
    )

    # Optimize with XCS
    from ember.api import xcs
    @xcs.jit
    def optimized_fn(x):
        return complex_computation(x)

"""

from **future** import annotations

import importlib.metadata
from typing import Any, Callable, Dict, Optional, TypeVar, Union

# Forward reference for return type

ModelRegistryType = TypeVar("ModelRegistryType")

# Import primary API components - these are the only public interfaces

from ember.api import models # Language model access (models.openai.gpt4, etc.)
from ember.api import non # Network of Networks patterns (non.UniformEnsemble, etc.)
from ember.api import operators # Operator registry (operators.get_operator(), etc.)
from ember.api import xcs # Execution optimization (xcs.jit, etc.)

# Core imports are done lazily in functions to avoid circular dependencies

# This follows the thread-local design pattern established in the EmberContext system

# and maintains the separation between API and implementation layers

# Version detection

try:
**version** = importlib.metadata.version("ember-ai")
except importlib.metadata.PackageNotFoundError:
**version** = "0.1.0"

# Package metadata

\_PACKAGE_METADATA = {
"name": "ember-ai",
"version": **version**,
"description": "Compositional framework for building and orchestrating "
"Compound AI Systems and Networks of Networks (NONs).",
}

def initialize*ember(
config_path: Optional[str] = None,
auto_discover: bool = True,
force_discovery: bool = False,
api_keys: Optional[Dict[str, str]] = None,
env_prefix: str = "EMBER*",
verbose_logging: bool = False,
) -> ModelRegistryType:
"""Initialize core Ember components.

    This function configures logging, API keys, and the model registry.
    The global Ember context is initialized lazily on first use via
    `ember.core.context.current_context()`.

    Args:
        config_path: Path to the main configuration file.
        auto_discover: Automatically discover models and plugins.
        force_discovery: Force re-discovery even if cache exists.
        api_keys: Dictionary of API keys (e.g., {"openai": "sk-..."}).
        env_prefix: Prefix for environment variables (e.g., EMBER_OPENAI_API_KEY).
        verbose_logging: Enable verbose debug logging.

    Returns:
        The initialized ModelRegistry.
    """
    # Import modules where needed to avoid circular dependencies
    from ember.core.config.manager import create_config_manager
    from ember.core.registry.model.initialization import initialize_registry
    from ember.core.utils.logging import configure_logging

    # 0. Configure logging first
    configure_logging(verbose=verbose_logging)

    # 1. Create the configuration manager with the provided config path
    config_manager = create_config_manager(config_path=config_path)

    # 2. Apply API keys if provided (highest precedence)
    if api_keys:
        for provider, api_key in api_keys.items():
            config_manager.set_provider_api_key(provider, api_key)

    # 3. Initialize the model registry
    registry = initialize_registry(
        config_manager=config_manager,
        auto_discover=auto_discover,
        force_discovery=force_discovery,
    )

    # Context is initialized lazily via current_context()

    # Return the registry
    return registry

def init(
config: Optional[Union[Dict[str, Any], Dict[str, Any]]] = None,
usage_tracking: bool = False,
) -> Callable:
"""Initialize Ember and return a unified model service.

    This function provides a simple entry point for initializing Ember and accessing
    models directly through a callable service object, as shown in the README examples.

    Args:
        config: Optional configuration to override defaults. Can be a dictionary or
            a ConfigManager
        usage_tracking: Whether to enable cost/token tracking

    Returns:
        A model service that can be called directly with models and prompts

    Examples:
        # Simple usage
        service = init()
        response = service("openai:gpt-4o", "What is the capital of France?")

        # With usage tracking
        service = init(usage_tracking=True)
        response = service(models.ModelEnum.gpt_4o, "What is quantum computing?")
        usage = service.usage_service.get_total_usage()
    """
    from ember.api.models import ModelService, UsageService
    from ember.core.config.manager import create_config_manager
    from ember.core.registry.model.initialization import initialize_registry

    # Initialize configuration if needed
    config_manager = None
    if isinstance(config, dict):
        config_manager = create_config_manager()
        for key, value in config.items():
            config_manager.set(key, value)
    elif config is not None and hasattr(config, "set") and hasattr(config, "get"):
        config_manager = config

    # Initialize the registry with auto-discovery
    registry = initialize_registry(auto_discover=True, config_manager=config_manager)

    # Create usage service if tracking is enabled
    usage_service = UsageService() if usage_tracking else None

    # Create a model service that can be called directly
    service = ModelService(registry=registry, usage_service=usage_service)

    # Create a wrapper function that allows direct calling with model ID and prompt
    def service_wrapper(model_id_or_enum, prompt, **kwargs):
        return service.invoke_model(model_id_or_enum, prompt, **kwargs)

    # Add service attributes to the wrapper
    service_wrapper.model_service = service
    service_wrapper.registry = registry
    if usage_service:
        service_wrapper.usage_service = usage_service

    return service_wrapper

# Public interface - only export the main API components

**all** = [
"models", # Language model access
"operators", # Operator registry
"non", # Network of Networks patterns
"xcs", # Execution optimization
"initialize_ember", # Global initialization function
"init", # Simple initialization function (matches README examples)
"configure_logging", # Logging configuration utility
"set_component_level", # Fine-grained logging control
"__version__",
]

</code>

ARCHITECTURE.md:
<code>

# Ember Architecture

This document describes Ember's architecture, core components, and design principles. It serves as both a high-level overview for users and a detailed guide for core contributors.

## Design Philosophy

Ember is built on these foundational principles:

1.  **Composability First**: The ability to combine, chain, and nest components (e.g. `Operator` components) is central to Ember's design
2.  **Type Safety**: Comprehensive type annotations ensure robustness and IDE support
3.  **Testability**: Components are designed with SOLID principles in mind, for easy isolation and testing
4.  **Scalability**: support for Parallel execution is built-in at the framework's core. This is more Tensorflow/JAX, than classic Torch spiritually
5.  **Extensibility**: Registry-based design makes it simple to add new components
6.  **Skeurmophism**: APIs follow familiar patterns from PyTorch/JAX, to somewhat control the learning curve
7.  **Simple-over-easy**: Minimal "magic" and a focus on explicitness

## System Architecture

Ember's architecture follows a layered design with clear separations of concern:

```
┌───────────────────────────────────────────────────────────────────────────────────────────┐
│                                       PUBLIC API LAYER                                    │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐    │
│  │  api.models             │  │  api.operator           │  │  api.xcs                │    │
│  │                         │  │                         │  │                         │    │
│  │  • LLM Interfaces       │  │  • Operator Base        │  │  • JIT Functions        │    │
│  │  • Model Service        │  │  • Specification        │  │  • Execution Options    │    │
│  │  • Model Registry       │  │  • Input/Output Models  │  │  • Graph Controls       │    │
│  │  • Usage Tracking       │  │  • Operator Registry    │  │  • Transform Functions  │    │
│  └─────────────────────────┘  └─────────────────────────┘  └─────────────────────────┘    │
│                                                                                           │
│  ┌─────────────────────────┐  ┌─────────────────────────┐                                 │
│  │  api.data               │  │  api.non                │                                 │
│  │                         │  │                         │                                 │
│  │  • Dataset Access       │  │  • Ensemble Patterns    │                                 │
│  │  • Data Loaders         │  │  • Verification         │                                 │
│  │  • Transformers         │  │  • Synthesis            │                                 │
│  │  • Evaluators           │  │  • Composition Helpers  │                                 │
│  └─────────────────────────┘  └─────────────────────────┘                                 │
│                                                                                           │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                    APPLICATION LAYER                                      │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐    │
│  │  NON Patterns           │  │  Auto Graph Builder     │  │  Enhanced JIT           │    │
│  │                         │  │                         │  │                         │    │
│  │  • UniformEnsemble      │  │  • Autograph            │  │  • Function Tracing     │    │
│  │  • JudgeSynthesis       │  │  • IR Graph Construct   │  │  • Optimized Execution  │    │
│  │  • Verifier             │  │  • Dependency Tracking  │  │  • Parallel Dispatch    │    │
│  │  • VariedEnsemble       │  │  • Visualization        │  │                         │    │
│  └─────────────────────────┘  └─────────────────────────┘  └─────────────────────────┘    │
│                                                                                           │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                      CORE COMPONENT LAYER                                 │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐    │
│  │  Model Registry         │  │  Operator System        │  │  Prompt Specifications  │    │
│  │                         │  │                         │  │                         │    │
│  │  • ModelInfo            │  │  • Base Operator        │  │  • Template Rendering   │    │
│  │  • ModelService         │  │  • Operator Registry    │  │  • Input Validation     │    │
│  │  • UsageService         │  │  • Core Operators       │  │  • Output Validation    │    │
│  │  • Provider Adapters    │  │  • Custom Operators     │  │  • Schema Generation    │    │
│  └─────────────────────────┘  └─────────────────────────┘  └─────────────────────────┘    │
│                                                                                           │
│  ┌─────────────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐    │
│  │  Data Processing        │  │  Evaluation Tools       │  │  Application Context    │    │
│  │                         │  │                         │  │                         │    │
│  │  • Dataset Loaders      │  │  • Evaluators           │  │  • Config Manager       │    │
│  │  • Transformers         │  │  • Metrics              │  │  • Dependency Injection │    │
│  │  • Samplers             │  │  • Result Analysis      │  │  • Service Registry     │    │
│  │  • Dataset Registry     │  │  • Visualization        │  │  • Logging Config       │    │
│  └─────────────────────────┘  └─────────────────────────┘  └─────────────────────────┘    │
│                                                                                           │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                  EXECUTION ENGINE (XCS)                                   │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐    │
│  │  Graph Definition       │  │  Tracer System          │  │  Execution Engine       │    │
│  │                         │  │                         │  │                         │    │
│  │  • XCSGraph IR          │  │  • Function Tracing     │  │  • Schedulers           │    │
│  │  • XCSNode Primitive    │  │  • Execution Recording  │  │  • Execution Plan       │    │
│  │                         │  │  • JIT Compilation      │  │  • Parallel Dispatch    │    │
│  │                         │  │  • Graph Optimization   │  │                         │    │
│  └─────────────────────────┘  └─────────────────────────┘  └─────────────────────────┘    │
│                                                                                           │
└───────────────────────────────────────────────────────────────────────────────────────────┘
```

## Layer Responsibilities

### 1. Execution Engine (XCS)

The foundational layer providing computation graph definition and execution:

- **Graph Definition**: Defines the structure of computation
- **Tracer System**: Records execution and enables optimization
- **Execution Engine**: Manages the actual running of operations with parallelization

### 2. Core Component Layer

The building blocks of Ember's functionality:

- **Model Registry**: Management of LLM providers and models
- **Operator System**: Core computational units and their registry
- **Prompt Specifications**: Type-safe template rendering and validation
- **Data Processing**: Dataset handling, transformation, and sampling
- **Evaluation Tools**: Benchmarking and performance analysis
- **Application Context**: Configuration and dependency management

### 3. Application Layer

High-level abstractions built on the core components:

- **NON Patterns**: Ready-to-use Networks of Networks patterns
- **Auto Graph Builder**: Automatic graph construction from code
- **Enhanced JIT**: Just-in-time compilation for optimized execution

## Component Details

### Application Context

The `EmberAppContext` serves as the central dependency injection container:

```python
from ember.core.app_context import get_app_context

# Access global context
context = get_app_context()

# Access services
model_service = context.model_service
usage_service = context.usage_service

# Access configuration via standardized manager
config_manager = context.config_manager
model_registry_config = config_manager.get_config("model_registry")
app_config = config_manager.get_config("app")

# Configuration values are accessed via dot notation
api_key = model_registry_config.providers.openai.api_key
```

Key responsibilities:

- Initialization and configuration of system components
- Service registration and dependency injection
- Standardized configuration management with schema validation
- Environment variable resolution and config merging
- Logging setup

### Model Registry System

The Model Registry manages connections to LLM providers:

```python
# Using the simplified API
from ember.api.models import ModelRegistry, ModelInfo, ModelService

# Register a model
registry = ModelRegistry()
registry.register_model(ModelInfo(id="openai:gpt-4o", ...))

# Create a service for model access
service = ModelService(registry=registry)
response = service.invoke_model("openai:gpt-4o", "Hello world")

# Even simpler with automatic initialization
from ember import initialize_ember

# Initialize and get the service in one step
service = initialize_ember()
response = service("anthropic:claude-3-sonnet", "Hello Claude")
```

#### Model Registry Component Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                           Model Registry System                        │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  ModelRegistry  │◄────►│  ModelFactory   │─────►│ Provider Models │ │
│  └────────┬────────┘      └─────────────────┘      └─────────────────┘ │
│           │                                                            │
│           │               ┌─────────────────┐      ┌─────────────────┐ │
│           └──────────────►│  ModelService   │◄────►│  UsageService   │ │
│                           └─────────────────┘      └─────────────────┘ │
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ OpenAI Provider │      │Anthropic Provid.│      │ Other Providers │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key components:

- `ModelRegistry`: Central repository for model metadata
- `ModelService`: High-level API for model invocation
- `UsageService`: Tracks token usage and cost
- Provider implementations: OpenAI, Anthropic, Google, IBM, etc.

### Operator System

Operators are the fundamental computational units in Ember:

```python
from ember.api.operators import Operator, Specification, EmberModel

class SummarizerInput(EmberModel):
    text: str
    max_words: int = 100

class SummarizerOutput(EmberModel):
    summary: str
    word_count: int

class SummarizerSpec(Specification):
    input_model = SummarizerInput
    structured_output = SummarizerOutput
    prompt_template = "Summarize the following text in {max_words} words or less:\n\n{text}"

class SummarizerOperator(Operator[SummarizerInput, SummarizerOutput]):
    specification = SummarizerSpec()

    def forward(self, *, inputs: SummarizerInput) -> SummarizerOutput:
        # Implementation
        ...
```

#### Operator System Component Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                           Operator System                              │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │OperatorRegistry │◄────►│OperatorFactory  │─────►│Operator Instance│ │
│  └─────────────────┘      └─────────────────┘      └────────┬────────┘ │
│                                                             │          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌────────▼────────┐ │
│  │  Base Operator  │◄─────┤Prompt Spec.     │◄───► │   forward()     │ |
│  └────────┬────────┘      └─────────────────┘      └─────────────────┘ │
│           │                                                            │
│           ▼                                                            │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ Core Operators  │      │ Custom Operators │      │  NON Operators │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key components:

- `Operator`: Base class for all operators
- `Specification`: Type definitions for operator I/O
- Core operators: Ensemble, Judge, Verifier, etc.
- Operator registry for discovery

### Prompt Specification System

Specifications define the contract between inputs and outputs:

```python
from ember.api.operators import Specification, EmberModel

class QuestionInput(EmberModel):
    question: str
    context: str

class AnswerOutput(EmberModel):
    answer: str
    confidence: float

class QASpecification(Specification):
    input_model = QuestionInput
    structured_output = AnswerOutput
    prompt_template = """
    Answer the question based on the context.

    Context: {context}
    Question: {question}
    """
```

#### Prompt Specification Component Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                          Prompt Specification System                   │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  Specification  │──────┤   Input Model   │      │  Output Model   │ │
│  └────────┬────────┘      └─────────────────┘      └─────────────────┘ │
│           │                                                            │
│           ▼                                                            │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │Prompt Template  │─────►│Template Renderer│─────►│  Input Val.     │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │Schema Generation│◄─────┤Output Validation│◄─────┤  Error Handl.   │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key features:

- Type validation for inputs and outputs
- Template rendering with validation
- Automatic placeholder checking
- Support for structured data extraction

### Execution Engine (XCS)

XCS handles graph-based execution:

```python
from ember.xcs import XCSGraph, execute_graph, execution_options

# Create execution graph
graph = XCSGraph()
graph.add_node(operator=ensemble, node_id="ensemble")
graph.add_node(operator=judge, node_id="judge")
graph.add_edge(from_id="ensemble", to_id="judge")

# Execute with parallelization
with execution_options(scheduler="wave", max_workers=4):
    result = execute_graph(
        graph=graph,
        inputs={"query": "What is quantum computing?"}
    )
```

#### XCS Engine Component Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                        Execution Engine (XCS)                          │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │    XCSGraph     │─────►│    XCSNode      │◄─────┤      Edge       │ │
│  └────────┬────────┘      └────────┬────────┘      └─────────────────┘ │
│           │                        │                                   │
│           ▼                        ▼                                   │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  Graph Compiler │─────►│ Execution Plan  │─────►│    Scheduler    │ │
│  └─────────────────┘      └────────┬────────┘      └────────┬────────┘ │
│                                    │                        │          │
│                                    ▼                        ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  Input Mapping  │◄─────┤ Parallel Worker │◄─────┤Output Collection│ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key components:

- `XCSGraph`: Directed acyclic graph representation
- `ExecutionPlan`: Compiled execution plan
- `Scheduler`: Controls execution strategy
- `Tracer`: Records execution for debugging

### Enhanced JIT System

Ember provides three complementary approaches to Just-In-Time optimization:

#### JIT Strategy Pattern

The `jit` decorator now uses a pluggable strategy pattern with multiple implementations:

```python
from ember.xcs import jit, JITMode
from ember.api.operators import Operator
from ember.api import non

# With automatic strategy selection
@jit
class MyEnsemble(Operator):
    def forward(self, *, inputs):
        # Complex computation automatically traced and optimized
        ensemble = non.UniformEnsemble(num_units=3, model_name="openai:gpt-4o")
        responses = ensemble(inputs={"query": inputs.query})
        return responses

# With explicit strategy selection
@jit(mode=JITMode.ENHANCED)
class Pipeline(Operator):
    def __init__(self):
        self.refiner = QuestionRefinement()
        self.ensemble = Ensemble()
        self.aggregator = MostCommon()

    def forward(self, *, inputs):
        refined = self.refiner(inputs=inputs)
        answers = self.ensemble(inputs=refined)
        return self.aggregator(inputs=answers)
```

The JIT system now supports three strategies:

1. **Trace Strategy** (`JITMode.TRACE`): Traditional execution tracing for dynamic flows
2. **Structural Strategy** (`JITMode.STRUCTURAL`): Analyzes operator structure without requiring execution
3. **Enhanced Strategy** (`JITMode.ENHANCED`): Combines static and dynamic analysis for optimal parallelization

#### Autograph Context Manager

For explicit graph construction:

```python
from ember.xcs import autograph, execute_graph, execution_options

with autograph() as graph:
    intermediate = op1(inputs={"query": "Example"})
    result = op2(inputs=intermediate)

# Execute the graph with optimized scheduling
results = execute_graph(
    graph=graph,
    options=execution_options(scheduler="wave", max_workers=4)
)
```

#### Unified JIT System Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                         Unified JIT System                             │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  JIT Decorator  │─────►│Strategy Selector│─────►│  JIT Cache      │ │
│  └────────┬────────┘      └────────┬────────┘      └────────┬────────┘ │
│           │                        │                        │          │
│           ▼                        ▼                        ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ Trace Strategy  │─────►│ Structural Strat│─────►│Enhanced Strategy│ │
│  └────────┬────────┘      └────────┬────────┘      └────────┬────────┘ │
│           │                        │                        │          │
│           ▼                        ▼                        ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │   Autograph     │─────►│Graph Dependency │─────►│ Unified Engine  │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key features:

- Unified strategy pattern with pluggable implementations:
  - `trace`: Optimized for dynamic execution patterns
  - `structural`: Static analysis of operator composition
  - `enhanced`: Combines static and dynamic analysis for optimal parallelism
- Automatic strategy selection based on operator characteristics
- Consistent caching mechanism across all strategies
- Advanced dependency analysis with wave-based scheduling
- Transformation composition for complex optimizations
- Comprehensive metrics and introspection tools

For a comprehensive explanation of the JIT system, see [JIT Overview](docs/xcs/JIT_OVERVIEW.md).

### Function Transformation System

The transformation system provides high-level operations for data and computation transformations:

```python
from ember.xcs import vmap, pmap, compose, DeviceMesh, PartitionSpec, mesh_sharded

# Vectorized mapping for batch processing
batch_processor = vmap(process_item)
batch_results = batch_processor(inputs={"data": [item1, item2, item3]})

# Parallel execution across multiple workers
parallel_processor = pmap(process_item, num_workers=4)
parallel_results = parallel_processor(inputs=complex_data)

# Combine transformations for complex pipelines
pipeline = compose(
    vmap(batch_size=32),
    pmap(num_workers=4)
)(process_item)

# Device mesh sharding for multi-device execution
mesh = DeviceMesh(devices=["gpu:0", "gpu:1", "gpu:2", "gpu:3"], mesh_shape=(2, 2))
partition = PartitionSpec("batch", "model")
sharded_op = mesh_sharded(pipeline, mesh=mesh, partition_spec=partition)
```

#### Transform System Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                       Transform System                                 │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │BaseTransformation│─────►│TransformProtocol│─────►│ BatchingOptions │ │
│  └────────┬────────┘      └─────────────────┘      └────────┬────────┘ │
│           │                                                 │          │
│           ▼                                                 ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │    vmap         │─────►│      pmap       │─────►│ParallelOptions  │ │
│  └────────┬────────┘      └────────┬────────┘      └────────┬────────┘ │
│           │                        │                        │          │
│           ▼                        ▼                        ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ mesh_sharded    │─────►│    compose      │─────►│  Unified JIT    │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key features:

- Common base class (`BaseTransformation`) with consistent interface
- Compositional design for combining transformations
- Integration with the JIT system for optimized execution
- Support for both data parallelism and model parallelism
- Extensible design for custom transformations

### Data Processing System

The data module provides tools for dataset management:

```python
from ember.core.utils.data.service import DataService
from ember.core.utils.data.base.samplers import RandomSampler

# Load a benchmark dataset
data_service = DataService()
mmlu_data = data_service.load_dataset(
    dataset_name="mmlu",
    subset="high_school_mathematics",
    sampler=RandomSampler(n=100)
)
```

#### Data Processing Component Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                         Data Processing System                         │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  DataService    │─────►│ Dataset Reg.    │─────►│ Dataset Loaders │ │
│  └────────┬────────┘      └─────────────────┘      └────────┬────────┘ │
│           │                                                 │          │
│           ▼                                                 ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ Dataset Cache   │◄─────┤  Dataset Item   │◄─────┤  External API   │ │
│  └─────────────────┘      └────────┬────────┘      └─────────────────┘ │
│                                    │                                   │
│                                    ▼                                   │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ Data Transformer│◄─────┤ Data Sampler    │─────►│ Data Validator  │ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key components:

- `DataService`: Central access point for datasets
- Dataset loaders for popular benchmarks
- Transformers for data preprocessing
- Samplers for dataset subsampling

### Evaluation System

The evaluation system measures model performance:

```python
from ember.core.utils.eval.pipeline import EvaluationPipeline
from ember.core.utils.eval.evaluators import MultipleChoiceEvaluator

# Create evaluation pipeline
eval_pipeline = EvaluationPipeline(
    dataset=test_data,
    evaluators=[MultipleChoiceEvaluator()],
    model=model
)

# Run evaluation
results = eval_pipeline.evaluate()
print(f"Accuracy: {results.metrics['accuracy']:.2f}")
```

#### Evaluation Component Architecture

```
┌────────────────────────────────────────────────────────────────────────┐
│                          Evaluation System                             │
├────────────────────────────────────────────────────────────────────────┤
│                                                                        │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │  Eval Pipeline  │─────►│  Eval Registry  │─────►│    Evaluator    │ │
│  └────────┬────────┘      └─────────────────┘      └────────┬────────┘ │
│           │                                                 │          │
│           ▼                                                 ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │ Data Provider   │─────►│ Model Runner    │─────►│ Result Collector│ │
│  └─────────────────┘      └─────────────────┘      └────────┬────────┘ │
│                                                             │          │
│                                                             ▼          │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐ │
│  │Metric Calculator│◄─────┤ Result Analyzer │◄─────┤ Report Generator│ │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘ │
│                                                                        │
└────────────────────────────────────────────────────────────────────────┘
```

Key components:

- `EvaluationPipeline`: Orchestrates evaluation
- Task-specific evaluators
- Metrics collection
- Result reporting

## Full System Dependency Flow

The diagram below illustrates the complete dependency flow between major components:

```
┌───────────────────────────────────────────────────────────────────────────────────────────┐
│                                   Configuration Layer                                     │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │    Config Files         │────►│    Config Manager       │────►│   Environment       │  │
│  │    (.yaml, .env)        │     │                         │     │   Variables         │  │
│  └─────────────────────────┘     └───────────┬─────────────┘     └─────────────────────┘  │
│                                              │                                            │
│                                              ▼                                            │
│                                 ┌─────────────────────────┐                               │
│                                 │    EmberAppContext      │                               │
│                                 └───────────┬─────────────┘                               │
│                                             │                                             │
└─────────────────────────────────────────────┼─────────────────────────────────────────────┘
                                              │
                                              ▼
┌───────────────────────────────────────────────────────────────────────────────────────────┐
│                                    Service Layer                                          │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │    Model Registry       │◄───►│     Model Service       │◄───►│   Usage Service     │  │
│  └───────────┬─────────────┘     └───────────┬─────────────┘     └─────────────────────┘  │
│              │                               │                                            │
│              ▼                               ▼                                            │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │   Provider Models       │◄───►│    Operator Registry    │◄───►│  Data Service       │  │
│  └─────────────────────────┘     └───────────┬─────────────┘     └───────────┬─────────┘  │
│                                              │                               │            │
└────────────────────────────────-─────────────┼───────────────────────────────┼─────────────┘
                                               │                               │
                                               ▼                               ▼
┌───────────────────────────────────────────────────────────────────────────────────────────┐
│                                    Component Layer                                        │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │    Base Operators       │◄───►│    Prompt Specifications│◄───►│   Dataset Loaders   │  │
│  └───────────┬─────────────┘     └─────────────────────────┘     └─────────────────────┘  │
│              │                                                                            │
│              ▼                                                                            │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │   Core Operators        │◄───►│     NON Patterns        │◄───►│   Evaluators        │  │
│  └───────────┬─────────────┘     └───────────┬─────────────┘     └─────────────────────┘  │
│              │                               │                                            │
└─────-────────┼───────────────────────────────┼────────────────────────────────────────────┘
               │                               │
               ▼                               ▼
┌───────────────────────────────────────────────────────────────────────────────────────────┐
│                                  Execution Engine Layer                                   │
├───────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                           │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │       XCSGraph          │◄───►│     Graph Compiler      │◄───►│   JIT Compiler      │  │
│  └───────────┬─────────────┘     └───────────┬─────────────┘     └───────────┬─────────┘  │
│              │                               │                               │            │
│              ▼                               ▼                               ▼            │
│  ┌─────────────────────────┐     ┌─────────────────────────┐     ┌─────────────────────┐  │
│  │    Execution Plan       │◄───►│      Scheduler          │◄───►│  Parallel Executor  │  │
│  └─────────────────────────┘     └─────────────────────────┘     └─────────────────────┘  │
│                                                                                           │
└───────────────────────────────────────────────────────────────────────────────────────────┘
```

## Configuration System

Ember's configuration system provides a standardized way to configure all aspects of the framework:

```python
from ember.core.configs import ConfigManager, create_config_manager

# Create configuration manager with standard discovery
config_manager = create_config_manager()

# Access typed, validated configuration
model_registry_config = config_manager.get_config("model_registry")
openai_api_key = model_registry_config.providers.openai.api_key

# Configuration sources (in order of precedence):
# 1. Runtime overrides
# 2. Environment variables
# 3. User config files (~/.ember/config.yaml)
# 4. Project config files (./ember.yaml)
# 5. Default config (package defaults)

# Modify configuration at runtime
config_manager.update_config(
    "model_registry",
    {"providers": {"openai": {"api_key": "new-key"}}}
)
```

Key features:

- Centralized schema-based configuration with Pydantic
- Multiple configuration sources with priority ordering
- Environment variable expansion (${VAR_NAME} syntax)
- Deep config merging with proper override behavior
- Thread-safe configuration access
- Extensible provider system

## Code Organization

The code is organized into the following package structure:

| Package                             | Purpose                                                        |
| ----------------------------------- | -------------------------------------------------------------- |
| `ember.api`                         | Simplified public API for clean imports                        |
| `ember.api.models`                  | Models API for LLMs and providers                              |
| `ember.api.operator`                | Operator API for computational units                           |
| `ember.api.non`                     | NON patterns API                                               |
| `ember.api.xcs`                     | Execution engine API                                           |
| `ember.api.data`                    | Data processing API                                            |
| `ember.core`                        | Core framework classes and utilities                           |
| `ember.core.app_context`            | Application context and DI container                           |
| `ember.core.configs`                | Standardized configuration system with typed schema validation |
| `ember.core.types`                  | Type system, protocols, and validation                         |
| `ember.core.registry.model`         | Model registry and provider implementations                    |
| `ember.core.registry.operator`      | Operator system                                                |
| `ember.core.registry.specification` | Prompt specification system                                    |
| `ember.core.utils`                  | Utility functions and helpers                                  |
| `ember.core.utils.data`             | Data processing and datasets                                   |
| `ember.core.utils.eval`             | Evaluation and metrics                                         |
| `ember.core.non`                    | High-level NON patterns                                        |
| `ember.xcs`                         | Execution engine                                               |
| `ember.xcs.graph`                   | Graph definition and manipulation                              |
| `ember.xcs.engine`                  | Execution scheduling                                           |
| `ember.xcs.tracer`                  | Tracing and JIT compilation                                    |

## Import System

Ember organizes imports through the `ember.api` namespace:

```python
from ember.api.operators import Operator, Specification, EmberModel
from ember.xcs import jit, execution_options
from ember.api import models, non
from ember.api.data import DataLoader
```

This approach:

- Separates public API from internal implementation details
- Maintains backward compatibility during internal refactoring
- Follows conventions from our favorite, established frameworks

## Design Patterns

Ember employs several design patterns that are consistent throughout the codebase:

### 1. Registry Pattern

```python
# Registry implementation
class ModelRegistry:
    def __init__(self):
        self._models = {}

    def register_model(self, model_info: ModelInfo) -> None:
        self._models[model_info.id] = model_info

    def get_model_info(self, model_id: str) -> ModelInfo:
        return self._models[model_id]

# Usage
registry = ModelRegistry()
registry.register_model(ModelInfo(id="model1", ...))
model = registry.get_model_info("model1")
```

### 2. Factory Pattern

```python
class ModelFactory:
    def create_model(self, model_info: ModelInfo) -> BaseProviderModel:
        provider_name = model_info.provider["name"]
        if provider_name == "OpenAI":
            return OpenAIModel(model_info)
        elif provider_name == "Anthropic":
            return AnthropicModel(model_info)
        # etc.
```

### 3. Dependency Injection

```python
class ModelService:
    def __init__(self, registry: ModelRegistry, usage_service: Optional[UsageService] = None):
        self.registry = registry
        self.usage_service = usage_service
```

### 4. Composition Pattern

```python
class EnsembleOperator(Operator[EnsembleInput, EnsembleOutput]):
    def __init__(self, lm_modules: List[LMModule]):
        self.lm_modules = lm_modules

    def forward(self, *, inputs: EnsembleInput) -> EnsembleOutput:
        # Use contained modules
        responses = [lm(inputs.query) for lm in self.lm_modules]
        return EnsembleOutput(responses=responses)
```

### 5. Strategy Pattern

```python
class Scheduler(Protocol):
    def run_plan(self, plan: ExecutionPlan, global_input: Dict, graph: XCSGraph) -> Any:
        ...

class SerialScheduler:
    def run_plan(self, plan: ExecutionPlan, global_input: Dict, graph: XCSGraph) -> Any:
        # Serial execution implementation

class ParallelScheduler:
    def run_plan(self, plan: ExecutionPlan, global_input: Dict, graph: XCSGraph) -> Any:
        # Parallel execution implementation
```

## Performance Considerations

Ember balances ease of use with high performance:

### Parallelization Strategy

1. **Graph-Based Parallelism**:

   - The XCS engine automatically identifies independent operations
   - Executes them concurrently using thread pools
   - Configurable max_workers parameter

2. **Operator-Level Concurrency**:

   - Operators can implement their own internal parallelism
   - Example: EnsembleOperator runs multiple models concurrently

3. **Efficient Resource Usage**:
   - Smart thread pooling to avoid over-subscription
   - Rate limiting to respect API constraints

### Memory Management

1. **Lazy Instantiation**:

   - Models are instantiated only when needed
   - Heavy resources are loaded on demand

2. **Caching Strategy**:
   - Configuration is cached after initial load
   - Discovery results are cached
   - Model instances are reused

### Optimization Techniques

1. **JIT Compilation**:

   - Traces function execution to build optimized graphs
   - Identifies parallelizable operations
   - Minimizes redundant computations

2. **Efficient Data Transfer**:
   - Minimizes copying of large data between operators
   - Uses references when possible

## Deployment Considerations

When deploying Ember in production, consider these best practices:

### 1. Configuration Management

- Store API keys securely in environment variables
- Use separate configurations for development/production
- Override defaults with environment-specific settings

```
# Development configuration
config/
  base.yaml        # Base configuration for all environments
  development.yaml # Development-specific overrides
  production.yaml  # Production-specific overrides
```

### 2. Resource Planning

- Set appropriate thread pool sizes (max_workers)
- Monitor token usage with UsageService
- Implement rate limiting strategies
- Set up cost budgets and alerts

### 3. Error Handling

- Implement proper error handling at the application level
- Set up exponential backoff for API rate limits
- Use the retry utilities for transient errors
- Log errors comprehensively

### 4. Monitoring and Observability

- Set up proper logging with appropriate log levels
- Monitor token and request metrics
- Track performance of individual operators
- Set alerts for abnormal behavior

### 5. Scaling Strategies

For high-throughput applications:

- Distribute workloads across multiple processes or machines
- Use horizontal scaling for independent operations
- Consider specialized execution engines for very large workloads
- Use caching for frequently used models or operations

## Request Flow Diagram

The following diagram illustrates the flow of a typical request through the Ember system:

```
┌──────────────┐      ┌──────────────┐      ┌──────────────┐      ┌──────────────┐
│              │      │              │      │              │      │              │
│   User API   │─────►│ ModelService │─────►│ ModelRegistry│─────►│ ModelFactory │
│  Request     │      │              │      │              │      │              │
└──────┬───────┘      └──────┬───────┘      └──────┬───────┘      └──────┬───────┘
       │                     │                     │                     │
       │                     │                     │                     ▼
       │                     │                     │              ┌──────────────┐
       │                     │                     │              │              │
       │                     │                     └─────────────►│ Provider     │
       │                     │                                    │ Impl.        │
       │                     │                                    └──────┬───────┘
       │                     │                                           │
       │                     │                                           ▼
       │                     │                                    ┌──────────────┐
       │                     │                                    │              │
       │                     └───────────────────────────────────►│ UsageService │
       │                                                          │              │
       │                                                          └──────┬───────┘
       │                                                                 │
       ▼                                                                 ▼
┌──────────────┐                                                  ┌──────────────┐
│              │                                                  │              │
│  User API    │◄─────────────────────────────────────────────────┤   Response   │
│  Response    │                                                  │              │
└──────────────┘                                                  └──────────────┘
```

## Architecture Evolution

The Ember architecture continues to evolve along these paths:

1. **Distributed Execution**: Support for distributed execution across multiple machines
2. **Enhanced Caching**: Improved caching for models and intermediate results
3. **Custom Hardware Support**: Optimizations for specialized hardware (GPUs, TPUs)
4. **Plugin System**: More comprehensive plugin interfaces for extensions
5. **Advanced Graph Optimizations**: Additional graph transformations and optimizations

## Additional Resources

For more detailed information, consult these resources:

- [Model Registry Documentation](docs/quickstart/model_registry.md)
- [Operator System Documentation](docs/quickstart/operators.md)
- [XCS Execution Engine Documentation](docs/xcs/README.md)
- [Enhanced JIT Documentation](docs/xcs/JIT_OVERVIEW.md)
- [Example Applications](src/ember/examples)
  </code>

conftest.py:
<code>
"""
Root conftest.py for pytest configuration
"""

import importlib
import logging
import os
import sys
import warnings
from pathlib import Path

import pytest

# Configure logging to handle closed streams during Python interpreter shutdown

def \_patch_logging_for_shutdown():
"""Patch logging handlers to gracefully handle closed streams at shutdown."""
if not hasattr(logging.StreamHandler, "\_ember_patched"): # Save the original emit method
original_emit = logging.StreamHandler.emit

        # Create a safer version that handles closed file errors
        def safe_emit(self, record):
            try:
                original_emit(self, record)
            except (ValueError, IOError, OSError) as e:
                if "closed" not in str(e).lower() and "closed file" not in str(e).lower():
                    raise

        # Apply the patch
        logging.StreamHandler.emit = safe_emit
        logging.StreamHandler._ember_patched = True

        # Configure problematic loggers
        for name in ["httpcore.connection", "httpcore.http11"]:
            logging.getLogger(name).setLevel(logging.INFO)

# Apply logging patch

\_patch_logging_for_shutdown()

# Setup paths

PROJECT_ROOT = Path(**file**).parent.absolute()
SRC_PATH = PROJECT_ROOT / "src"

print(f"Unit test Python path: {sys.path}")
print(f"Unit test current directory: {os.getcwd()}")

# Add src directory to path

sys.path.insert(0, str(SRC_PATH))
sys.path.insert(0, str(PROJECT_ROOT))

# Configure asyncio

pytest_plugins = ["pytest_asyncio"]

# Silence common warnings

warnings.filterwarnings("ignore", message="._XCS functionality partially unavailable._")

# Configure pytest-asyncio

def pytest_configure(config):
"""Configure pytest-asyncio and register custom marks."""
import pytest_asyncio

    # Use session-scoped event loops by default
    pytest_asyncio.LOOP_SCOPE = "session"

    # Register custom marks
    config.addinivalue_line("markers", "discovery: mark tests that interact with model discovery")
    config.addinivalue_line("markers", "xcs: mark tests related to XCS functionality")
    config.addinivalue_line("markers", "performance: mark tests that measure performance characteristics")

@pytest.hookimpl(tryfirst=True)
def pytest_collection_modifyitems(config, items):
"""Modify test items based on command line options."""
run_all = config.getoption("--run-all-tests")
run_api = config.getoption("--run-api-tests")
run_perf = config.getoption("--run-perf-tests")

    for item in items:
        skip_marks = [mark for mark in item.own_markers if mark.name == "skip"]
        skipif_marks = [mark for mark in item.own_markers if mark.name == "skipif"]

        # Special handling for different test types
        if any(mark.name == "performance" for mark in item.own_markers):
            if run_all or run_perf:
                for mark in skip_marks:
                    item.own_markers.remove(mark)
        elif any("API_KEY" in str(mark.args) for mark in skipif_marks):
            if run_api:
                for mark in skip_marks:
                    item.own_markers.remove(mark)
        elif run_all:
            for mark in skip_marks:
                item.own_markers.remove(mark)

def pytest_addoption(parser):
"""Add custom command line options to pytest."""
parser.addoption(
"--run-perf-tests",
action="store_true",
default=False,
help="Run performance tests that are skipped by default",
)
parser.addoption(
"--run-all-tests",
action="store_true",
default=False,
help="Run all tests including skipped tests (except those requiring API keys)",
)
parser.addoption(
"--run-api-tests",
action="store_true",
default=False,
help="Run tests that require API keys and external services",
)

@pytest.fixture(scope="session", autouse=True)
def \_add_config_helper(request):
"""Add config attribute to pytest module for backward compatibility."""
pytest.config = request.config

@pytest.fixture(scope="session")
def event_loop_policy():
"""Return the event loop policy to use."""
import asyncio
return asyncio.get_event_loop_policy()
</code>

CONTRIBUTING.md:
<code>

# Contributing to Ember

Thank you for your interest in contributing to Ember! This document provides guidelines and instructions for contributing to the project.

## Table of Contents

- [Getting Started](#getting-started)
  - [Development Environment](#development-environment)
  - [Project Structure](#project-structure)
  - [Running Tests](#running-tests)
  - [Code Style and Quality](#code-style-and-quality)
- [Contribution Workflow](#contribution-workflow)
  - [Finding Issues](#finding-issues)
  - [Creating Issues](#creating-issues)
  - [Making Changes](#making-changes)
  - [Pull Requests](#pull-requests)
  - [Code Review](#code-review)
- [Development Guidelines](#development-guidelines)
  - [Documentation](#documentation)
  - [Testing](#testing)
  - [Performance Considerations](#performance-considerations)
  - [Typed Code](#typed-code)
- [Release Process](#release-process)
- [Community](#community)
- [License](#license)

## Getting Started

### Development Environment

1. **Fork and clone the repository**:

   ```bash
   git clone https://github.com/YOUR-USERNAME/ember.git
   cd ember
   ```

2. **Install uv (recommended)**:
   We use uv for dependency management. [Install uv](https://github.com/astral-sh/uv) if you haven't already:

   ```bash
   # On macOS and Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # On Windows
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

   # Or with pip
   pip install uv
   ```

3. **Install dependencies**:

   ```bash
   # Install with all development dependencies
   uv pip install -e ".[dev]"
   ```

4. **Working with the environment**:

   ```bash
   # Option 1: Run commands directly (recommended)
   uv run pytest
   uv run python src/ember/examples/basic/minimal_example.py

   # Option 2: Create and activate a virtual environment
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

5. **Set up pre-commit hooks** (recommended):
   ```bash
   uv run pre-commit install
   # Or if you've activated a virtual environment:
   pre-commit install
   ```

#### Note on Imports

The project is set up with standard Python packaging, so you should import from `ember` directly, rather than from `src.` generally:

```python
# Correct way to import
from ember.core import non
from ember.xcs.tracer import jit

# No need to manipulate sys.path or use symlinks
```

### Project Structure

The Ember codebase is organized into the following structure:

```
ember/
├── core/               # Core framework modules (compatibility with src structure)
│   ├── registry/       # Model registry components
│   └── utils/          # Utility modules
├── docs/               # Documentation
│   ├── design/         # Design documents
│   ├── quickstart/     # Quick start guides
│   └── xcs/            # XCS documentation
├── src/                # Source code
│   └── ember/          # Main Python package
│       ├── api/        # Public API interfaces
│       ├── cli.py      # Python CLI entrypoint
│       ├── core/       # Core framework
│       │   ├── config/ # Configuration management
│       │   ├── registry/ # Registry components
│       │   │   ├── model/   # Model registry
│       │   │   ├── operator/ # Operator registry
│       │   │   └── specification/ # Specifications
│       │   ├── types/  # Type definitions
│       │   └── utils/  # Utility functions
│       ├── examples/   # Example applications
│       │   ├── advanced/ # Advanced examples
│       │   ├── basic/  # Basic examples
│       │   ├── data/   # Data handling examples
│       │   ├── models/ # Model usage examples
│       │   └── operators/ # Operator examples
│       ├── xcs/        # Execution engine
│       │   ├── api/    # XCS API
│       │   ├── engine/ # Engine components
│       │   ├── graph/  # Graph representation
│       │   ├── tracer/ # Tracing functionality
│       │   ├── transforms/ # Transformation utilities
│       │   └── utils/  # XCS utilities
│       └── non.py      # Non-deterministic operations
├── tests/              # Test suite
│   ├── helpers/        # Test helpers
│   ├── integration/    # Integration tests
│   │   ├── core/       # Core integration tests
│   │   ├── performance/ # Performance tests
│   │   ├── tracer/     # Tracer integration tests
│   │   └── xcs/        # XCS integration tests
│   ├── unit/           # Unit tests
│   │   ├── core/       # Core unit tests
│   │   ├── plugin_system/ # Plugin system tests
│   │   └── xcs/        # XCS unit tests
│   └── fuzzing/        # Fuzzing tests
├── pyproject.toml      # Python project configuration
├── poetry.lock         # Dependencies lock file (we're transitioning to uv)
├── pytest.ini          # Pytest configuration
├── mypy.ini            # Type checking configuration
└── README.md           # Project overview
```

The `.gitignore` file is configured to exclude common development files, caches, and sensitive configuration files.

### Running Tests

We use pytest for testing. To run the test suite:

```bash
# Run all tests
uv run pytest

# Run specific tests
uv run pytest tests/unit/core

# Run tests with code coverage
uv run pytest --cov=src/ember

# Run a specific test file
uv run pytest tests/unit/core/test_app_context.py
```

### Code Style and Quality

We enforce high code quality standards:

1. **Code Formatting**:

   - We use Black for code formatting
   - Line length is set to 88 characters
   - Run `uvx black src tests` before committing

2. **Import Sorting**:

   - We use isort for import sorting
   - Run `uvx isort src tests` before committing

3. **Linting**:

   - We use ruff and pylint for linting
   - Run `uvx ruff check src tests` before committing
   - Run `uvx pylint src/ember` for more detailed linting

4. **Type Checking**:
   - We use mypy for static type checking
   - Run `uvx mypy src` before committing

All these checks are also performed automatically when you submit a pull request.

## Contribution Workflow

### Finding Issues

- Check our [issue tracker](https://github.com/pyember/ember/issues) for open issues
- Look for issues tagged with [`good first issue`](https://github.com/pyember/ember/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) if you're new to the project
- Feel free to ask questions in the issue comments if you need clarification

### Creating Issues

When opening a new issue, please:

- **Search existing issues** first to avoid duplicates
- **Use a clear and descriptive title**
- **Follow the issue template** if one is provided
- For bug reports, include:
  - Steps to reproduce
  - Expected behavior
  - Actual behavior
  - Environment details (OS, Python version, etc.)
  - Code samples or error traces when relevant
- For feature requests, explain:
  - The problem you're trying to solve
  - Your proposed solution
  - Alternatives you've considered

### Making Changes

1. **Create a feature branch**:

   ```bash
   git checkout -b feature/your-feature-name
   ```

2. **Make your changes**:

   - Write clean, well-commented code. We attempt to adhere to the `Google Python Style Guide`.
   - Please add/update tests to cover your changes
   - Remember to update documentation as needed
   - Ensure your code passes all tests and style checks

3. **Commit your changes**:

   - Use clear, meaningful commit messages
   - Reference issue numbers where applicable

   ```bash
   git commit -m "Add feature X, fixes #123"
   ```

4. **Keep your branch updated**:
   ```bash
   git fetch origin
   git rebase origin/main
   ```

### Pull Requests

When submitting a pull request:

1. **Fill out the PR template** completely
2. **Link to related issues**
3. **Describe your changes** in detail
4. **Ensure all tests and checks pass**
5. **Include screenshots or examples** for UI or behavior changes
6. **Request reviews** from maintainers or contributors familiar with the area of code

### Code Review

During code review:

- It is your responsibility to get your code reviewed. Feel free to `chase` reviewers as needed, respectfully of course.
- Be patient and respectful. User error is not the default assumption -- assume any question is designer error (either in the implementation or the documentation)
- Remember that the goal is to improve code quality for all of us.

## Development Guidelines

### Documentation

Good documentation is essential:

1. **Docstrings**:

   - All public modules, classes, and functions must have docstrings
   - We follow Google-style docstrings
   - Include type hints in docstrings for complex parameters
   - Example:

   ```python
   def process_data(data: List[Dict[str, Any]], options: Optional[Dict[str, Any]] = None) -> Result:
       """Process input data with optional configuration.

       Args:
           data: List of data dictionaries to process
           options: Optional configuration parameters

       Returns:
           A Result object containing processed output

       Raises:
           ValueError: If data is empty or malformed
       """
   ```

2. **README and Documentation Files**:

   - Update relevant documentation for significant changes
   - Keep examples up-to-date
   - Add new documentation for new features

3. **Code Comments**:
   - Use comments for complex or non-obvious logic
   - Avoid redundant comments that just restate the code
   - Use TODO comments for future improvements (with issue references)

### Testing

We strive for high test coverage:

1. **Test Coverage**:

   - All new code should have corresponding tests
   - We aim for 90%+ code coverage
   - Critical paths should have 100% coverage

2. **Test Types**:

   - **Unit tests**: For testing individual functions and classes in isolation
   - **Integration tests**: For testing interactions between components
   - **Property-based tests**: Using Hypothesis for testing invariants
   - **Fuzzing tests**: For finding edge cases and security issues

3. **Test Naming and Organization**:

   - Test files should be named `test_*.py`
   - Test classes should be named `Test*`
   - Test functions should be named `test_*`
   - Group related tests in the same file or directory

4. **Test Quality**:
   - Tests should be deterministic and reliable
   - Mock external dependencies appropriately, but not excessively
   - Test edge cases and error conditions
   - Include both positive and negative test cases

### Performance Considerations

Performance is important in Ember:

1. **Measurement**:

   - Use profiling tools to identify bottlenecks
   - Include benchmarks for performance-critical code
   - Compare before/after performance for optimizations

2. **Optimizations**:

   - Optimize for readability and maintainability first
   - Focus optimizations on critical paths
   - Document performance trade-offs in comments
   - Use appropriate data structures and algorithms

3. **Concurrency**:
   - Ensure thread safety for shared resources
   - Use appropriate locking mechanisms
   - Consider asynchronous approaches where applicable

### Typed Code

We use Python type hints extensively:

1. **Type Annotations**:

   - Annotate all function parameters and return values
   - Use appropriate generic types when needed
   - Use Optional, Union, and other typing constructs as needed

2. **Type Checking**:

   - Run `mypy` to check for type errors
   - Address all type warnings
   - Use TypeVar and Generic for polymorphic code

3. **Custom Types**:
   - Define new type aliases for complex types
   - Use Protocol for structural typing
   - Document type parameters and constraints

## Release Process

Our release process follows these steps:

1. Feature development in feature branches
2. Pull requests to the main branch after code review
3. Continuous integration tests on all PRs
4. Periodic releases with semantic versioning:
   - MAJOR version for incompatible API changes
   - MINOR version for backwards-compatible functionality
   - PATCH version for backwards-compatible bug fixes
5. Release notes summarizing changes and upgrades

## Community

- **Discussions**: Join our [GitHub Discussions](https://github.com/pyember/ember/discussions) for questions and ideas
- **Issues**: Use [GitHub Issues](https://github.com/pyember/ember/issues) for bug reports and feature requests
- **Slack**: Join our [Slack](https://join.slack.com/t/ember-y0w7887/shared_invite/zt-31nm1aqdz-JtFcRWaatNg11OiUVEhhUw) for real-time discussion

---

Thank you for contributing to Ember! Your time and effort help make this project better for everyone.

## License

By contributing to Ember, you agree that your contributions will be licensed under the project's [MIT License](LICENSE).
</code>

DEBUGGING.md:
<code>

# Ember Examples Debugging

This document tracks the troubleshooting process for fixing failing examples in the Ember framework.

## 1. Operator Composition Example (`composition_example.py`)

### Issue

The example fails with an error when attempting to invoke the "openai:gpt-4o" model.

### Root Cause Analysis

1. **Incorrect Parameter Name**: The `LMModuleConfig` class expects an `id` field, but the code uses `model_name`:

   ```python
   self.lm_module = LMModule(
       config=LMModuleConfig(
           model_name=model_name,  # INCORRECT: should be "id" not "model_name"
           temperature=temperature,
       )
   )
   ```

2. **No Error Handling**: The `QuestionRefinement` operator doesn't include any error handling for model invocation, making the example fragile.

### Fix Plan

1. Update parameter name in `LMModuleConfig` from `model_name` to `id`
2. Add proper error handling to the `QuestionRefinement.forward` method
3. Update the model name in `main()` to ensure it's available (e.g., use "openai:gpt-3.5-turbo" as it's more likely to be available)

## 2. Transformation Example (`transformation_example.py`)

### Issue

The example fails with an error related to invalid input/output types in the `SimpleOperator` class within the `demonstrate_vmap` function.

### Root Cause Analysis

1. **Type Mismatch in vmap**: The `vmap` transformation returns a dictionary of results, but the operator's specification expects a `SimpleOutput` model.

2. **Missing Type Conversion**: The `_combine_outputs` function in `vmap.py` doesn't convert the combined dictionary results back to the expected model type.

### Fix Plan

1. Modify the `_combine_outputs` function in `vmap.py` to check the operator's `specification.structured_output` and convert the combined result dictionary to the appropriate model type.
2. Alternatively, update the `SimpleOperator` class to correctly handle the dictionary output from vmap.

## Architectural Considerations

1. **Unified Error Handling**: Consider a standardized approach for error handling in operators that use LLMs.
2. **Type Compatibility**: Ensure transformations like `vmap` properly respect the type specifications of operators.
3. **Graceful Degradation**: Examples should have graceful error handling to make them more robust, especially for cases involving external APIs.
4. **Automated Testing**: Add tests to catch these issues before they appear in examples.
   </code>

ENVIRONMENT_MANAGEMENT.md:
<code>

# Ember Environment Management Guide

This guide explains how to effectively manage Python environments when working with Ember using uv.

## Python Environment Management with uv

uv provides simplified Python environment management with these benefits:

- **Dependency Isolation**: Prevents conflicts between project dependencies
- **Reproducible Environments**: Ensures consistent behavior across development setups
- **Simplified Workflow**: Reduces the need for explicit environment activation

## Environment Management Approaches

### 1. Using uv's Simplified Environment Management (Recommended)

The simplest approach is to use uv's `run` command, which handles environments automatically:

```bash
# Install Ember
cd ember
uv pip install -e "."

# Run Python code without explicit environment activation
uv run python src/ember/examples/basic/minimal_example.py

# Run tools without explicit environment activation
uv run pytest
```

### 2. Traditional Virtual Environment Workflow

If you prefer a more traditional virtual environment workflow:

```bash
# Create a virtual environment in the project directory
uv venv

# Activate the environment (still required for interactive use)
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install Ember in the active environment
uv pip install -e "."

# Run code in the activated environment
python src/ember/examples/basic/minimal_example.py
```

### 3. Using Other Virtual Environment Tools

If you prefer using other environment managers:

```bash
# Create environment with venv
python -m venv ember_env
source ember_env/bin/activate  # On Windows: ember_env\Scripts\activate

# Install with uv in this environment
uv pip install -e "."
```

## Environment Management Best Practices

1. **Always use isolated environments** - Never install Ember in your global Python environment
2. **For simple usage, use `uv run`** - This handles environment management automatically
3. **For interactive shell work:**
   - Create a virtual environment with `uv venv`
   - Activate it with `source .venv/bin/activate`
4. **For running tools directly** - Use `uvx` which runs tools in isolated environments:
   ```bash
   uvx black src tests
   uvx mypy src
   ```

## Common Environment Commands

```bash
# Create a virtual environment in the current directory
uv venv

# Create a virtual environment with a specific Python version
uv venv --python=3.11

# Install packages
uv pip install -e "."
uv pip install -e ".[dev]"  # With development extras

# Run in an isolated environment
uv run python script.py
uv run pytest tests/
```

## Python Version Management

uv can also manage Python versions:

```bash
# Install Python versions
uv python install 3.10 3.11 3.12

# Use a specific Python version
uv venv --python 3.11
uv run --python 3.11 -- python script.py

# Pin a Python version for a project
uv python pin 3.11  # Creates .python-version
```

## Troubleshooting

### Python Version Issues

```bash
# Check Python version
python --version

# Specify a Python version for a virtual environment
uv venv --python 3.11

# Install a specific Python version with uv
uv python install 3.11
```

### Path Issues

If Python can't find Ember modules:

```bash
# Ensure you're running from the project root
cd /path/to/ember
uv run python src/ember/examples/basic/minimal_example.py
```

### Dependency Resolution Issues

If you encounter dependency conflicts:

```bash
# Use cached resolution if available
uv pip install -e "." --cache-only

# Force re-resolution
uv pip install -e "." --no-cache
```

</code>

INSTALLATION_GUIDE.md:
<code>

# Ember Installation Guide

This guide provides detailed instructions for installing Ember in different environments.

## System Requirements

- **Python**: 3.9 or newer (3.10, 3.11, and 3.12 supported)
- **Operating System**: macOS, Linux, or Windows

## Installation Methods

### Method 1: Basic Installation with uv (Recommended)

[uv](https://astral.sh/uv) is the recommended package manager for Ember. It is extremely fast (10-100x faster than pip) and simplifies Python environment management.

1. **Install uv** if you don't have it already:

   ```bash
   # On macOS and Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # On Windows
   powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

   # Or with pip if you prefer
   pip install uv

   # Verify the installation
   uv --version
   ```

2. **Install Ember from PyPI**:

   ```bash
   # Install Ember directly (creates a virtual environment automatically if needed)
   uv pip install ember-ai

   # Run examples without activating an environment
   ```

3. **Install from source**:

   ```bash
   # Clone the repository
   git clone https://github.com/pyember/ember.git
   cd ember

   # Install in development mode (editable installation)
   uv pip install -e "."

   # Run examples directly without environment activation
   uv run python src/ember/examples/basic/minimal_example.py
   ```

   By default, this installs Ember with OpenAI, Anthropic, and Google/Deepmind provider support.

### Method 2: Development Installation with uv

If you want to develop or contribute to Ember:

1. **Clone the repository**:

   ```bash
   git clone https://github.com/pyember/ember.git
   cd ember
   ```

2. **Install with development dependencies**:

   ```bash
   # Install including development dependencies
   uv pip install -e ".[dev]"

   # Run commands directly with uv
   uv run pytest
   ```

3. **Running tools**:
   ```bash
   # Run linters, formatters, and other tools without installation
   uvx black src tests
   uvx mypy src
   uvx pytest
   ```

### Method 3: Traditional pip Installation (Alternative)

If you prefer using standard pip or don't want to install uv:

```bash
# Create a virtual environment (recommended)
python -m venv ember_env
source ember_env/bin/activate  # On Windows: ember_env\Scripts\activate

# Install Ember with pip
pip install ember-ai

# For development installation
pip install -e ".[dev]"
```

Note: This method is significantly slower for dependency resolution and doesn't provide the environment management benefits of uv.

## OS-Specific Installation Notes

### macOS

On macOS, you might encounter issues with the default Python installation:

```bash
# If you encounter Python-related errors:
# Install Python using Homebrew (recommended)
brew install python@3.11

# Use the Homebrew Python with uv
/opt/homebrew/bin/python3.11 -m pip install uv
/opt/homebrew/bin/python3.11 -m uv pip install -e "."
```

### Windows

On Windows, ensure you have the latest Python installed from python.org:

```powershell
# Add uv to your PATH if needed
$env:PATH += ";$env:USERPROFILE\.uv\bin"

# Install and run directly
uv pip install -e "."
uv run python src/ember/examples/basic/minimal_example.py
```

## Troubleshooting

### Python Version Issues

If you encounter Python version errors:

```bash
# Check your Python version
python --version

# Specify a Python version for uv
uv venv --python=3.11
source .venv/bin/activate

# Or run with a specific Python version
uv run --python=3.11 -- python script.py
```

### uv Installation Issues

If you have problems with uv:

```bash
# Ensure uv is in your PATH
which uv

# Update uv to the latest version
uv self update

# Reinstall uv if needed
pip install --upgrade uv
```

### Virtual Environment Issues

If you have problems with virtual environments:

```bash
# Create a fresh virtual environment
uv venv --force

# Activate the environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

See [ENVIRONMENT_MANAGEMENT.md](ENVIRONMENT_MANAGEMENT.md) for more details on managing environments.

### Dependency Conflicts

If you encounter dependency conflicts:

```bash
# Try reinstalling without using cache
uv pip install -e "." --no-cache

# Install with specific package versions if needed
uv pip install -e "." --no-deps
uv pip install "specific-package==version"
```

### Other Known Installation Issue Resolutions

When using conda with or without uv, you may encounter known pyarrow installation issues.

```
# Try installing pyarrow from conda-forge
conda install -c conda-forge pyarrow
```

## Testing Your Installation

After installation, verify everything is working:

```bash
# From the project root directory, using uv
uv run python src/ember/examples/basic/minimal_example.py

# Or if you're in an activated virtual environment
python src/ember/examples/basic/minimal_example.py
```

## Getting Help

If you encounter issues with installation:

- Check our [GitHub Issues](https://github.com/pyember/ember/issues)
- Review the [ENVIRONMENT_MANAGEMENT.md](ENVIRONMENT_MANAGEMENT.md) guide
- See the [TESTING_INSTALLATION.md](TESTING_INSTALLATION.md) for verification steps
  </code>

LICENSE:
<code>
MIT License

Copyright (c) 2025 Foundry Technologies, Inc (mlfoundry.com)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</code>

LLM_SPECIFICATIONS.md:
<code>

# Ember Framework: LLM Specifications

This document provides precise specifications for building systems with the Ember framework. It defines essential patterns, APIs, and concepts with minimal overhead.

## Table of Contents

- [Core Concepts](#core-concepts)
- [Architecture Overview](#architecture-overview)
- [Model Registry](#model-registry)
- [Operators](#operators)
- [Execution Engine (XCS)](#execution-engine-xcs)
- [Network of Networks (NON)](#network-of-networks-non)
- [Data System](#data-system)
- [Context System](#context-system)
- [Code Style Guidelines](#code-style-guidelines)
- [Examples and Templates](#examples-and-templates)

## Core Concepts

### EmberModel

Base class for structured input/output with type validation:

```python
from ember.core.types.ember_model import EmberModel, Field

class QueryInput(EmberModel):
    query: str = Field(..., description="Query to process")
    temperature: float = Field(0.7, description="Sampling temperature")

class AnswerOutput(EmberModel):
    answer: str = Field(..., description="Generated response")
    confidence: float = Field(..., description="Confidence score 0-1")
```

### Operator

Fundamental computational unit with typed I/O:

```python
from typing import ClassVar
from ember.api.operators import Operator, Specification

class MySpec(Specification):
    input_model = QueryInput
    structured_output = AnswerOutput

class MyOperator(Operator[QueryInput, AnswerOutput]):
    specification: ClassVar[Specification] = MySpec()

    def forward(self, *, inputs: QueryInput) -> AnswerOutput:
        # Implementation here
        return AnswerOutput(answer="...", confidence=0.9)
```

### JIT Compilation

Execute optimized computational graphs:

```python
from ember.xcs import jit

@jit  # Auto-optimize execution path
class FastOperator(Operator[QueryInput, AnswerOutput]):
    # Implementation with parallel execution where possible
```

## Architecture Overview

Ember's layered architecture:

1. **Execution Engine (XCS)** - Base layer providing computation graph definition and execution
2. **Core Component Layer** - Building blocks including model registry, operators, specifications
3. **Application Layer** - High-level abstractions like NON patterns and auto graph building
4. **Public API Layer** - Clean interfaces exposed through the `ember.api` namespace

## Model Registry

The model registry provides unified access to LLM providers:

### Function-Style API (Recommended)

```python
from ember.api import models

# Direct invocation
response = models.model("gpt-4o")("What is quantum computing?")

# Provider namespaces
response = models.openai.gpt4o("What is quantum computing?")

# Reusable models
gpt4 = models.model("gpt-4o", temperature=0.7)
response1 = gpt4("Question 1")
response2 = gpt4("Question 2")

# Configuration context
with models.configure(temperature=0.2, max_tokens=100):
    response = models.model("gpt-4o")("Write a haiku")
```

### Type-Safe Enum References

```python
from ember.api.models import ModelEnum
response = models.from_enum(ModelEnum.OPENAI_GPT4O)("Hello")
```

### Builder Pattern (Alternative)

```python
from ember.api.models import ModelBuilder
model = (ModelBuilder()
    .temperature(0.7)
    .max_tokens(100)
    .build("anthropic:claude-3-5-sonnet"))
response = model.generate("Explain quantum computing")
```

### Custom Contexts

```python
from ember.api.models import ModelContext, ContextConfig
context = ModelContext(config=ContextConfig(
    api_keys={"openai": "your-key"}
))
response = models.model("gpt-4o", context=context)("Hello")
```

## Operators

### Basic Operator Pattern

```python
from typing import ClassVar
from ember.api.operators import Operator, Specification, EmberModel
from ember.api import models

class InputType(EmberModel):
    query: str

class OutputType(EmberModel):
    answer: str

class MySpec(Specification):
    input_model = InputType
    structured_output = OutputType

class MyOperator(Operator[InputType, OutputType]):
    # Class-level specification
    specification: ClassVar[Specification] = MySpec()

    # Declare instance attributes
    model: object

    def __init__(self, model_name: str = "gpt-4o"):
        self.model = models.model(model_name)

    def forward(self, *, inputs: InputType) -> OutputType:
        response = self.model(inputs.query)
        return OutputType(answer=str(response))
```

### Composition Pattern

```python
class Pipeline(Operator[InputType, OutputType]):
    specification: ClassVar[Specification] = MySpec()

    # Declare component operators
    refiner: QueryRefiner
    answerer: AnswerGenerator

    def __init__(self):
        self.refiner = QueryRefiner()
        self.answerer = AnswerGenerator()

    def forward(self, *, inputs: InputType) -> OutputType:
        refined = self.refiner(inputs=inputs)
        return self.answerer(inputs=refined)
```

### Built-in Operators

```python
from ember.api.operators import (
    EnsembleOperator,
    MostCommonAnswerSelector,
    VerifierOperator,
    SelectorJudgeOperator,
    JudgeSynthesisOperator
)

# Ensemble of models
ensemble = EnsembleOperator(
    operators=[
        MyOperator(model="gpt-4o"),
        MyOperator(model="claude-3-5-sonnet"),
    ]
)

# Selector for aggregation
pipeline = MostCommonAnswerSelector(
    operator=ensemble
)
```

## Execution Engine (XCS)

### Basic JIT

```python
from ember.xcs import jit

@jit
class MyOperator(Operator):
    def forward(self, *, inputs):
        # Implementation
        return result
```

### JIT with Strategy Selection

```python
from ember.xcs import jit

# Auto-select optimal strategy
@jit
class MyOperator(Operator):
    # Implementation...

# Explicit strategy
@jit(mode="enhanced")
class ComplexOperator(Operator):
    # Implementation...
```

### Execution Options

```python
from ember.xcs import execution_options

# Configure execution parameters
with execution_options(scheduler="wave", max_workers=4):
    result = pipeline(query="Complex question...")
```

### Transformations

```python
from ember.xcs import vmap, pmap, compose

# Vectorized mapping for batch processing
batch_processor = vmap(my_operator)
batch_results = batch_processor(inputs={"data": [item1, item2, item3]})

# Parallel execution across multiple workers
parallel_processor = pmap(my_operator, num_workers=4)
parallel_results = parallel_processor(inputs=complex_data)

# Compose transformations
pipeline = compose(vmap(batch_size=32), pmap(num_workers=4))(my_operator)
```

## Network of Networks (NON)

### Standard API

```python
from ember.api import non

# Create ensemble of identical models
ensemble = non.UniformEnsemble(
    num_units=3,
    model_name="openai:gpt-4o",
    temperature=0.7
)

# Create judge to synthesize outputs
judge = non.JudgeSynthesis(
    model_name="anthropic:claude-3-5-sonnet",
    temperature=0.0
)

# Create sequential pipeline
pipeline = non.Sequential(operators=[ensemble, judge])

# Execute pipeline
result = pipeline(query="What causes tsunamis?")
```

### Compact Notation

```python
from ember.api import non

# Same pipeline with compact notation
pipeline = non.build_graph([
    "3:E:gpt-4o:0.7",              # Ensemble with 3 instances
    "1:J:claude-3-5-sonnet:0.0"    # Judge synthesis
])

# Execute with identical interface
result = pipeline(query="What causes tsunamis?")
```

### Component Reuse

```python
# Define reusable components
components = {
    "sub": ["2:E:gpt-4o:0.0", "1:V:gpt-4o:0.0"]  # Ensemble → Verifier
}

# Create branch architecture
nested = non.build_graph([
    "$sub",                # First branch
    "$sub",                # Second branch
    "1:J:gpt-4o:0.0"       # Final judge
], components=components)
```

### Custom Operator Types

```python
# Create a registry with custom operator types
registry = non.OpRegistry.create_standard_registry()
registry.register(
    "CE",  # Custom ensemble type
    lambda count, model, temp: non.Sequential(operators=[
        non.UniformEnsemble(num_units=count, model_name=model, temperature=temp),
        non.MostCommon()  # Auto-aggregation
    ])
)

# Use custom operator type
pipeline = non.build_graph(["3:CE:gpt-4o:0.7"], type_registry=registry)
```

## Data System

### DatasetBuilder Pattern

```python
from ember.api.data import DatasetBuilder

# Load and transform a dataset
dataset = (DatasetBuilder()
    .from_registry("mmlu")    # Use a registered dataset
    .subset("physics")        # Select a specific subset
    .split("test")            # Choose the test split
    .sample(100)              # Random sample of 100 items
    .transform(               # Apply transformations
        lambda x: {"query": f"Question: {x['question']}"}
    )
    .build())
```

### Evaluation Pipeline

```python
from ember.api.eval import EvaluationPipeline, Evaluator

# Create an evaluation pipeline
eval_pipeline = EvaluationPipeline([
    # Standard metrics
    Evaluator.from_registry("accuracy"),
    Evaluator.from_registry("response_quality"),

    # Custom metrics
    Evaluator.from_function(
        lambda prediction, reference: {
            "factual_accuracy": score_factual_content(prediction, reference)
        }
    )
])

# Evaluate a model or operator
results = eval_pipeline.evaluate(my_model, dataset)
print(f"Accuracy: {results['accuracy']:.2f}")
```

## Context System

### Basic Context Usage

```python
from ember.core.context import current_context

# Get the current thread's context
ctx = current_context()

# Get a model
model = ctx.get_model("gpt4o")

# Generate text
result = model.generate("Hello, world!")
```

### Temporary Components

```python
from ember.core.context import current_context, temp_component

# Use a temporary component
with temp_component("model", "temp-model", MyModel("temporary")) as model:
    # Use the model within this scope
    result = model.generate("Hello")
```

### Configuration Access

```python
from ember.core.context import current_context
from ember.core.context.config_integration import config_override

# Access configuration through context
temperature = ctx.config.model.temperature

# Override configuration temporarily
with config_override({"model": {"temperature": 0.2}}):
    # Config value changed in this scope
    new_temp = ctx.config.model.temperature
```

## Code Style Guidelines

1. **Package Structure**:

   - Use the `ember.api` namespace for clean imports
   - Follow the layered import pattern:
     ```python
     from ember.api.models import ModelBuilder
     from ember.api.operators import Operator
     from ember.api.xcs import jit
     from ember.api import non
     ```

2. **Naming Conventions**:

   - Classes: `PascalCase`
   - Functions: `snake_case`
   - Constants: `UPPER_SNAKE_CASE`
   - Private attributes: `_leading_underscore`

3. **Typing**:

   - Use explicit type annotations
   - Leverage TypeVars for generic operators
   - Include descriptions in Field definitions

4. **Documentation**:

   - Follow Google docstring format
   - Document parameters, returns, and raises
   - Include examples for non-trivial usage

5. **Error Handling**:
   - Use specific exception types
   - Include helpful error messages
   - Handle API errors gracefully

## Examples and Templates

### Basic Operator Template

```python
from typing import ClassVar
from ember.api.operators import Operator, Specification, EmberModel, Field

class MyInput(EmberModel):
    query: str = Field(..., description="The input query")

class MyOutput(EmberModel):
    result: str = Field(..., description="The computed result")

class MySpec(Specification):
    input_model = MyInput
    structured_output = MyOutput

class MyOperator(Operator[MyInput, MyOutput]):
    specification: ClassVar[Specification] = MySpec()

    def __init__(self, param1: str):
        self.param1 = param1

    def forward(self, *, inputs: MyInput) -> MyOutput:
        # Implementation logic
        return MyOutput(result=f"Processed: {inputs.query}")
```

### NON Pattern Template

```python
from ember.api import non

def create_ensemble_judge_pipeline(
    ensemble_size: int = 3,
    model_name: str = "openai:gpt-4o",
    judge_model: str = "anthropic:claude-3-5-sonnet"
) -> non.Sequential:
    """Create an ensemble-judge pipeline.

    Args:
        ensemble_size: Number of ensemble units
        model_name: Model to use for ensemble
        judge_model: Model to use for judge

    Returns:
        A configured pipeline
    """
    return non.Sequential(operators=[
        non.UniformEnsemble(
            num_units=ensemble_size,
            model_name=model_name,
            temperature=0.7
        ),
        non.JudgeSynthesis(
            model_name=judge_model,
            temperature=0.0
        )
    ])
```

### Complete Application Example

```python
"""Minimal Ember application with JIT optimization."""

from typing import ClassVar
from ember.api import models, non
from ember.api.operators import Operator, Specification, EmberModel
from ember.xcs import jit

# Define I/O types
class QuestionInput(EmberModel):
    question: str

class AnswerOutput(EmberModel):
    answer: str
    confidence: float

# Define specification
class QuestionSpec(Specification):
    input_model = QuestionInput
    structured_output = AnswerOutput

# Define JIT-optimized operator
@jit
class QuestionAnswerer(Operator[QuestionInput, AnswerOutput]):
    specification: ClassVar[Specification] = QuestionSpec()

    # Declare instance attributes
    ensemble: non.UniformEnsemble
    judge: non.JudgeSynthesis

    def __init__(self, width: int = 3):
        self.ensemble = non.UniformEnsemble(
            num_units=width,
            model_name="gpt-4o",
            temperature=0.7
        )
        self.judge = non.JudgeSynthesis(
            model_name="claude-3-5-sonnet"
        )

    def forward(self, *, inputs: QuestionInput) -> AnswerOutput:
        # Get ensemble responses
        ensemble_result = self.ensemble(query=inputs.question)

        # Synthesize with judge
        synthesis = self.judge(
            query=inputs.question,
            responses=ensemble_result["responses"]
        )

        # Build response
        return AnswerOutput(
            answer=synthesis["synthesized_response"],
            confidence=float(synthesis.get("confidence", 0.8))
        )

# Main entrypoint
def main():
    # Create operator
    answerer = QuestionAnswerer(width=5)

    # Process question
    result = answerer(inputs=QuestionInput(
        question="What is relativity?"
    ))

    # Use result
    print(f"Answer: {result.answer}")
    print(f"Confidence: {result.confidence:.2f}")

if __name__ == "__main__":
    main()
```

</code>

mypy.ini:
<code>
[mypy]
python_version = 3.8
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_incomplete_defs = True
check_untyped_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True
strict_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_no_return = True
warn_unreachable = True

[mypy.plugins.pydantic.*]
implicit_reexport = True

[mypy.plugins.ember.*]
implicit_reexport = True

[mypy-pytest.*]
ignore_missing_imports = True

[mypy-numpy.*]
ignore_missing_imports = True
</code>

pyproject.toml:
<code>
[project]
name = "ember-ai"
version = "0.1.0"
description = "Compositional framework for building and orchestrating Compound AI Systems and Networks of Networks (NONs)."
authors = [
{name = "Jared Quincy Davis", email = "jaredq@cs.stanford.edu"}
]
maintainers = [
{name = "Ember Team", email = "team@pyember.org"}
]
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.9,<3.13"
classifiers = [
"Development Status :: 4 - Beta",
"Intended Audience :: Developers",
"Intended Audience :: Science/Research",
"Intended Audience :: Information Technology",
"License :: OSI Approved :: MIT License",
"Programming Language :: Python :: 3",
"Programming Language :: Python :: 3.11",
"Topic :: Scientific/Engineering :: Artificial Intelligence",
"Topic :: Software Development :: Libraries :: Python Modules",
"Topic :: Software Development :: Libraries :: Application Frameworks",
"Operating System :: OS Independent",
]
keywords = ["AI", "LLM", "Networks of Networks", "Machine Learning", "Compound AI", "Orchestration", "AI Framework", "LLM Orchestration", "AI System Design"]
dependencies = [ # Core dependencies
"pandas>=1.0.0,<2.2.0",
"numpy>=1.21.0,<1.27.0",
"pydantic>=2.7.4",
"pydantic-core>=2.18.4",
"pydantic-settings>=2.3.0",
"PyYAML>=6.0.1",
"typing_extensions>=4.12.2",

    # LLM providers - Core providers
    "openai>=1.57.2",
    "anthropic>=0.40.0",
    "google-generativeai>=0.8.3",

    # HTTP and async - Critical core functionality
    "aiohttp>=3.9.5",
    "aiosignal>=1.3.1",
    "httpx>=0.27.0",
    "requests>=2.32.2",

    # Data processing
    "datasets>=2.20.0",

    # Core utilities - Required for robust operation
    "tqdm>=4.67.1",
    "tenacity>=9.0.0",
    "cachetools>=5.4.0",
    "dill>=0.3.8",
    "prettytable>=3.12.0",  # Required for examples

    # Basic required dependencies
    "annotated-types>=0.7.0",
    "anyio>=4.4.0",
    "attrs>=23.2.0",
    "certifi>=2024.6.2",
    "charset-normalizer>=3.3.2",
    "packaging>=24.1",
    "six>=1.16.0",
    "idna>=3.7",
    "urllib3>=1.26.19,<2.0.0",
    "sniffio>=1.3.1",

]

[project.optional-dependencies]

# Main extras groups

all = [
"openai>=1.57.2",
"anthropic>=0.40.0",
"google-generativeai>=0.8.3",
"scikit-learn>=1.6.0",
"scipy>=1.13.1",
"huggingface-hub>=0.26.5",
"pyarrow>=16.1.0",
"pyarrow-hotfix>=0.6",
"matplotlib>=3.9.1",
"prettytable>=3.12.0",
"google-ai-generativelanguage>=0.6.6",
"google-api-core>=2.19.1",
"google-api-python-client>=2.139.0",
"google-auth>=2.32.0",
"google-auth-httplib2>=0.2.0",
"googleapis-common-protos>=1.63.2",
"pytest>=8.3.2",
"pytest-asyncio>=0.23.4",
"parameterized>=0.9.0",
"pytest-cov>=4.1.0",
"hypothesis>=6.99.0",
"mutmut>=2.4.4",
"tox>=4.11.4",
"jupyterlab>=4.0.6",
"ipykernel>=6.26.0",
"black>=23.12.0",
"isort>=5.12.0",
"mypy>=1.7.1",
"pylint>=3.0.2",
"pre-commit>=3.5.0",
"ruff>=0.1.6",
"sphinx>=7.1.0",
"sphinx-rtd-theme>=1.3.0",
"nbsphinx>=0.9.3",
"myst-parser>=2.0.0",
"jupyter",
"notebook",
]
minimal = ["openai>=1.57.2"] # Minimal viable installation with just OpenAI

# Provider-specific extras

openai = ["openai>=1.57.2"]
anthropic = ["anthropic>=0.40.0"]
google = [
"google-generativeai>=0.8.3",
"google-ai-generativelanguage>=0.6.6",
"google-api-core>=2.19.1",
"google-api-python-client>=2.139.0",
"google-auth>=2.32.0",
"google-auth-httplib2>=0.2.0",
"googleapis-common-protos>=1.63.2",
]
allproviders = [
"openai>=1.57.2",
"anthropic>=0.40.0",
"google-generativeai>=0.8.3",
]

# Feature-specific extras

data = [
"datasets>=2.20.0",
"scikit-learn>=1.6.0",
"scipy>=1.13.1",
"huggingface-hub>=0.26.5",
"pyarrow>=16.1.0",
"pyarrow-hotfix>=0.6",
]
viz = [
"matplotlib>=3.9.1",
"prettytable>=3.12.0",
]

# Developer extras

dev = [
"pytest>=8.3.2",
"pytest-asyncio>=0.23.4",
"parameterized>=0.9.0",
"pytest-cov>=4.1.0",
"hypothesis>=6.99.0",
"mutmut>=2.4.4",
"tox>=4.11.4",
"jupyterlab>=4.0.6",
"ipykernel>=6.26.0",
"black>=23.12.0",
"isort>=5.12.0",
"mypy>=1.7.1",
"pylint>=3.0.2",
"pre-commit>=3.5.0",
"ruff>=0.1.6",
]
docs = [
"sphinx>=7.1.0",
"sphinx-rtd-theme>=1.3.0",
"nbsphinx>=0.9.3",
"myst-parser>=2.0.0",
"jupyter",
"notebook",
]

[project.urls]
repository = "https://github.com/pyember/ember"
documentation = "https://docs.pyember.org"
homepage = "https://pyember.org"

[project.entry-points."ember.dataset_preppers"]
truthful_qa = "ember.core.utils.data.datasets_registry.truthful_qa:TruthfulQAPrepper"
short_answer = "ember.core.utils.data.datasets_registry.short_answer:ShortAnswerPrepper"
commonsense_qa = "ember.core.utils.data.datasets_registry.commonsense_qa:CommonsenseQAPrepper"
halueval = "ember.core.utils.data.datasets_registry.halueval:HaluEvalPrepper"
mmlu = "ember.core.utils.data.datasets_registry.mmlu:MMLUPrepper"

[tool.pytest.ini_options]
pythonpath = [
"src",
"tests"
]
testpaths = ["tests"]
python*files = "test*_.py"
python_classes = "Test_"
python*functions = "test*\*"
addopts = "--import-mode=importlib --cov=src/ember --cov-report=term --cov-report=html --cov-report=xml"

[tool.coverage.run]
source = ["src/ember"]
omit = ["*/__init__.py", "*/test_*.py", "tests/*"]
branch = true

[tool.coverage.report]
exclude_lines = [
"pragma: no cover",
"def __repr__",
"raise NotImplementedError",
"if TYPE_CHECKING",
"pass",
"\\.\\.\\."
]
fail_under = 90
show_missing = true

[tool.hypothesis]
deadline = 500
max_examples = 100
verbosity = 1
suppress_health_check = ["too_slow"]

[tool.black]
line-length = 88
target-version = ["py39"]
include = '\.pyi?$'

[tool.isort]
profile = "black"
line_length = 88
multi_line_output = 3

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
strict_optional = true

[tool.ruff]
line-length = 88
target-version = "py39"

[tool.ruff.lint]
select = ["E", "F", "B", "I"]
ignore = []

[build-system]
requires = ["setuptools>=68.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
package-dir = {"" = "src"}
packages = ["ember"]

# CLI is being developed separately and is currently excluded

# [project.scripts]

# ember = "ember.cli:main"

</code>

pytest.ini:
<code>
[pytest]
markers =
performance: mark test as a performance test.
integration: mark test as an integration test requiring external dependencies.
no*collect: mark a class or module so that pytest will not collect it as a test.
api_dependent: mark tests that depend on the ember.api module.
run_perf_tests: mark tests that should only run when performance benchmarking is enabled.
perf: mark test as a performance test for data context.
concurrency: mark test as testing concurrent behavior.
pythonpath =
.
src
addopts = --import-mode=importlib --deselect=tests/integration/core/utils/data/test_data_api_facade.py::TestDataAPIFacade::test_custom_dataset_with_transformations
filterwarnings =
ignore::DeprecationWarning:pkg_resources:3154
ignore:The configuration option:pytest.PytestDeprecationWarning
ignore::RuntimeWarning:unittest.mock:2105
ignore:coroutine '.*' was never awaited:RuntimeWarning
ignore:cannot collect test class .\_:pytest.PytestCollectionWarning
ignore:urllib3 v2 only supports OpenSSL 1.1.1:UserWarning

# Set default asyncio mode to strict

asyncio_mode = strict

# Integration Test Instructions:

# To run integration tests, use the following command:

# RUN_INTEGRATION_TESTS=1 python -m pytest tests/integration -v

#

# To run tests that make actual API calls:

# RUN_INTEGRATION_TESTS=1 ALLOW_EXTERNAL_API_CALLS=1 python -m pytest tests/integration -v

#

# To run performance tests:

# python -m pytest --run-perf-tests tests/unit/xcs/transforms -v

</code>

QUICKSTART.md:
<code>

# Ember Quickstart Guide

This guide will help you quickly get started with Ember, the compositional framework for building and orchestrating Compound AI Systems.

## Installation

```bash
# Install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh  # macOS/Linux
# or
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"  # Windows
# or
pip install uv  # Any platform

# Quick install with uv (recommended)
uv pip install ember-ai

# Or from source
git clone https://github.com/pyember/ember.git
cd ember
uv pip install -e ".[dev]"  # Install with development dependencies

# Run examples directly with uv
uv run python src/ember/examples/basic/minimal_example.py

# For CLI tools, you can run directly without installation
uvx ember-ai <arguments>  # If you have CLI commands
```

For detailed installation instructions including troubleshooting, please see:

- [INSTALLATION_GUIDE.md](INSTALLATION_GUIDE.md) - Complete installation instructions
- [ENVIRONMENT_MANAGEMENT.md](ENVIRONMENT_MANAGEMENT.md) - Guide to managing Python environments
- [TESTING_INSTALLATION.md](TESTING_INSTALLATION.md) - Steps to verify your installation

## Setting Up API Keys and Configuration

Ember supports multiple ways to configure API keys for LLM providers.

### Option 1: Environment Variables

Set your API keys as environment variables:

```bash
# For bash/zsh
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"

# For making environment variables persistent (add to your shell profile)
echo 'export OPENAI_API_KEY="your-openai-key"' >> ~/.bashrc  # or ~/.zshrc
```

### Option 2: Configuration File

Create a configuration file at one of these locations (searched in order):

1. Current directory: `./config.yaml`
2. User home config: `~/.ember/config.yaml`
3. System config: `/etc/ember/config.yaml`

Example configuration file:

```yaml
model_registry:
  providers:
    openai:
      api_key: ${OPENAI_API_KEY} # Will use environment variable
      organization_id: "your-org-id" # Optional organization ID
    anthropic:
      api_key: "your-anthropic-key" # Direct value (example)
    google:
      api_key: ${GOOGLE_API_KEY} # Will use environment variable
```

### Option 3: Programmatic Configuration

Set configuration values directly in your code:

```python
import ember
from ember.core.config.manager import ConfigManager

# Initialize with custom configuration
config = ConfigManager()
config.set("model_registry.providers.openai.api_key", "your-openai-key")
config.set("model_registry.providers.anthropic.api_key", "your-anthropic-key")

# Initialize Ember with this configuration
service = ember.init(config=config)
```

### Setting Provider-Specific Options

You can configure provider-specific options in your configuration file:

```yaml
model_registry:
  providers:
    openai:
      api_key: ${OPENAI_API_KEY}
      base_url: "https://api.openai.com/v1" # Custom API endpoint
      timeout: 30 # Timeout in seconds
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      max_retries: 3
```

See [Configuration Quickstart](docs/quickstart/configuration.md) for more options and detailed configuration examples.

## Basic Usage

Here's how to get started with Ember in just a few lines of code:

```python
# Import the package
import ember

# Initialize and get the 'default' model service
service = ember.init()

# Make a simple model call
response = service("openai:gpt-4o", "What is the capital of France?")
print(response.data)
```

## Building Your First Compound AI System

Let's create a simple example that uses multiple models with parallelization:

```python
from typing import ClassVar
from ember.api.xcs import jit
from ember.api.operator import Operator, Specification
from ember.api.models import EmberModel
from ember.core import non

# Define structured input/output models
class QueryInput(EmberModel):
    query: str

class QueryOutput(EmberModel):
    answer: str
    confidence: float

# Define the specification
class QuerySpecification(Specification):
    input_model = QueryInput
    structured_output = QueryOutput

# Create a compound system using the @jit decorator for optimization
@jit
class ParallelQuerySystem(Operator[QueryInput, QueryOutput]):
    # Class-level specification declaration with ClassVar
    specification: ClassVar[Specification] = QuerySpecification()

    # Class-level field declarations
    ensemble: non.UniformEnsemble
    aggregator: non.MostCommon

    def __init__(self, parallel_calls: int = 3, model: str = 'openai: gpt-4o-mini', temp: float = 0.4):
        # Init ensemble
        self.ensemble = non.UniformEnsemble(
            num_units=parallel_calls,
            model_name=model,
            temperature=temp
        )

        # Aggregate ensemble responses with "Most Common", "voting-bsed" aggregation
        self.aggregator = non.MostCommon()

    def forward(self, *, inputs: QueryInput) -> QueryOutput:
        # Get responses from multiple models (automatically parallelized)
        ensemble_result = self.ensemble(inputs={"query": inputs.query})

        # Aggregate the results (input dict format for operator invocation, vs. kwargs format in README.md. Both are supported.)
        aggregated = self.aggregator(inputs={
            "query": inputs.query,
            "responses": ensemble_result["responses"]
        })

        # Return structured output
        return QueryOutput(
            answer=aggregated["final_answer"],
            confidence=aggregated.get("confidence", 0.0)
        )

# Create and use the system
system = ParallelQuerySystem()
result = system(inputs={"query": "What is the speed of light?"})

print(f"Answer: {result.answer}")
print(f"Confidence: {result.confidence:.2f}")
```

## Next Steps

- Explore the [Model Registry](docs/quickstart/model_registry.md) for using different LLM providers
- Learn about [Operators](docs/quickstart/operators.md) for building reusable components
- Check out [Networks of Networks](docs/quickstart/non.md) for complex AI systems
- Set up [Data Processing](docs/quickstart/data.md) for dataset handling
- Configure your application with [Configuration](docs/quickstart/configuration.md)
- Use [Simplified Imports](SIMPLIFIED_IMPORTS.md) for cleaner code

To see Ember in action, explore these key examples:

- [Minimal Example](src/ember/examples/minimal_example.py) - Get started with basic usage
- [Model API Example](src/ember/examples/model_api_example.py) - Learn the models API
- [Ensemble Operator Example](src/ember/examples/diverse_ensemble_operator_example.py) - Build parallel model ensembles
- [API Operators Example](src/ember/examples/api_operators_example.py) - Use streamlined imports
- [Enhanced JIT Example](src/ember/examples/enhanced_jit_example.py) - Optimize execution with JIT

For a full walkthrough of Ember's capabilities, see the [Examples Directory](src/ember/examples).

## Getting Help

- GitHub Issues: [https://github.com/pyember/ember/issues](https://github.com/pyember/ember/issues)
- Documentation: See the documentation files in the `docs/` directory
- Examples: Explore the examples in `src/ember/examples/`
  </code>

README.md:
<code>

<p align="center">
  <img src="docs/assets/logo_ember_icon@2x.png" alt="Ember Logo" width="150"/>
</p>
<p align="center">
  <img src="docs/assets/ember_workmark.svg" alt="Ember" width="350"/>
</p>

<p align="center">
<strong>Contributors</strong>
</p>

<p align="center">
This repository is in collaboration with the following early users, contributors, and reviewers:
</p>

<p align="center">
Jared Quincy Davis<sup>F,S</sup>, Marquita Ellis<sup>I</sup>, Diana Arroyo<sup>I</sup>, Pravein Govindan Kannan<sup>I</sup>, Paul Castro<sup>I</sup>, Siddharth Sharma<sup>F,S</sup>, Lingjiao Chen<sup>MS</sup>, Omar Khattab<sup>D,MT</sup>, Alan Zhu<sup>B</sup>, Parth Asawa<sup>B</sup>, Connor Chow<sup>B</sup>, Jason Lee<sup>B</sup>, Jay Adityanag Tipirneni<sup>B</sup>, Chad Ferguson<sup>B</sup>, Kathleen Ge<sup>B</sup>, Kunal Agrawal<sup>B</sup>, Rishab Bhatia<sup>B</sup>, Rohan Penmatcha<sup>B</sup>, Sai Kolasani<sup>B</sup>, Théo Jaffrelot Inizan<sup>B</sup>, Deepak Narayanan<sup>N</sup>, Long Fei<sup>F</sup>, Aparajit Raghavan<sup>F</sup>, Eyal Cidon<sup>F</sup>, Jacob Schein<sup>F</sup>, Prasanth Somasundar<sup>F</sup>, Boris Hanin<sup>F,P</sup>, James Zou<sup>S</sup>, Alex Dimakis<sup>B</sup>, Joey Gonzalez<sup>B</sup>, Peter Bailis<sup>G,S</sup>, Ion Stoica<sup>A,B,D</sup>, Matei Zaharia<sup>D,B</sup>
</p>

<p align="center">
<sup>F</sup> Foundry (MLFoundry), <sup>D</sup> Databricks, <sup>I</sup> IBM Research, <sup>S</sup> Stanford University, <sup>B</sup> UC Berkeley, <sup>MT</sup> MIT, <sup>N</sup> NVIDIA, <sup>MS</sup> Microsoft, <sup>A</sup> Anyscale, <sup>G</sup> Google, <sup>P</sup> Princeton
</p>

# <span style="color:#0366d6;">Ember</span>: A Compositional Framework for Compound AI Systems

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

## Ember in a Nutshell

Aspirationally, Ember is to Networks of Networks (NONs) Compound AI Systems development what PyTorch
and XLA are to Neural Networks (NN) development. It's a compositional framework with both eager
execution affordances and graph execution optimization capabilities. It enables users to compose
complex NONs, and supports automatic parallelization and optimization of these.

Ember's vision is to enable development of **compound AI systems composed of, one day, millions-billions of inference calls** and beyond. Simple constructs--like **best-of-N graphs**, **verifier-prover structures**, and **ensembles with “voting-based” aggregation**--work surprisingly well in many regimes.

```python
# With Ember's "compact notation" it is one line to build a simple parallel system with 101 GPT-4o instances synthesized by Claude
system = non.build_graph(["101:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"])  # Automatically parallelized
result = system(query="What's the most effective climate change solution?")
```

This led us to believe that there is a rich architecture space for constructing and optimizing what we call “networks of networks” graphs, or **NONs**. This is analogous to how neural network architecture research uncovered many emergent properties of systems composed of simple artificial neurons. It would be frictionful to conduct NN research if we had to implement architectures from scratch via for-loops or implement bespoke libraries for vectorization and efficient execution. Similarly, it can be challenging at present to compose NON architectures of many calls, despite the **rapidly falling cost-per-token of intelligence**.

Ember's goal is to help unlock research and practice along this new frontier.

## Documentation & Examples

- [Architecture Overview](ARCHITECTURE.md)
- [Quick Start Guide](QUICKSTART.md)
- [LLM Specifications](LLM_SPECIFICATIONS.md)
- [Model Registry Guide](docs/quickstart/model_registry.md)
- [Operators Guide](docs/quickstart/operators.md)
- [NON Patterns](docs/quickstart/non.md)
- [Data Processing](docs/quickstart/data.md)
- [Configuration](docs/quickstart/configuration.md)
- [Examples Directory](src/ember/examples)

## Simple Example: Ensemble Reasoning with Automatic Parallelization

```python
class QueryInput(EmberModel):
    query: str

class ConfidenceOutput(EmberModel):
    answer: str
    confidence: float

class ReasonerSpec(Specification):
    input_model = QueryInput
    structured_output = ConfidenceOutput

@jit # Autonomically optimize execution with JIT compilation (e.g. TopoSort with Parallel Dispatch)
class EnsembleReasoner(Operator[QueryInput, ConfidenceOutput]):
    specification = ReasonerSpec()

    def __init__(self, width: int = 3):
        self.ensemble = non.UniformEnsemble(
            num_units=width,
            model_name="openai:gpt-4o",
            temperature=0.7
        )

        self.judge = non.JudgeSynthesis(
            model_name="anthropic:claude-3-5-sonnet",
        )

    def forward(self, *, inputs: QueryInput) -> ReasonedOutput:
        # These operations are automatically parallelized by Ember's XCS system
        ensemble_result = self.ensemble(query=inputs.query)

        synthesis = self.judge(
            query=inputs.query,
            responses=ensemble_result["responses"]
        )

        return ConfidenceOutput(
            answer=synthesis["final_answer"],
            confidence=float(synthesis.get("confidence", 0.0))
        )

# Use it like any Python function
compound_system = EnsembleReasoner()
result = compound_system(query="What causes the northern lights?")
print(f"Answer: {result.answer}")
print(f"Confidence: {result.confidence:.2f}")

# Alternatively, build the same pipeline with compact notation
pipeline = non.build_graph(["3:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.2"])
result = pipeline(query="What causes the northern lights?")
```

## Compact Notation

Ember's compact notation allows expression of complex AI architectures in minimal code:

```python
# Compact notation: "count:type:model:temperature" - each component precisely specified

# BASIC: Single-line systems with automatic parallelization
basic = non.build_graph(["7:E:gpt-4o:0.7"])                             # 7-model ensemble
voting = non.build_graph(["7:E:gpt-4o:0.7", "1:M"])                     # With majority voting
judged = non.build_graph(["7:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"])   # With judge synthesis

# STANDARD API: Equivalent to compact notation but with explicit objects
standard_system = non.Sequential(operators=[
    non.UniformEnsemble(num_units=7, model_name="gpt-4o", temperature=0.7),
    non.JudgeSynthesis(model_name="claude-3-5-sonnet", temperature=0.0)
])

# ADVANCED: Reusable components for complex architectures
components = {
    # Define building blocks once, reuse everywhere
    "reasoning": ["3:E:gpt-4o:0.7", "1:V:gpt-4o:0.0"],           # Verification pipeline
    "research": ["3:E:claude-3-5-sonnet:0.5", "1:V:claude-3-5-sonnet:0.0"]  # Different models
}

# Build sophisticated multi-branch architecture in just 4 lines
advanced = non.build_graph([
    "$reasoning",                     # First branch: reasoning with verification
    "$research",                      # Second branch: research with verification
    "1:J:claude-3-5-opus:0.0"         # Final synthesis of both branches
], components=components)             # Automatically optimized for parallel execution

# HORIZONTAL SCALING: Systematically explore scaling behavior
systems = {
    # Scaling with MostCommon aggregation
    "width_3_voting": non.build_graph(["3:E:gpt-4o:0.7", "1:M"]),
    "width_7_voting": non.build_graph(["7:E:gpt-4o:0.7", "1:M"]),
    "width_11_voting": non.build_graph(["11:E:gpt-4o:0.7", "1:M"]),

    # Scaling with judge synthesis
    "width_3_judge": non.build_graph(["3:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"]),
    "width_7_judge": non.build_graph(["7:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"]),
    "width_11_judge": non.build_graph(["11:E:gpt-4o:0.7", "1:J:claude-3-5-sonnet:0.0"]),
}

# Execute with full parallelism (XCS optimizes the execution graph automatically)
query = "What's the most effective climate change solution?"
results = {name: system(query=query) for name, system in systems.items()}
```

## Core Elements

1. **Composable Operators with Rigorous Specification**: Build reliable compound AI systems from
   type-safe, reusable components with validated inputs and outputs
2. **Automatic Parallelization**: Independent operations are automatically executed concurrently
   across a full computational graph
3. **XCS Optimization Framework**: "Accelerated Compound Systems" Just-in-time tracing and execution optimization with multiple strategies (trace, structural, enhanced). XCS is inspired by XLA, but intended more for accelerating compound systems vs. linear algebra operations, tuned for models and dicts, vs for vectors and numerical computation.
4. **Multi-Provider Support**: Unified API across OpenAI, Anthropic, Claude, Gemini, and more
   with standardized usage tracking
5. **Transformation System**: Function transformations for vectorization (vmap), parallelization (pmap), and device sharding (mesh), with a composable interface for building complex transformations

## XCS Architecture

The Accelerated Compound Systems (XCS) module provides a computational graph-based system for building, optimizing, and executing complex operator pipelines:

1. **Unified JIT System**: Multiple compilation strategies under a consistent interface:

   - `trace`: Traditional execution tracing
   - `structural`: Structure-based analysis
   - `enhanced`: Improved parallelism detection and code analysis

2. **Scheduler Framework**: Pluggable scheduler implementations for different execution patterns:

   - `sequential`: Serial execution for debugging and determinism
   - `parallel`: Thread-based parallel execution
   - `wave`: Execution wave scheduling for optimal parallelism
   - `topological`: Dependency-based execution ordering

3. **Transform System**: High-level operations for data and computation transformations:

   - `vmap`: Vectorized mapping for batch processing
   - `pmap`: Parallel mapping across multiple workers
   - `mesh`: Device mesh-based sharding for multi-device execution

4. **Dependency Analysis**: Automatic extraction of dependencies between operations:
   - Transitive closure calculation for complete dependency mapping
   - Topological sorting with cycle detection
   - Execution wave computation for parallel scheduling

## Installation

Ember uses [uv](https://github.com/astral-sh/uv) as its recommended package manager for significantly faster installations and dependency resolution.

```bash
# First, install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh  # macOS/Linux
# or
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"  # Windows
# or
pip install uv  # Any platform

# Quick install using uv (recommended)
uv pip install ember-ai

# Run examples directly with uv (no activation needed)
uv run python -c "import ember; print(ember.__version__)"

# Install from source for development
git clone https://github.com/pyember/ember.git
cd ember
uv pip install -e ".[dev]"

# Traditional pip installation (alternative, slower)
pip install ember-ai
```

For detailed installation instructions, troubleshooting, and environment management, see our [Installation Guide](INSTALLATION_GUIDE.md).

## Model Registry & Provider Integration

Access models from any provider through a unified interface:

```python
from ember import initialize_ember
from ember.api.models import ModelEnum

# Initialize with multiple providers
service = initialize_ember(usage_tracking=True)

# Access models from different providers with the same API
response = service(ModelEnum.gpt_4o, "What is quantum computing?")
print(response.data)

# Track usage across providers
usage = service.usage_service.get_total_usage()
print(f"Total cost: ${usage.cost:.4f}")
```

## NON Patterns & Ensembling

Build compound AI system architectures using the Network of Networks (NON) pattern with pre-built components:

```python
from ember.api import non

# Standard API: Create a verification pipeline of ensemble→judge→verifier
pipeline = non.Sequential(operators=[
    # 1. Ensemble of 5 model instances running in parallel
    non.UniformEnsemble(
        num_units=5,
        model_name="openai:gpt-4o-mini",
        temperature=0.7
    ),

    # 2. Judge to synthesize the ensemble responses
    non.JudgeSynthesis(
        model_name="anthropic:claude-3-5-sonnet",
        temperature=0.2
    ),

    # 3. Verifier for quality control and fact-checking
    non.Verifier(
        model_name="anthropic:claude-3-5-haiku",
        temperature=0.0
    )
])

# Alternatively, create the same pipeline with compact notation
pipeline = non.build_graph([
    "5:E:gpt-4o-mini:0.7",        # Ensemble with 5 instances
    "1:J:claude-3-5-sonnet:0.2",  # Judge synthesis
    "1:V:claude-3-5-haiku:0.0"    # Verification
])

# Build advanced architectures like NestedNetwork from example_architectures.py
# Define reusable SubNetwork component
components = {
    "sub": ["2:E:gpt-4o:0.0", "1:V:gpt-4o:0.0"]  # Ensemble → Verifier
}

# Create a NestedNetwork with identical structure to the OOP implementation
nested = non.build_graph([
    "$sub",                # First SubNetwork branch
    "$sub",                # Second SubNetwork branch
    "1:J:gpt-4o:0.0"       # Judge to synthesize results
], components=components)

# Extend with custom operator types
custom_registry = non.OpRegistry.create_standard_registry()
custom_registry.register(
    "CE",  # Custom ensemble type
    lambda count, model, temp: non.Sequential(operators=[
        non.UniformEnsemble(num_units=count, model_name=model, temperature=temp),
        non.MostCommon()  # Auto-aggregation
    ])
)

# Use custom operators
advanced = non.build_graph(["3:CE:gpt-4o:0.7"], type_registry=custom_registry)

# Execute with a single call
result = pipeline(query="What causes tsunamis?")
```

## Graph Optimization & Execution

Ember's XCS system provides JAX/XLA-inspired tracing, transformation, and automatic parallelization:

```python
from ember.xcs import jit, execution_options, vmap, pmap, compose, explain_jit_selection
from ember.api.operators import Operator

# Basic JIT compilation with automatic strategy selection
@jit
class SimplePipeline(Operator):
    # ... operator implementation ...

# JIT with explicit mode selection
@jit(mode="enhanced")
class ComplexPipeline(Operator):
    def __init__(self):
        self.op1 = SubOperator1()
        self.op2 = SubOperator2()
        self.op3 = SubOperator3()

    def forward(self, *, inputs):
        # These operations will be automatically parallelized
        result1 = self.op1(inputs=inputs)
        result2 = self.op2(inputs=inputs)

        # Combine the parallel results
        combined = self.op3(inputs={"r1": result1, "r2": result2})
        return combined

# Configure execution parameters
with execution_options(scheduler="wave", max_workers=4):
    result = pipeline(query="Complex question...")

# Get explanation for JIT strategy selection
explanation = explain_jit_selection(pipeline)
print(f"JIT strategy: {explanation['strategy']}")
print(f"Rationale: {explanation['rationale']}")

# Vectorized mapping for batch processing
batch_processor = vmap(my_operator)
batch_results = batch_processor(inputs={"data": [item1, item2, item3]})

# Parallel execution across multiple workers
parallel_processor = pmap(my_operator, num_workers=4)
parallel_results = parallel_processor(inputs=complex_data)

# Compose transformations (vectorization + parallelism)
pipeline = compose(vmap(batch_size=32), pmap(num_workers=4))(my_operator)
```

## Data Handling & Evaluation

Ember provides a comprehensive data processing and evaluation framework with pre-built datasets and metrics:

```python
from ember.api.data import DatasetBuilder
from ember.api.eval import EvaluationPipeline, Evaluator

# Load a dataset with the builder pattern
dataset = (DatasetBuilder()
    .from_registry("mmlu")  # Use a registered dataset
    .subset("physics")      # Select a specific subset
    .split("test")          # Choose the test split
    .sample(100)            # Random sample of 100 items
    .transform(              # Apply transformations
        lambda x: {"query": f"Question: {x['question']}"}
    )
    .build())

# Create a comprehensive evaluation pipeline
eval_pipeline = EvaluationPipeline([
    # Standard metrics
    Evaluator.from_registry("accuracy"),
    Evaluator.from_registry("response_quality"),

    # Custom evaluation metrics
    Evaluator.from_function(
        lambda prediction, reference: {
            "factual_accuracy": score_factual_content(prediction, reference)
        }
    )
])

# Evaluate a model or operator
results = eval_pipeline.evaluate(my_model, dataset)
print(f"Accuracy: {results['accuracy']:.2f}")
print(f"Response Quality: {results['response_quality']:.2f}")
print(f"Factual Accuracy: {results['factual_accuracy']:.2f}")
```

## License

Ember is released under the [MIT License](LICENSE).

</code>

setup.py:
<code>
import setuptools

if **name** == "**main**":
setuptools.setup()

</code>
